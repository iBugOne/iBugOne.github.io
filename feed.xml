<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ibug.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ibug.io/" rel="alternate" type="text/html" /><updated>2025-07-11T03:13:18+00:00</updated><id>https://ibug.io/feed.xml</id><title type="html">iBug</title><subtitle>The little personal site for iBug</subtitle><author><name>iBug</name></author><entry><title type="html">Fixing OneDrive not expanding in Explorer sidebar on Windows 10 LTSC</title><link href="https://ibug.io/blog/2025/04/windows-10-ltsc-onedrive-explorer-sidebar-fix/" rel="alternate" type="text/html" title="Fixing OneDrive not expanding in Explorer sidebar on Windows 10 LTSC" /><published>2025-04-22T00:00:00+00:00</published><updated>2025-04-23T00:39:07+00:00</updated><id>https://ibug.io/blog/2025/04/windows-10-ltsc-onedrive-explorer-sidebar-fix</id><content type="html" xml:base="https://ibug.io/blog/2025/04/windows-10-ltsc-onedrive-explorer-sidebar-fix/"><![CDATA[<p>I’ve recently re-installed Windows 10 LTSC for both of my computers, and after setting up OneDrive (from Office 365 suite), I noticed that the OneDrive folder in the Explorer sidebar was not expanding when clicked, showing a static gray arrow.
				This <em>could</em> be a known issue with Windows 10 LTSC, as it doesn’t include OneDrive integration by default, unlike consumer versions.</p>
			<p>For readers who want to skip the details, just head to the <a href="#solution"><strong>Solution</strong></a> section.</p>
			<h2 id="gathering-information">Gathering Information</h2>
			<p>Looking for “Windows 10 OneDrive doesn’t expand” on Google, <a href="https://answers.microsoft.com/en-us/msoffice/forum/all/one-drive-folder-tab-not-expanding/04eac1d9-2665-4c5e-b02b-9976fbcf2c49">One drive folder tab not expanding</a> shows up first.
				It contains no useful information other than a link to another: <a href="https://answers.microsoft.com/en-us/windows/forum/all/cannot-access-onedrive-personal-folder-through/fb21a27c-bf5a-4371-926e-9ff869070f03">Cannot access Onedrive - Personal folder through Quick Access of File Explorer</a>.</p>
			<p>The second link is much better populated with information, notably there’s a valid workaround:</p>
			<blockquote>
				<div class="language-ini highlighter-rouge">
					<div class="highlight">
						<pre class="highlight"><code><span class="err">Windows</span> <span class="err">Registry</span> <span class="err">Editor</span> <span class="err">Version</span> <span class="err">5.00</span>

<span class="nn">[HKEY_CLASSES_ROOT\CLSID\{018D5C66-4533-4307-9B53-224DE2ED1FE6}\Instance\InitPropertyBag]</span>
<span class="err">"TargetKnownFolder"=""</span>

<span class="nn">[HKEY_CLASSES_ROOT\WOW6432Node\CLSID\{018D5C66-4533-4307-9B53-224DE2ED1FE6}\Instance\InitPropertyBag]</span>
<span class="err">"TargetKnownFolder"=""</span>
</code></pre>
					</div>
  </div>
				<p>Save them as a .reg file, run it each time you log on (and after One Drive running)</p>
			</blockquote>
			<p>Note how this needs to be applied every time OneDrive starts, which is something to further investigate.</p>
			<p>With a bit more trying and Googling, it turns out OneDrive sets these keys to <code class="language-plaintext highlighter-rouge">{a52bba46-e9e1-435f-b3d9-28daa648c0f6}</code>, which is the KnownFolderID for the OneDrive folder, and is missing from the registry.</p>
			<p>From the two <code class="language-plaintext highlighter-rouge">bbs.pcbeta.com</code> links (Chinese: 远景论坛, <a href="https://bbs.pcbeta.com/viewthread-1933367-2-2.html">1</a> #32, <a href="https://bbs.pcbeta.com/viewthread-1936542-1-2.html">2</a> #17) mentioned in the thread, adding back the missing registry keys for <code class="language-plaintext highlighter-rouge">{a52bba46-e9e1-435f-b3d9-28daa648c0f6}</code> should fix the issue.</p>
			<h2 id="solution">Solution</h2>
			<p>Just import the missing registry keys for OneDrive.
				These registry keys are exported from a working Windows 10 Enterprise installation.</p>
			<div class="language-ini highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="err">Windows</span> <span class="err">Registry</span> <span class="err">Editor</span> <span class="err">Version</span> <span class="err">5.00</span>

<span class="nn">[HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\FolderDescriptions\{A52BBA46-E9E1-435f-B3D9-28DAA648C0F6}]</span>
<span class="err">"Attributes"=dword:00000001</span>
<span class="err">"Category"=dword:00000004</span>
<span class="err">"DefinitionFlags"=dword:00000040</span>
<span class="err">"Icon"=hex(2):25,00,53,00,79,00,73,00,74,00,65,00,6d,00,52,00,6f,00,6f,00,74,</span><span class="se">\
</span>  <span class="err">00,25,00,5c,00,73,00,79,00,73,00,74,00,65,00,6d,00,33,00,32,00,5c,00,69,00,</span><span class="se">\
</span>  <span class="err">6d,00,61,00,67,00,65,00,72,00,65,00,73,00,2e,00,64,00,6c,00,6c,00,2c,00,2d,</span><span class="se">\
</span>  <span class="err">00,31,00,30,00,34,00,30,00,00,00</span>
<span class="err">"LocalizedName"=hex(2):40,00,25,00,53,00,79,00,73,00,74,00,65,00,6d,00,52,00,</span><span class="se">\
</span>  <span class="err">6f,00,6f,00,74,00,25,00,5c,00,53,00,79,00,73,00,74,00,65,00,6d,00,33,00,32,</span><span class="se">\
</span>  <span class="err">00,5c,00,53,00,65,00,74,00,74,00,69,00,6e,00,67,00,53,00,79,00,6e,00,63,00,</span><span class="se">\
</span>  <span class="err">43,00,6f,00,72,00,65,00,2e,00,64,00,6c,00,6c,00,2c,00,2d,00,31,00,30,00,32,</span><span class="se">\
</span>  <span class="err">00,34,00,00,00</span>
<span class="err">"LocalRedirectOnly"=dword:00000001</span>
<span class="err">"Name"="OneDrive"</span>
<span class="err">"ParentFolder"="{5E6C858F-0E22-4760-9AFE-EA3317B67173}"</span>
<span class="err">"ParsingName"="shell:::{018D5C66-4533-4307-9B53-224DE2ED1FE6}"</span>
<span class="err">"RelativePath"="OneDrive"</span>

<span class="nn">[HKEY_LOCAL_MACHINE\SOFTWARE\WOW6432Node\Microsoft\Windows\CurrentVersion\Explorer\FolderDescriptions\{A52BBA46-E9E1-435f-B3D9-28DAA648C0F6}]</span>
<span class="err">"Attributes"=dword:00000001</span>
<span class="err">"Category"=dword:00000004</span>
<span class="err">"DefinitionFlags"=dword:00000040</span>
<span class="err">"Icon"=hex(2):25,00,53,00,79,00,73,00,74,00,65,00,6d,00,52,00,6f,00,6f,00,74,</span><span class="se">\
</span>  <span class="err">00,25,00,5c,00,73,00,79,00,73,00,74,00,65,00,6d,00,33,00,32,00,5c,00,69,00,</span><span class="se">\
</span>  <span class="err">00,31,00,30,00,34,00,30,00,00,00</span>
<span class="err">"LocalizedName"=hex(2):40,00,25,00,53,00,79,00,73,00,74,00,65,00,6d,00,52,00,</span><span class="se">\
</span>  <span class="err">6f,00,6f,00,74,00,25,00,5c,00,53,00,79,00,73,00,74,00,65,00,6d,00,33,00,32,</span><span class="se">\
</span>  <span class="err">00,5c,00,53,00,65,00,74,00,74,00,69,00,6e,00,67,00,53,00,79,00,6e,00,63,00,</span><span class="se">\
</span>  <span class="err">43,00,6f,00,72,00,65,00,2e,00,64,00,6c,00,6c,00,2c,00,2d,00,31,00,30,00,32,</span><span class="se">\
</span>  <span class="err">00,34,00,00,00</span>
<span class="err">"LocalRedirectOnly"=dword:00000001</span>
<span class="err">"Name"="OneDrive"</span>
<span class="err">"ParentFolder"="{5E6C858F-0E22-4760-9AFE-EA3317B67173}"</span>
<span class="err">"ParsingName"="shell:::{018D5C66-4533-4307-9B53-224DE2ED1FE6}"</span>
<span class="err">"RelativePath"="OneDrive"</span>
</code></pre>
				</div>
			</div>
			<p>Note that this article happens to reproduce <a href="https://lolicp.com/windows/202309626.html">this blog</a> from lolicp.com, which may be of Chinese readers’ interest.</p>
			]]></content><author><name>iBug</name></author><category term="windows" /><summary type="html"><![CDATA[I’ve recently re-installed Windows 10 LTSC for both of my computers, and after setting up OneDrive (from Office 365 suite), I noticed that the OneDrive folder in the Explorer sidebar was not expanding when clicked, showing a static gray arrow. This could be a known issue with Windows 10 LTSC, as it doesn’t include OneDrive integration by default, unlike consumer versions.]]></summary></entry><entry><title type="html">Beating $3k SSD with $2k HDD?</title><link href="https://ibug.io/blog/2024/10/ustc-mirrors-zfs-rebuild/" rel="alternate" type="text/html" title="Beating $3k SSD with $2k HDD?" /><published>2024-10-27T00:00:00+00:00</published><updated>2024-12-11T18:19:07+00:00</updated><id>https://ibug.io/blog/2024/10/ustc-mirrors-zfs-rebuild</id><content type="html" xml:base="https://ibug.io/blog/2024/10/ustc-mirrors-zfs-rebuild/"><![CDATA[<p>A.K.A. Practical ZFS application on USTC Mirrors. A writeup of the talk I gave at Nanjing University this August.</p>
		<h2 id="background">Background</h2>
		<p><a href="https://mirrors.ustc.edu.cn/">USTC Open-Source Software Mirrors</a> is one of the largest public mirror sites in China. In the two months of May and June 2024, we served an average daily egress traffic of some 36&nbsp;TiB, which breaks down as follows:</p>
		<ul>
			<li>19&nbsp;TiB from HTTP/HTTPS, among 17M requests</li>
			<li>10.3&nbsp;TiB from rsync, among 21.8k requests (if we count one absurd client in, the number of requests goes to 147.8k)</li>
		</ul>
		<p>Over the years, as mirror repositories have grown and new repositories have been added, we have been running tight on disk space. For our two servers responsible for the mirror service, we have reached unhealthy levels of disk usage:</p>
		<ul>
			<li>HTTP server (XFS): 63.3&nbsp;TiB used out of 66.0&nbsp;TiB (96%, achieved on December 18, 2023)</li>
			<li>Rsync server (ZFS): 42.4&nbsp;TiB used out of 43.2&nbsp;TiB (98%, achieved on November 21, 2023)</li>
		</ul>
		<p>The servers have the following configurations:</p>
		<dl>
			<dt>HTTP server</dt>
			<dd>
				<ul>
					<li>Set up in Fall 2020</li>
					<li>Intel Cascade Lake CPU, 256&nbsp;GB DDR4 RAM</li>
					<li>Twelve 10&nbsp;TB HDDs + One 2&nbsp;TB SSD</li>
					<li>XFS on LVM on hardware RAID</li>
					<li>Reserved free PEs on LVM VG level as XFS cannot be shrunk</li>
				</ul>
			</dd>
			<dt>Rsync server</dt>
			<dd>
				<ul>
					<li>Set up in Winter 2016</li>
					<li>Intel Broadwell CPU, 256&nbsp;GB DDR4 RAM</li>
					<li>Twelve 6&nbsp;TB HDDs + some smaller SSDs for OS and cache</li>
					<li>RAID-Z3 on ZFS, 8 data disks + 3 parity disks + 1 hot spare</li>
					<li>All default parameters (except <code class="language-plaintext highlighter-rouge">zfs_arc_max</code>)</li>
				</ul>
			</dd>
		</dl>
		<p>These servers are constantly running at an I/O utilization of over 90%, which results in less than 50&nbsp;MB/s download speed even from within USTC campus. Clearly this is not the ideal performance for this kind of dedicated storage servers.</p>
		<figure class=""><a href="https://image.ibugone.com/grafana/mirrors-io-utilization-may-2024.png" class="image-popup" title="I/O load of two servers from USTC Mirrors in May 2024
"><img src="https://image.ibugone.com/grafana/mirrors-io-utilization-may-2024.png" alt="I/O load of two servers from USTC Mirrors in May 2024" /></a>
			<figcaption>
				I/O load of two servers from USTC Mirrors in May 2024
			</figcaption>
		</figure>
		<h2 id="zfs">ZFS</h2>
		<p>ZFS is usually known for being the ultimate single-node storage solution. It combines RAID, volume management, and filesystem in one, and provides advanced features like snapshots, clones and send/receive. Everything in ZFS is checksummed, ensuring data integrity. For servers dedicated to storage, ZFS appears to be a “fire and forget” solution, which is easily challenged by its tremendous amount of tunables and parameters.</p>
		<p>As preliminary learning and experiments, I sourced some drives for my own workstation and set up two ZFS pools on them. Then I signed up for some private tracker (PT) sites for I/O load to tune for. The results were quite satisfying: In two years and a half, my single-node PT station has generated 1.20&nbsp;PiB of uploads.</p>
		<p>Over the years, I have gathered some of my most important sources for learning ZFS:</p>
		<ul>
			<li>Chris’s Wiki: <a href="https://utcc.utoronto.ca/~cks/space/blog/">https://utcc.utoronto.ca/~cks/space/blog/</a></li>
			<li>OpenZFS Documentation: <a href="https://openzfs.github.io/openzfs-docs/">https://openzfs.github.io/openzfs-docs/</a></li>
			<li>My own blog: <a href="/p/62">Understanding ZFS block sizes</a>
				<ul>
					<li>Plus all references in the article</li>
				</ul>
			</li>
		</ul>
		<figure class=""><a href="https://image.ibugone.com/grafana/qb/2024-06-05.png" class="image-popup" title="A byproduct of my ZFS learning: A Grafana dashboard for qBittorrent (lol…)
"><img src="https://image.ibugone.com/grafana/qb/2024-06-05.png" alt="A Grafana dashboard for qBittorrent" /></a>
			<figcaption>
				A byproduct of my ZFS learning: A Grafana dashboard for qBittorrent (lol…)
			</figcaption>
		</figure>
		<p>After these years of learning ZFS, I realized that there’s a substantial room for improvement in our mirror servers, by embracing ZFS and tuning it properly.</p>
		<h2 id="mirrors">Mirrors</h2>
		<p>Before we move on to rebuilding the ZFS pool, we need to understand our I/O workload. In essence, a mirror site:</p>
		<ul>
			<li>Provides file downloads</li>
			<li>Also (begrudgingly) serves as speed tests</li>
			<li>Mostly reads, and almost all reads are whole-file sequential reads</li>
			<li>Can withstand minimal data loss as mirror contents can be easily re-synced</li>
		</ul>
		<figure class=""><a href="https://image.ibugone.com/server/mirrors-file-size-distribution-2024-08.png" class="image-popup" title="File size distribution of USTC Mirrors in August 2024
"><img src="https://image.ibugone.com/server/mirrors-file-size-distribution-2024-08.png" alt="File size distribution of USTC Mirrors in August 2024" /></a>
			<figcaption>
				File size distribution of USTC Mirrors in August 2024
			</figcaption>
		</figure>
		<p>With those in mind, we analyzed our mirror content. As can be seen from the graph above, half of the 40M files are less than 10&nbsp;KiB in size, and 90% of the files are less than 1&nbsp;MiB. Still, the files are averaged at 1.6&nbsp;MiB.</p>
		<h2 id="mirrors2">Rebuilding the Rsync server</h2>
		<p>In June, we set out to rebuild the Rsync server as it had a lower service traffic and importance, yet a disproportionately higher disk usage. We laid out the following plan:</p>
		<ul>
			<li>First, the RAID overhead of RAID-Z3 was too high (reiterating: half of the files are less than 10&nbsp;KiB, and the disks have 4&nbsp;KiB sectors), so we decided to switch to RAID-Z2 as well as split the RAID group into two. Two RAIDZ vdevs also implies double the IOPS, as each “block” (in ZFS parlance) is stored on only one vdev.</li>
			<li>We then carefully select dataset properties to optimize for our workload:
				<ul>
					<li><code class="language-plaintext highlighter-rouge">recordsize=1M</code> to maximize sequential throughput and minimize fragmentation</li>
					<li><code class="language-plaintext highlighter-rouge">compression=zstd</code> to (try to) save some disk space
						<ul>
							<li>
								<p>Since OpenZFS 2.2, a mechanism called “early-abort” has been extended to Zstd compression (level 3+), which saves CPU cycles by testing data compressibility with LZ4 then Zstd 1, before actually trying to compress with Zstd.</p>
								<p>We know that most of our mirror content is already compressed (like software packages and ISOs), so early-abort is urging us to use Zstd.</p>
							</li>
						</ul>
					</li>
					<li><code class="language-plaintext highlighter-rouge">xattr=off</code> as we don’t need extended attributes for mirror content.</li>
					<li><code class="language-plaintext highlighter-rouge">atime=off</code> as we don’t need access time. Also cuts off a lot of writes.</li>
					<li><code class="language-plaintext highlighter-rouge">setuid=off</code>, <code class="language-plaintext highlighter-rouge">exec=off</code>, <code class="language-plaintext highlighter-rouge">devices=off</code> to disable what we don’t need.</li>
					<li><code class="language-plaintext highlighter-rouge">secondarycache=metadata</code> to cache metadata only, as this Rsync server has a much more uniform access pattern than the HTTP server. We would like to save our SSDs from unnecessary writes.</li>
				</ul>
			</li>
			<li>Some slightly dangerous properties:
				<ul>
					<li><code class="language-plaintext highlighter-rouge">sync=disabled</code> to disable synchronous writes. This allows ZFS to buffer writes up to <code class="language-plaintext highlighter-rouge">zfs_txg_timeout</code> seconds and make better allocation decisions.</li>
					<li><code class="language-plaintext highlighter-rouge">redundant_metadata=some</code> to trade some metadata redundancy for better write performance.</li>
				</ul>
				<p>We believe these changes are in alignment with our evaluation of data safety and loss tolerance.</p>
			</li>
			<li>
				<p>For ZFS module parameters, the sheer number of 290+ tunables is overwhelming. Thanks to @happyaron, the current ZFS maintainer in Debian and administrator of BFSU Mirror, we selected a handful of them:</p>
				<div class="language-shell highlighter-rouge">
					<div class="highlight">
						<pre class="highlight"><code><span class="c"># Set ARC size to 160-200&nbsp;GiB, keep 16&nbsp;GiB free for OS</span>
options zfs <span class="nv">zfs_arc_max</span><span class="o">=</span>214748364800
options zfs <span class="nv">zfs_arc_min</span><span class="o">=</span>171798691840
options zfs <span class="nv">zfs_arc_sys_free</span><span class="o">=</span>17179869184

<span class="c"># Favor metadata to data by 20x (OpenZFS 2.2+)</span>
options zfs <span class="nv">zfs_arc_meta_balance</span><span class="o">=</span>2000

<span class="c"># Allow up to 80% of ARC to be used for dnodes</span>
options zfs <span class="nv">zfs_arc_dnode_limit_percent</span><span class="o">=</span>80

<span class="c"># See man page section "ZFS I/O Scheduler"</span>
options zfs <span class="nv">zfs_vdev_async_read_max_active</span><span class="o">=</span>8
options zfs <span class="nv">zfs_vdev_async_read_min_active</span><span class="o">=</span>2
options zfs <span class="nv">zfs_vdev_scrub_max_active</span><span class="o">=</span>5
options zfs <span class="nv">zfs_vdev_max_active</span><span class="o">=</span>20000

<span class="c"># Never throttle the ARC</span>
options zfs <span class="nv">zfs_arc_lotsfree_percent</span><span class="o">=</span>0

<span class="c"># Tune L2ARC</span>
options zfs <span class="nv">l2arc_headroom</span><span class="o">=</span>8
options zfs <span class="nv">l2arc_write_max</span><span class="o">=</span>67108864
options zfs <span class="nv">l2arc_noprefetch</span><span class="o">=</span>0
</code></pre>
					</div>
    </div>
				<p>And also <code class="language-plaintext highlighter-rouge">zfs_dmu_offset_next_sync</code>, which is enabled by default since OpenZFS 2.1.5, so it’s omitted from our list.</p>
			</li>
		</ul>
		<p>After relocating Rsync service to our primary server (HTTP server), we broke up the existing ZFS pool and rebuilt it anew, before syncing previous repositories back from external sources. To our surprise, the restoration took only 3 days, much faster than we had anticipated. Other numbers also looked promising:</p>
		<ul>
			<li>
				<p>Compression ratio: 39.5T / 37.1T (1.07x)</p>
				<p>We’d like to point out that ZFS only provides two digits after the decimal point for compression ratio, so if you want a higher precision, you need take the raw numbers and calculate it yourself:</p>
				<div class="language-shell highlighter-rouge">
					<div class="highlight">
						<pre class="highlight"><code>zfs list <span class="nt">-po</span> name,logicalused,used
</code></pre>
					</div>
    </div>
				<p>Our actual number was 1 + 6.57%, at 2.67&nbsp;TB (2.43&nbsp;TiB) saved, which means equivalently 9 copies of WeChat data <a href="https://image.ibugone.com/teaser/lenovo-legion-wechat-data.jpg">as advertised by Lenovo Legion</a>.</p>
			</li>
			<li>
				<p>And most importantly, a much saner I/O load:</p>
				<figure class=""><a href="https://image.ibugone.com/grafana/mirrors2-io-utilization-and-free-space-june-july-2024.png" class="image-popup" title="I/O load of server “mirrors2” before and after the rebuild
"><img src="https://image.ibugone.com/grafana/mirrors2-io-utilization-and-free-space-june-july-2024.png" alt="I/O load of server mirrors2 before and after the rebuild" /></a>
					<figcaption>
						I/O load of server “mirrors2” before and after the rebuild
					</figcaption>
				</figure>
			</li>
		</ul>
		<p>We can see that, after a few days of warm-up, the I/O load has maintained at around 20%, whereas it was constantly at 90% before the rebuild.</p>
		<h2 id="mirrors4">Rebuilding the HTTP server</h2>
		<p>Our HTTP server was set up in late 2020 and under a different background.
			When we were first deciding the technology stack, we were not confident in ZFS and were discouraged by the abysmal performance of our Rsync server.
			So we opted for an entirely different stack for this server: hardware RAID, LVM (because the RAID controller didn’t allow RAID groups across two controllers), and XFS.
			For memory caching, we relied on kernel’s page cache, and for SSD caching, we tried LVMcache, which was quite new at the moment and rather immature.</p>
		<p>These unpracticed technologies have, without a doubt, ended up a pain.</p>
		<ul>
			<li>XFS cannot be shrunk, so we had to reserve free PEs at LVM VG level. We also cannot fill the FS, so there are two levels of free space reservation. Double the waste.</li>
			<li>We initially allocated 1.5&nbsp;TB of SSD cache, but given LVMcache’s recommendation of no more than 1 million chunks, we opted for just 1&nbsp;TiB (1&nbsp;MiB chunk size × 1 Mi chunks).</li>
			<li>There were no options for cache eviction policy, so later we dug into the kernel source code and found that it was a 64-level LRU.</li>
			<li>The first thing to die was GRUB2. Due to GRUB’s parsing of LVM metadata, it was unable to boot from a VG where a cached volume was present. We had to <a href="https://github.com/taoky/grub/commit/85b260baec91aa4f7db85d7592f6be92d549a0ae">patch</a> GRUB for it to handle this case.</li>
			<li>With an incorrect understanding of chunk size and number of chunks, our SSD ran severely over its write endurance in under 2 years, and we had to replace it with a new one.</li>
		</ul>
		<p>Even after understanding the algorithm and still going for 128&nbsp;KiB chunk size and over 8 Mi chunks, LVMcache still didn’t offer a competitive hit rate:</p>
		<figure class=""><a href="https://image.ibugone.com/grafana/mirrors4-dmcache-may-june-2024.png" class="image-popup" title="LVMcache hit rate over May to June 2024
"><img src="https://image.ibugone.com/grafana/mirrors4-dmcache-may-june-2024.png" alt="LVMcache hit rate over May to June 2024" /></a>
			<figcaption>
				LVMcache hit rate over May to June 2024
			</figcaption>
		</figure>
		<p>We had already been fed up with those troubles through the years, and the success with our Rsync server rebuild gave us great confidence with ZFS.
			So in less than a month, we laid out a similar plan for our HTTP server, but trying something new:</p>
		<ul>
			<li>We updated the kernel to <code class="language-plaintext highlighter-rouge">6.8.8-3-pve</code>, which bundles the latest <code class="language-plaintext highlighter-rouge">zfs.ko</code> for us. This means we don’t have to waste time on DKMS.</li>
			<li>Since the number of disks is the same (12 disks), we also went for two RAID-Z2 vdevs with 6 disks each.
				<ul>
					<li>As this server provides HTTP service to end users, the access pattern will have a greater hot/cold distinction than the Rsync server. So we keep <code class="language-plaintext highlighter-rouge">secondarycache=all</code> for this server (leave the default value unchanged).</li>
					<li>This newer server has a better CPU, so we increased compression level to <code class="language-plaintext highlighter-rouge">zstd-8</code> in hope for a better compression ratio.</li>
				</ul>
			</li>
			<li>Since we already have the Rsync server running ZFS with desired parameters, we have <code class="language-plaintext highlighter-rouge">zfs send -Lcp</code> available when syncing the data back. This allows us to restore 50+ TiB of data in just 36 hours.</li>
			<li>Due to having a slightly different set of repositories, the compression ratio is slightly lower at 1 + 3.93% (2.42&nbsp;TiB / 2.20&nbsp;TiB saved).</li>
		</ul>
		<p>We put the I/O loads of both servers together for comparison:</p>
		<figure class=""><a href="https://image.ibugone.com/grafana/mirrors2-4-io-utilization-june-july-2024.png" class="image-popup" title="I/O load of two servers from USTC Mirrors before and after rebuild
"><img src="https://image.ibugone.com/grafana/mirrors2-4-io-utilization-june-july-2024.png" alt="I/O load of two servers from USTC Mirrors before and after rebuild" /></a>
			<figcaption>
				I/O load of two servers from USTC Mirrors before and after rebuild
			</figcaption>
		</figure>
		<p>This graph starts with the initial state. The first server was rebuilt at 1/3, and the second server was rebuilt at 2/3.</p>
		<p>The hit rate of ZFS ARC is also quite satisfying:</p>
		<figure class=""><a href="https://image.ibugone.com/grafana/mirrors2-4-zfs-arc-hit-rate.png" class="image-popup" title="ZFS ARC hit rate of two servers
"><img src="https://image.ibugone.com/grafana/mirrors2-4-zfs-arc-hit-rate.png" alt="ZFS ARC hit rate of two servers" /></a>
			<figcaption>
				ZFS ARC hit rate of two servers
			</figcaption>
		</figure>
		<p>The stablized I/O load is even lower after both servers were rebuilt.</p>
		<figure class=""><a href="https://image.ibugone.com/grafana/mirrors2-4-disk-io-after-rebuild.png" class="image-popup" title="Sustained disk I/O of two servers after rebuild
"><img src="https://image.ibugone.com/grafana/mirrors2-4-disk-io-after-rebuild.png" alt="Sustained disk I/O of two servers after rebuild" /></a>
			<figcaption>
				Sustained disk I/O of two servers after rebuild
			</figcaption>
		</figure>
		<h2 id="misc">Misc</h2>
		<h3 id="zfs-compression">ZFS compression</h3>
		<p>We are slightly surprised to see that so many repositories are well-compressible:</p>
		<table>
			<thead>
				<tr>
					<th style="text-align: left">NAME</th>
					<th style="text-align: right">LUSED</th>
					<th style="text-align: right">USED</th>
					<th style="text-align: right">RATIO</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td style="text-align: left">pool0/repo/crates.io-index</td>
					<td style="text-align: right">2.19G</td>
					<td style="text-align: right">1.65G</td>
					<td style="text-align: right">3.01x</td>
				</tr>
				<tr>
					<td style="text-align: left">pool0/repo/elpa</td>
					<td style="text-align: right">3.35G</td>
					<td style="text-align: right">2.32G</td>
					<td style="text-align: right">1.67x</td>
				</tr>
				<tr>
					<td style="text-align: left">pool0/repo/rfc</td>
					<td style="text-align: right">4.37G</td>
					<td style="text-align: right">3.01G</td>
					<td style="text-align: right">1.56x</td>
				</tr>
				<tr>
					<td style="text-align: left">pool0/repo/debian-cdimage</td>
					<td style="text-align: right">1.58T</td>
					<td style="text-align: right">1.04T</td>
					<td style="text-align: right">1.54x</td>
				</tr>
				<tr>
					<td style="text-align: left">pool0/repo/tldp</td>
					<td style="text-align: right">4.89G</td>
					<td style="text-align: right">3.78G</td>
					<td style="text-align: right">1.48x</td>
				</tr>
				<tr>
					<td style="text-align: left">pool0/repo/loongnix</td>
					<td style="text-align: right">438G</td>
					<td style="text-align: right">332G</td>
					<td style="text-align: right">1.34x</td>
				</tr>
				<tr>
					<td style="text-align: left">pool0/repo/rosdistro</td>
					<td style="text-align: right">32.2M</td>
					<td style="text-align: right">26.6M</td>
					<td style="text-align: right">1.31x</td>
				</tr>
			</tbody>
		</table>
		<p>A few numbers (notably the first one) don’t make sense, which we attribute to <a href="https://github.com/openzfs/zfs/issues/7639"><i class="fab fa-github"></i> openzfs/zfs#7639</a>.</p>
		<p>If we sort the table by difference, it would be:</p>
		<table>
			<thead>
				<tr>
					<th style="text-align: left">NAME</th>
					<th style="text-align: right">LUSED</th>
					<th style="text-align: right">USED</th>
					<th style="text-align: right">DIFF</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td style="text-align: left">pool0/repo</td>
					<td style="text-align: right">58.3T</td>
					<td style="text-align: right">56.1T</td>
					<td style="text-align: right">2.2T</td>
				</tr>
				<tr>
					<td style="text-align: left">pool0/repo/debian-cdimage</td>
					<td style="text-align: right">1.6T</td>
					<td style="text-align: right">1.0T</td>
					<td style="text-align: right">549.6G</td>
				</tr>
				<tr>
					<td style="text-align: left">pool0/repo/opensuse</td>
					<td style="text-align: right">2.5T</td>
					<td style="text-align: right">2.3T</td>
					<td style="text-align: right">279.7G</td>
				</tr>
				<tr>
					<td style="text-align: left">pool0/repo/turnkeylinux</td>
					<td style="text-align: right">1.2T</td>
					<td style="text-align: right">1.0T</td>
					<td style="text-align: right">155.2G</td>
				</tr>
				<tr>
					<td style="text-align: left">pool0/repo/loongnix</td>
					<td style="text-align: right">438.2G</td>
					<td style="text-align: right">331.9G</td>
					<td style="text-align: right">106.3G</td>
				</tr>
				<tr>
					<td style="text-align: left">pool0/repo/alpine</td>
					<td style="text-align: right">3.0T</td>
					<td style="text-align: right">2.9T</td>
					<td style="text-align: right">103.9G</td>
				</tr>
				<tr>
					<td style="text-align: left">pool0/repo/openwrt</td>
					<td style="text-align: right">1.8T</td>
					<td style="text-align: right">1.7T</td>
					<td style="text-align: right">70.0G</td>
				</tr>
			</tbody>
		</table>
		<p><code class="language-plaintext highlighter-rouge">debian-cdimage</code> alone contributes to a quarter of the saved space.</p>
		<h3 id="grafana-for-zfs-io">Grafana for ZFS I/O</h3>
		<p>We also fixed a Grafana panel for ZFS I/O so it’s displaying the correct numbers.
			Because ZFS I/O statistics are exported through <code class="language-plaintext highlighter-rouge">/proc/spl/kstat/zfs/$POOL/objset-$OBJSETID_HEX</code> and is cumulative per “object set” (i.e. dataset), we need to calculate the derivative of the numbers and <em>then</em> sum by pool.
			This means the use of subqueries is inevitable.</p>
		<div class="language-sql highlighter-rouge">
			<div class="highlight">
				<pre class="highlight"><code><span class="k">SELECT</span>
  <span class="n">non_negative_derivative</span><span class="p">(</span><span class="k">sum</span><span class="p">(</span><span class="nv">"reads"</span><span class="p">),</span> <span class="mi">1</span><span class="n">s</span><span class="p">)</span> <span class="k">AS</span> <span class="nv">"read"</span><span class="p">,</span>
  <span class="n">non_negative_derivative</span><span class="p">(</span><span class="k">sum</span><span class="p">(</span><span class="nv">"writes"</span><span class="p">),</span> <span class="mi">1</span><span class="n">s</span><span class="p">)</span> <span class="k">AS</span> <span class="nv">"write"</span>
<span class="k">FROM</span> <span class="p">(</span>
  <span class="k">SELECT</span>
    <span class="k">first</span><span class="p">(</span><span class="nv">"reads"</span><span class="p">)</span> <span class="k">AS</span> <span class="nv">"reads"</span><span class="p">,</span>
    <span class="k">first</span><span class="p">(</span><span class="nv">"writes"</span><span class="p">)</span> <span class="k">AS</span> <span class="nv">"writes"</span>
  <span class="k">FROM</span> <span class="nv">"zfs_pool"</span>
  <span class="k">WHERE</span> <span class="p">(</span><span class="nv">"host"</span> <span class="o">=</span> <span class="s1">'taokystrong'</span> <span class="k">AND</span> <span class="nv">"pool"</span> <span class="o">=</span> <span class="s1">'pool0'</span><span class="p">)</span> <span class="k">AND</span> <span class="err">$</span><span class="n">timeFilter</span>
  <span class="k">GROUP</span> <span class="k">BY</span> <span class="nb">time</span><span class="p">(</span><span class="err">$</span><span class="n">interval</span><span class="p">),</span> <span class="nv">"host"</span><span class="p">::</span><span class="n">tag</span><span class="p">,</span> <span class="nv">"pool"</span><span class="p">::</span><span class="n">tag</span><span class="p">,</span> <span class="nv">"dataset"</span><span class="p">::</span><span class="n">tag</span> <span class="n">fill</span><span class="p">(</span><span class="k">null</span><span class="p">)</span>
<span class="p">)</span>
<span class="k">WHERE</span> <span class="err">$</span><span class="n">timeFilter</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="nb">time</span><span class="p">(</span><span class="err">$</span><span class="n">interval</span><span class="p">),</span> <span class="nv">"pool"</span><span class="p">::</span><span class="n">tag</span> <span class="n">fill</span><span class="p">(</span><span class="n">linear</span><span class="p">)</span>
</code></pre>
			</div>
		</div>
		<p>This query is a bit slow (due to the subquery) and unfortunately there’s not much we can do about it.</p>
		<p>To display I/O bandwidth, simply replace <code class="language-plaintext highlighter-rouge">reads</code> and <code class="language-plaintext highlighter-rouge">writes</code> with <code class="language-plaintext highlighter-rouge">nread</code> and <code class="language-plaintext highlighter-rouge">nwritten</code> in the inner query.</p>
		<figure class=""><a href="https://image.ibugone.com/grafana/mirrors2-4-zfs-io-count.png" class="image-popup" title="ZFS I/O count and bandwidth
"><img src="https://image.ibugone.com/grafana/mirrors2-4-zfs-io-count.png" alt="ZFS I/O count and bandwidth" /></a>
			<figcaption>
				ZFS I/O count and bandwidth
			</figcaption>
		</figure>
		<p>We are astonished to see an HDD array can sustain 15k IOPS and peaking at 50k IOPS.
			This becomes all explained when we discovered that these numbers took ARC hits into account, and a minimal proportion were actually hitting the disks.</p>
		<h3 id="apparmor">AppArmor</h3>
		<p>It didn’t take long before we noticed all our sync tasks were failing.
			We found <code class="language-plaintext highlighter-rouge">rsync</code> failing with <code class="language-plaintext highlighter-rouge">EPERM</code> for <code class="language-plaintext highlighter-rouge">socketpair(2)</code> calls, which never manifested before.
			Interestingly, these were denied by AppArmor.
			We traced down the cause to be Ubuntu’s addition to the kernel, <code class="language-plaintext highlighter-rouge">security/apparmor/af_unix.c</code>.
			As Proxmox VE forks its kernel from Ubuntu, this change also made its way into our server.</p>
		<p>We also found PVE packaging their own copy of AppArmor <code class="language-plaintext highlighter-rouge">features</code>, so we decided to adopt the same approach:</p>
		<div class="language-shell highlighter-rouge">
			<div class="highlight">
				<pre class="highlight"><code>dpkg-divert <span class="nt">--package</span> lxc-pve <span class="nt">--rename</span> <span class="nt">--divert</span> /usr/share/apparmor-features/features.stock <span class="nt">--add</span> /usr/share/apparmor-features/features
wget <span class="nt">-O</span> /usr/share/apparmor-features/features https://github.com/proxmox/lxc/raw/master/debian/features
</code></pre>
			</div>
		</div>
		<h3 id="file-deduplication">File deduplication</h3>
		<p>For a small set of repositories, possibly due to limitations of syncing methods, we noticed a lot of identically-looking directories.</p>
		<figure class=""><a href="https://image.ibugone.com/server/ls-zerotier-redhat-el.png" class="image-popup" title="Some folders from ZeroTier repository
"><img src="https://image.ibugone.com/server/ls-zerotier-redhat-el.png" alt="Some folders from ZeroTier repository" /></a>
			<figcaption>
				Some folders from ZeroTier repository
			</figcaption>
		</figure>
		<p>ZFS deduplication immediately came to our mind, so we made a preliminary test on ZT:</p>
		<div class="language-shell highlighter-rouge">
			<div class="highlight">
				<pre class="highlight"><code>zfs create <span class="nt">-o</span> <span class="nv">dedup</span><span class="o">=</span>on pool0/repo/zerotier
<span class="c"># dump content into it</span>
</code></pre>
			</div>
		</div>
		<div class="language-console highlighter-rouge">
			<div class="highlight">
				<pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>zdb <span class="nt">-DDD</span> pool0
<span class="go">dedup = 4.93, compress = 1.23, copies = 1.00, dedup * compress / copies = 6.04
</span></code></pre>
			</div>
		</div>
		<p>The results look promising, but we are still hesitant to enable deduplication due to the potential performance impact even on these selected datasets.</p>
		<p>Guess what we ended up with?</p>
		<div class="language-shell highlighter-rouge">
			<div class="highlight">
				<pre class="highlight"><code><span class="c"># post-sync.sh</span>
<span class="c"># Do file-level deduplication for select repos</span>
<span class="k">case</span> <span class="s2">"</span><span class="nv">$NAME</span><span class="s2">"</span> <span class="k">in
  </span>docker-ce|influxdata|nginx|openresty|proxmox|salt|tailscale|zerotier<span class="p">)</span>
    jdupes <span class="nt">-L</span> <span class="nt">-Q</span> <span class="nt">-r</span> <span class="nt">-q</span> <span class="s2">"</span><span class="nv">$DIR</span><span class="s2">"</span> <span class="p">;;</span>
<span class="k">esac</span>
</code></pre>
			</div>
		</div>
		<p>As attractive as it looks, this userspace file deduplication tool is as good as ZFS can do, but without the performance loss.</p>
		<table>
			<thead>
				<tr>
					<th>Name</th>
					<th>Orig</th>
					<th>Dedup</th>
					<th>Diff</th>
					<th>Ratio</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td>proxmox</td>
					<td>395.4G</td>
					<td>162.6G</td>
					<td>232.9G</td>
					<td>2.43x</td>
				</tr>
				<tr>
					<td>docker-ce</td>
					<td>539.6G</td>
					<td>318.2G</td>
					<td>221.4G</td>
					<td>1.70x</td>
				</tr>
				<tr>
					<td>influxdata</td>
					<td>248.4G</td>
					<td>54.8G</td>
					<td>193.6G</td>
					<td>4.54x</td>
				</tr>
				<tr>
					<td>salt</td>
					<td>139.0G</td>
					<td>87.2G</td>
					<td>51.9G</td>
					<td>1.59x</td>
				</tr>
				<tr>
					<td>nginx</td>
					<td>94.9G</td>
					<td>59.7G</td>
					<td>35.2G</td>
					<td>1.59x</td>
				</tr>
				<tr>
					<td>zerotier</td>
					<td>29.8G</td>
					<td>6.1G</td>
					<td>23.7G</td>
					<td>4.88x</td>
				</tr>
				<tr>
					<td>mysql-repo</td>
					<td>647.8G</td>
					<td>632.5G</td>
					<td>15.2G</td>
					<td>1.02x</td>
				</tr>
				<tr>
					<td>openresty</td>
					<td>65.1G</td>
					<td>53.4G</td>
					<td>11.7G</td>
					<td>1.22x</td>
				</tr>
				<tr>
					<td>tailscale</td>
					<td>17.9G</td>
					<td>9.0G</td>
					<td>9.0G</td>
					<td>2.00x</td>
				</tr>
			</tbody>
		</table>
		<p>We decided to exclude <code class="language-plaintext highlighter-rouge">mysql-repo</code> as the deduplication ratio is too low to justify the I/O load after each sync.</p>
		<h2 id="conclusion">Conclusion</h2>
		<p>ZFS solved a number of problems we had with our mirror servers, and with the current setup, we are delighted to announce that ZFS is <em>the</em> best solution for mirrors.</p>
		<p>With ZFS:</p>
		<ul>
			<li>We no longer need to worry about partitioning, as ZFS can grow and shrink as needed.</li>
			<li>Our HDD array is now running faster than SSDs. Amazing!
				<ul>
					<li>Be the first one to no longer <strong>envy</strong> TUNA’s SSD server!</li>
				</ul>
			</li>
			<li>Extra capacity at no cost, thanks to ZFS compression.
				<ul>
					<li>Even more so with deduplication.</li>
				</ul>
			</li>
		</ul>
		<h3 id="considerations">Considerations</h3>
		<p>While our ZFS looks very promising, we’re aware that ZFS is not known for its long-term performance stability due to fragmentation.
			We’ll continue to monitor our servers and see if this performance is sustainable.</p>
		]]></content><author><name>iBug</name></author><category term="linux" /><category term="server" /><category term="zfs" /><summary type="html"><![CDATA[A.K.A. Practical ZFS application on USTC Mirrors. A writeup of the talk I gave at Nanjing University this August.]]></summary></entry><entry><title type="html">Make Python 3.12 install user packages without complaints</title><link href="https://ibug.io/blog/2024/09/python3.12-user-packages/" rel="alternate" type="text/html" title="Make Python 3.12 install user packages without complaints" /><published>2024-09-02T00:00:00+00:00</published><updated>2024-09-03T11:59:51+00:00</updated><id>https://ibug.io/blog/2024/09/python3.12-user-packages</id><content type="html" xml:base="https://ibug.io/blog/2024/09/python3.12-user-packages/"><![CDATA[<p>I have a habit of <code class="language-plaintext highlighter-rouge">pip3 install --user</code> and then expecting these packages under <code class="language-plaintext highlighter-rouge">~/.local/lib/</code> to be available for my Python scripts whenever I need them. However, with PEP 668 landing in Python 3.12, I now have to add <code class="language-plaintext highlighter-rouge">--break-system-packages</code> even for <em>user</em> packages. This is super annoying considered that I have multiple projects sharing the same set of common packages (e.g. <a href="https://squidfunk.github.io/mkdocs-material/"><code class="language-plaintext highlighter-rouge">mkdocs-material</code></a>, a nice MkDocs theme). So time to tell <code class="language-plaintext highlighter-rouge">pip</code> to jerk off on that complaint.</p>
	<p>Obviously, aliasing <code class="language-plaintext highlighter-rouge">pip3</code> (as per my personal habit, I always prefer <code class="language-plaintext highlighter-rouge">python3</code> and <code class="language-plaintext highlighter-rouge">pip3</code> over <code class="language-plaintext highlighter-rouge">python</code> and <code class="language-plaintext highlighter-rouge">pip</code>) to <code class="language-plaintext highlighter-rouge">pip3 --break-system-packages</code> could work, with all the limitations that any other shell alias bear.</p>
	<p>The key here is, by examining how virtual environments work, we can trick Python into thinking that <code class="language-plaintext highlighter-rouge">~/.local</code> is one of them. This is already documented in the <a href="https://docs.python.org/3/library/site.html"><code class="language-plaintext highlighter-rouge">site</code> package</a>:</p>
	<blockquote>
		<p>If a file named <code class="language-plaintext highlighter-rouge">pyvenv.cfg</code> exists one directory above <code class="language-plaintext highlighter-rouge">sys.executable</code> …</p>
	</blockquote>
	<p>So here’s the solution, assuming <code class="language-plaintext highlighter-rouge">~/.local/bin</code> is already in your <code class="language-plaintext highlighter-rouge">$PATH</code>:</p>
	<ol>
		<li>Symlink <code class="language-plaintext highlighter-rouge">/usr/bin/python3</code> to <code class="language-plaintext highlighter-rouge">~/.local/bin/python3</code></li>
		<li>Copy <code class="language-plaintext highlighter-rouge">/usr/bin/pip3</code> to <code class="language-plaintext highlighter-rouge">~/.local/bin/pip3</code>, and change the shebang line to <code class="language-plaintext highlighter-rouge">#!/home/example/.local/bin/python3</code> (you’ll have to use the absolute path here, though).</li>
		<li>
			<p>Create <code class="language-plaintext highlighter-rouge">~/.local/pyvenv.cfg</code> with just one line of content:</p>
			<div class="language-ini highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code> <span class="py">include-system-site-packages</span> <span class="p">=</span> <span class="s">true</span>
</code></pre>
				</div>
    </div>
			<p>You can, of course, add other settings for <code class="language-plaintext highlighter-rouge">venv</code>, which is completely optional and up to you.</p>
		</li>
	</ol>
	<p>Now whenever you install something with <code class="language-plaintext highlighter-rouge">pip3</code>, it’ll happily install it under <code class="language-plaintext highlighter-rouge">~/.local/lib/python3.12/site-packages</code> even without the need for <code class="language-plaintext highlighter-rouge">--user</code>.</p>
	<p>If you prefer <code class="language-plaintext highlighter-rouge">python</code> or <code class="language-plaintext highlighter-rouge">pip</code> commands, you can just change the file names in the above steps accordingly.</p>
	<p>Noteworthy points are:</p>
	<ol>
		<li>This method certainly can work for the system Python installation (at <code class="language-plaintext highlighter-rouge">/usr</code>), which I wouldn’t recommend for obvious reasons. If you insist, you should at least do this under <code class="language-plaintext highlighter-rouge">/usr/local</code> instead.</li>
		<li>
			<s>This method (when applied to `~/.local`) is ineffective against scripts already shebanged with `#!/usr/bin/python3`. Consider developing the habit of running `python3 script.py` instead of `./script.py` like I do.</s>
		</li>
	</ol>
	<h2 id="corrigenda">Corrigenda</h2>
	<ul>
		<li>Scripts shebanged with <code class="language-plaintext highlighter-rouge">#!/usr/bin/python3</code> <em>will</em> pick up packages under <code class="language-plaintext highlighter-rouge">~/.local/lib/</code> as this is the default user site directory, which comes in <code class="language-plaintext highlighter-rouge">sys.path</code> even before the system site directory.</li>
	</ul>
	]]></content><author><name>iBug</name></author><category term="linux" /><category term="python" /><summary type="html"><![CDATA[I have a habit of pip3 install --user and then expecting these packages under ~/.local/lib/ to be available for my Python scripts whenever I need them. However, with PEP 668 landing in Python 3.12, I now have to add --break-system-packages even for user packages. This is super annoying considered that I have multiple projects sharing the same set of common packages (e.g. mkdocs-material, a nice MkDocs theme). So time to tell pip to jerk off on that complaint.]]></summary></entry><entry><title type="html">镜像站 ZFS 调优实践</title><link href="https://ibug.io/blog/2024/08/nju-talk/" rel="alternate" type="text/html" title="镜像站 ZFS 调优实践" /><published>2024-08-17T00:00:00+00:00</published><updated>2025-07-11T11:03:43+00:00</updated><id>https://ibug.io/blog/2024/08/nju-talk</id><content type="html" xml:base="https://ibug.io/blog/2024/08/nju-talk/"><![CDATA[<section id="title">
		<h1 class="title">2000 元的机械硬盘 &gt; 3000 元的固态硬盘？</h1>
		<h2 style="font-weight: normal;">A.K.A. 镜像站 ZFS 调优实践</h2>
		<hr />
		<p class="date">iBug @ USTC</p>
		<p class="date">2024 年 8 月 17 日<br />
			南京大学 开源软件论坛</p>
	</section>
	<section>
		<section id="background">
			<h2>USTC Mirrors</h2>
			<ul>
				<li>日均服务量：（2024-05 ~ 2024-06）
					<ul>
						<li>出流量 ~36&nbsp;TiB</li>
						<li>HTTP 请求数 17M，响应流量 19&nbsp;TiB</li>
						<li>Rsync 请求数 147.8K（21.8K），输出流量 10.3&nbsp;TiB</li>
					</ul>
				</li>
				<li>极限情况的仓库容量：
					<ul>
						<li>HTTP 服务器（XFS）：63.3&nbsp;TiB / 66.0&nbsp;TiB (96%, 2023-12-18)</li>
						<li>Rsync 服务器（ZFS）：42.4&nbsp;TiB / 43.2&nbsp;TiB (98%, 2023-11-21)</li>
					</ul>
				</li>
			</ul>
		</section>
		<section id="background-2">
			<h2>背景</h2>
			<ul>
				<li>HTTP 服务器：
					<ul>
						<li>2020 年下半年搭建</li>
						<li>10&nbsp;TB <i class="fas fa-compact-disc fa-spin"></i> &times; 12</li>
						<li>2&nbsp;TB <i class="fas fa-floppy-disk"></i> &times; 1</li>
						<li>XFS on LVM on HW RAID</li>
						<li>考虑到 XFS 不能缩，VG 留了 free PE</li>
					</ul>
				</li>
				<li>Rsync 服务器：
					<ul>
						<li>2016 年下半年搭建</li>
						<li>6&nbsp;TB <i class="fas fa-compact-disc fa-spin"></i> &times; 12</li>
						<li>240&nbsp;GB <i class="fas fa-floppy-disk"></i> &times; 2 + 480&nbsp;GB <i class="fas fa-floppy-disk"></i> &times; 1 (Optane 900p)</li>
						<li>RAID-Z3（8 data + 3 parity + 1 hot spare）</li>
						<li>全默认参数（除了 <code>zfs_arc_max</code>）</li>
					</ul>
				</li>
			</ul>
			<p>硬盘 I/O 日常 &gt; 90%，校内下载 iso 不足 50&nbsp;MB/s</p>
		</section>
		<section id="background-2-image">
			<div class="img-container">
				<img src="https://image.ibugone.com/grafana/mirrors-io-utilization-may-2024.png" />
				<p>USTC 镜像站两台服务器在 2024 年 5 月期间的磁盘负载</p>
			</div>
		</section>
	</section>
	<section>
		<section id="zfs">
			<h2>ZFS</h2>
			<ul>
				<li>单机存储的终极解决方案</li>
				<li>集 RAID、LVM、FS 于一体</li>
				<li>所有数据都有 checksum</li>
				<li><s>Fire and forget</s></li>
				<li>好多参数啊</li>
			</ul>
		</section>
		<section id="zfs-learning">
			<h3>前期学习与实验</h3>
			<ul>
				<li><s>从（另一个）老师那嫖了点盘</s>装上了 ZFS，用于 研&ensp;究&ensp;学&ensp;习</li>
				<li>I/O 负载来源？<s>上 PT 站</s></li>
				<li>练习时长两年半的成果：<i class="fas fa-arrow-up"></i> 1.20&nbsp;PiB, <i class="fas fa-arrow-down"></i> 1.83&nbsp;TiB</li>
			</ul>
			<hr />
			<p>重要学习资料：</p>
			<ul>
				<li><a href="https://utcc.utoronto.ca/~cks/space/blog/">Chris Siebenmann's blog</a></li>
				<li><a href="https://openzfs.github.io/openzfs-docs/">OpenZFS Documentation</a></li>
				<li>iBug's blog: <a href="/p/62">Understanding ZFS block sizes</a> (<a href="/p/62">ibug.io/p/62</a>)
					<ul>
						<li>以及这篇 blog 底下的众多参考文献</li>
					</ul>
				</li>
			</ul>
		</section>
		<section id="zfs-image">
			<div class="img-container">
				<img src="https://image.ibugone.com/grafana/qb/2024-06-05.png" />
				<p>好像给什么奇怪的东西加入了 Grafana</p>
			</div>
		</section>
	</section>
	<section>
		<section id="mirrors">
			<h2>镜像站</h2>
			<ul>
				<li>提供文件下载服务</li>
				<li><s>也提供「家庭宽带上下行流量比例平衡」服务</s></li>
				<li>读多写少，并且几乎所有操作都是整个文件顺序读写</li>
				<li>少量的数据损坏没啥不良后果</li>
			</ul>
		</section>
		<section id="mirrors-file-distrib">
			<div class="img-container">
				<img src="https://image.ibugone.com/server/mirrors-file-size-distribution-2024-08.png" />
				<p>2024 年 8 月 USTC 镜像仓库内的文件大小分布
					<br>
					其中中位数为 9.83&nbsp;KiB，平均大小为 1.60&nbsp;MiB</p>
			</div>
		</section>
		<section id="mirrors2">
			<h3>重建 Rsync 服务器</h3>
			<ul>
				<li>RAID-Z3 的 overhead 较高，而且拆成两组 RAID-Z2 = 两倍的 IOPS</li>
				<li>镜像站调优计划：
					<ul>
						<li><code>recordsize=1M</code>：反正都是全文件顺序读</li>
						<li><code>compression=zstd</code>：至少可以压掉 &gt; 1M 文件的 padding
							<ul>
								<li>OpenZFS 2.2 将 early abort 机制推广到了 Zstd 3+，不必担心性能问题</li>
							</ul>
						</li>
						<li><code>xattr=off</code>：谁家镜像需要 xattr？</li>
						<li><code>atime=off</code>, <code>setuid=off</code>, <code>exec=off</code>, <code>devices=off</code>：开着干啥？</li>
						<li><code>secondarycache=metadata</code>：Rsync 就别来消耗固态寿命了</li>
					</ul>
				</li>
				<li>Danger Zone：
					<ul>
						<li><code>sync=disabled</code>：囤到 <code>zfs_txg_timeout</code> 再写盘</li>
						<li><code>redundant_metadata=some</code>：偶尔坏个文件也没事</li>
					</ul>
				</li>
				<li>Full version: <a href="https://docs.ustclug.org/services/mirrors/zfs/#setup">LUG @ USTC Documentation</a></li>
			</ul>
		</section>
		<section id="zfs-parameters">
			<h3>ZFS 参数</h3>
			<ul>
				<li>290+ 参数不能个个都学习嘛（感谢 Aron Xu @ BFSU）</li>
				<li>ARC 容量：
					<pre><code class="language-sh" data-trim>
# Set ARC size to 160-200&nbsp;GiB, keep 16&nbsp;GiB free for OS
options zfs zfs_arc_max=214748364800
options zfs zfs_arc_min=171798691840
options zfs zfs_arc_sys_free=17179869184
        </code></pre>
				</li>
				<li>ARC 内容：
					<pre><code class="language-sh" data-trim>
# Favor metadata to data by 20x (OpenZFS 2.2+)
options zfs zfs_arc_meta_balance=2000

# Allow up to 80% of ARC to be used for dnodes
options zfs zfs_arc_dnode_limit_percent=80
        </code></pre>
				</li>
				<li>I/O 队列深度：
					<pre><code class="language-sh" data-trim>
# See man page section "ZFS I/O Scheduler"
options zfs zfs_vdev_async_read_max_active=8
options zfs zfs_vdev_async_read_min_active=2
options zfs zfs_vdev_scrub_max_active=5
options zfs zfs_vdev_max_active=20000
        </code></pre>
				</li>
				<li>Full version: <a href="https://docs.ustclug.org/services/mirrors/zfs/#zfs-kernel-module">LUG @ USTC Documentation</a></li>
			</ul>
		</section>
		<section id="mirrors2-rebuild-results">
			<h3>重建成果</h3>
			<ul>
				<li>略感惊喜的压缩率：39.5T / 37.1T = 1.07x
					<ul>
						<li>正确用法：<code>zfs list -po name,logicalused,used</code></li>
						<li>实际压缩率：1 + 6.57%（-2.67&nbsp;TB / -2.43&nbsp;TiB）</li>
						<li><s>等于删了 <a href="https://image.ibugone.com/teaser/lenovo-legion-wechat-data.jpg">9 份微信数据</a></s></li>
					</ul>
				</li>
				<li>合理的磁盘 I/O</li>
			</ul>
		</section>
		<section id="mirrors2-io-image">
			<div class="img-container">
				<img src="https://image.ibugone.com/grafana/mirrors2-io-utilization-and-free-space-june-july-2024.png" />
				<p>重建前后 Rsync 服务器的磁盘负载与空闲空间比较</p>
			</div>
		</section>
	</section>
	<section>
		<section id="mirrors4">
			<h2>HTTP 服务器</h2>
			<ul>
				<li>硬件 RAID + LVM + XFS + Kernel page cache（开箱即用？）</li>
				<li>SSD？LVMCache！
					<ul>
						<li>1M extents? Block size? Algorithm?</li>
						<li><i class="fas fa-skull"></i> GRUB2</li>
						<li><i class="fas fa-skull"></i> "oldssd"</li>
					</ul>
				</li>
				<li>XFS 不能缩，所以 VG 和 FS 两层都要留空间备用</li>
			</ul>
		</section>
		<section id="mirrors4-dmcache-image">
			<div class="img-container">
				<img src="https://image.ibugone.com/grafana/mirrors4-dmcache-may-june-2024.png" />
				<p>重建前 HTTP 服务器采用的 LVMcache 方案的命中率</p>
			</div>
		</section>
		<section id="mirrors4-rebuild">
			<h2>如法炮制</h2>
			<ul>
				<li>体验一下更加先进的 kernel：<code>6.8.8-3-pve</code>（无需 DKMS 哦）</li>
				<li>重建为两组 RAID-Z2，开压缩
					<ul>
						<li>面向 HTTP 用户的服务器，所以 <code>secondarycache=all</code>（放着不动）</li>
						<li>更好的 CPU，所以 <code>compression=zstd-8</code></li>
					</ul>
				</li>
				<li>更快的 <code>zfs send -Lcp</code>：36 小时倒完 50+ TiB 仓库</li>
				<li>压缩率：1 + 3.93%（-2.42&nbsp;TB / -2.20&nbsp;TiB）</li>
			</ul>
		</section>
		<section id="mirrors2-4-io-image">
			<div class="img-container">
				<img src="https://image.ibugone.com/grafana/mirrors2-4-io-utilization-june-july-2024.png" />
				<p>重建前后两台服务器的磁盘负载比较
					<br>
					左边为重建前，中间为仅 Rsync 服务器重建后，右边为两台服务器均重建后的负载</p>
			</div>
		</section>
		<section id="mirrors2-4-zfs-arc-image">
			<div class="img-container">
				<img src="https://image.ibugone.com/grafana/mirrors2-4-zfs-arc-hit-rate.png" />
				<p>两台服务器的 ZFS ARC 命中率</p>
			</div>
		</section>
		<section id="mirrors2-4-recent-io-image">
			<div class="img-container">
				<img src="https://image.ibugone.com/grafana/mirrors2-4-disk-io-after-rebuild.png" />
				<p>两台服务器重建后稳定的磁盘利用率</p>
			</div>
		</section>
	</section>
	<section>
		<section id="misc">
			<h2>杂项</h2>
		</section>
		<section id="zfs-compressratio">
			<h3>ZFS 压缩率</h3>
			<table>
				<thead>
					<tr>
						<th>NAME</th>
						<th>LUSED</th>
						<th>USED</th>
						<th>RATIO</th>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td>pool0/repo/crates.io-index</td>
						<td>2.19G</td>
						<td>1.65G</td>
						<td>3.01x</td>
					</tr>
					<tr>
						<td>pool0/repo/elpa</td>
						<td>3.35G</td>
						<td>2.32G</td>
						<td>1.67x</td>
					</tr>
					<tr>
						<td>pool0/repo/rfc</td>
						<td>4.37G</td>
						<td>3.01G</td>
						<td>1.56x</td>
					</tr>
					<tr>
						<td>pool0/repo/debian-cdimage</td>
						<td>1.58T</td>
						<td>1.04T</td>
						<td>1.54x</td>
					</tr>
					<tr>
						<td>pool0/repo/tldp</td>
						<td>4.89G</td>
						<td>3.78G</td>
						<td>1.48x</td>
					</tr>
					<td>pool0/repo/loongnix</td>
					<td>438G</td>
					<td>332G</td>
					<td>1.34x</td>
				</tr>
				<tr>
					<td>pool0/repo/rosdistro</td>
					<td>32.2M</td>
					<td>26.6M</td>
					<td>1.31x</td>
				</tr>
				<tr>
				</tbody>
			</table>
			<p>我数学不好：<a href="https://github.com/openzfs/zfs/issues/7639"><i class="fab fa-github"></i> openzfs/zfs#7639</a></p>
		</section>
		<section id="zfs-compressratio-diff">
			<h3>ZFS 压缩量</h3>
			<table>
				<thead>
					<tr>
						<th>NAME</th>
						<th>LUSED</th>
						<th>USED</th>
						<th>DIFF</th>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td>pool0/repo</td>
						<td>58.3T</td>
						<td>56.1T</td>
						<td>2.2T</td>
					</tr>
					<tr>
						<td>pool0/repo/debian-cdimage</td>
						<td>1.6T</td>
						<td>1.0T</td>
						<td>549.6G</td>
					</tr>
					<tr>
						<td>pool0/repo/opensuse</td>
						<td>2.5T</td>
						<td>2.3T</td>
						<td>279.7G</td>
					</tr>
					<tr>
						<td>pool0/repo/turnkeylinux</td>
						<td>1.2T</td>
						<td>1.0T</td>
						<td>155.2G</td>
					</tr>
					<tr>
						<td>pool0/repo/loongnix</td>
						<td>438.2G</td>
						<td>331.9G</td>
						<td>106.3G</td>
					</tr>
					<tr>
						<td>pool0/repo/alpine</td>
						<td>3.0T</td>
						<td>2.9T</td>
						<td>103.9G</td>
					</tr>
					<tr>
						<td>pool0/repo/openwrt</td>
						<td>1.8T</td>
						<td>1.7T</td>
						<td>70.0G</td>
					</tr>
				</tbody>
			</table>
		</section>
		<section id="grafana-zfs-io">
			<h3>Grafana I/O 统计</h3>
			<pre><code data-trim>
SELECT
  non_negative_derivative(sum("reads"), 1s) AS "read",
  non_negative_derivative(sum("writes"), 1s) AS "write"
FROM (
  SELECT
    first("reads") AS "reads",
    first("writes") AS "writes"
  FROM "zfs_pool"
  WHERE ("host" = 'taokystrong' AND "pool" = 'pool0') AND $timeFilter
  GROUP BY time($interval), "host"::tag, "pool"::tag, "dataset"::tag fill(null)
)
WHERE $timeFilter
GROUP BY time($interval), "pool"::tag fill(linear)
</code></pre>
			<p>跑得有点慢（毕竟要先 <code>GROUP BY</code> 每个 ZFS dataset 再一起 <code>sum</code>）</p>
			<p>I/O 带宽：把里层的 <code>reads</code> 和 <code>writes</code> 换成 <code>nread</code> 和 <code>nwritten</code> 即可</p>
		</section>
		<section id="grafana-zfs-io-image">
			<div class="img-container">
				<img src="https://image.ibugone.com/grafana/mirrors2-4-zfs-io-count.png" />
			</div>
			<p></p>
			<ul>
				<li>如何用机械盘跑出平均 15K、最高 50K 的 IOPS？</li>
				<li><s>把 ARC hit 算进去</s></li>
			</ul>
		</section>
	</section>
	<section>
		<section id="hearse">
			<h2>灵车时间</h2>
		</section>
		<section id="pve-kernel">
			<h2>Proxmox Kernel</h2>
			<ul>
				<li>≈ Ubuntu Kernel</li>
				<li><i class="fas fa-skull"></i> Rsync 容器</li>
				<li><code>security/apparmor/af_unix.c</code>???</li>
				<li><a href="https://docs.ustclug.org/faq/apparmor/">LUG Documentation: AppArmor</a></li>
			</ul>
			<pre><code class="language-sh" data-trim>
dpkg-divert --package lxc-pve --rename --divert /usr/share/apparmor-features/features.stock --add /usr/share/apparmor-features/features
wget -O /usr/share/apparmor-features/features https://github.com/proxmox/lxc/raw/master/debian/features
    </code></pre>
		</section>
		<section id="zerotier-data">
			<div class="img-container">
				<img src="https://image.ibugone.com/server/ls-zerotier-redhar-el.png" />
				<p>ZeroTier 仓库中一眼重复的内容</p>
			</div>
		</section>
		<section id="dedup">
			<h3>Dedup!</h3>
			<pre><code class="language-sh">zfs create -o dedup=on pool0/repo/zerotier</code></pre>
			<pre><code class="language-sh" data-trim>
# zdb -DDD pool0
dedup = 4.93, compress = 1.23, copies = 1.00, dedup * compress / copies = 6.04
    </code></pre>
			<p>效果倒是不错，但是不想像 ZFS dedup 这么灵怎么办？</p>
		</section>
		<section id="jdupes">
			<h3>jdupes</h3>
			<pre><code class="language-sh" data-trim>
# post-sync.sh
# Do file-level deduplication for select repos
case "$NAME" in
  docker-ce|influxdata|nginx|openresty|proxmox|salt|tailscale|zerotier)
    jdupes -L -Q -r -q "$DIR" ;;
esac
    </code></pre>
		</section>
		<section id="jdupes-table">
			<h3>jdupes 效果</h3>
			<table>
				<thead>
					<tr>
						<th>Name</th>
						<th>Orig</th>
						<th>Dedup</th>
						<th>Diff</th>
						<th>Ratio</th>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td>proxmox</td>
						<td>395.4G</td>
						<td>162.6G</td>
						<td>232.9G</td>
						<td>2.43x</td>
					</tr>
					<tr>
						<td>docker-ce</td>
						<td>539.6G</td>
						<td>318.2G</td>
						<td>221.4G</td>
						<td>1.70x</td>
					</tr>
					<tr>
						<td>influxdata</td>
						<td>248.4G</td>
						<td>54.8G</td>
						<td>193.6G</td>
						<td>4.54x</td>
					</tr>
					<tr>
						<td>salt</td>
						<td>139.0G</td>
						<td>87.2G</td>
						<td>51.9G</td>
						<td>1.59x</td>
					</tr>
					<tr>
						<td>nginx</td>
						<td>94.9G</td>
						<td>59.7G</td>
						<td>35.2G</td>
						<td>1.59x</td>
					</tr>
					<tr>
						<td>zerotier</td>
						<td>29.8G</td>
						<td>6.1G</td>
						<td>23.7G</td>
						<td>4.88x</td>
					</tr>
					<tr>
						<td>mysql-repo</td>
						<td>647.8G</td>
						<td>632.5G</td>
						<td>15.2G</td>
						<td>1.02x</td>
					</tr>
					<tr>
						<td>openresty</td>
						<td>65.1G</td>
						<td>53.4G</td>
						<td>11.7G</td>
						<td>1.22x</td>
					</tr>
					<tr>
						<td>tailscale</td>
						<td>17.9G</td>
						<td>9.0G</td>
						<td>9.0G</td>
						<td>2.00x</td>
					</tr>
				</tbody>
			</table>
		</section>
	</section>
	<section id="conclusion">
		<h2>只要 ZFS 用得好</h2>
		<ul>
			<li>妈妈再也不用担心我的硬盘分区</li>
			<li>机械硬盘 <s>比西方的固态硬盘跑得还快</s></li>
			<li>成为第一个不再<b>羡慕</b> TUNA 全闪的镜像站</li>
			<li>免费的额外容量
				<ul>
					<li>Dedup 会员红包</li>
				</ul>
			</li>
			<li>碎片率？</li>
		</ul>
	</section>
	<section id="outro">
		<h1>谢谢！</h1>
		<small>
			<p>本页面的链接：<a href="/p/72"><i class="fas fa-fw fa-link"></i> ibug.io/p/72</a></p>
			<p>
				<a href="https://lug.ustc.edu.cn/planet/2024/12/ustc-mirrors-zfs-rebuild/"><i class="fas fa-fw fa-link"></i> 中文文章</a>
				|
				<a href="/p/74"><i class="fas fa-fw fa-link"></i> English article</a>
			</p>
			<p>友情链接：2023 年南京大学报告：<a href="/p/59"><i class="fas fa-fw fa-link"></i> ibug.io/p/59</a></p>
		</small>
	</section>
	]]></content><author><name>iBug</name></author></entry><entry><title type="html">Why my IPv4 gets stuck? - Debugging network issues with bpftrace</title><link href="https://ibug.io/blog/2024/08/first-touch-bpftrace/" rel="alternate" type="text/html" title="Why my IPv4 gets stuck? - Debugging network issues with bpftrace" /><published>2024-08-03T00:00:00+00:00</published><updated>2024-08-03T03:22:32+00:00</updated><id>https://ibug.io/blog/2024/08/first-touch-bpftrace</id><content type="html" xml:base="https://ibug.io/blog/2024/08/first-touch-bpftrace/"><![CDATA[<p>I run a Debian-based software router on my home network. It’s connected to multiple ISPs, so I have some policy routing rules to balance the traffic between them. Some time ago, I noticed that the IPv4 connectivity got stuck intermittently when it didn’t use to, while IPv6 was working fine. It’s also interesting that the issue only happened with one specific ISP, in the egress direction, and only a few specific devices were affected.</p>
	<p>At first I suspected the ISP’s equipment, but a clue quickly dismissed that suspicion: Connection to the same ISP worked fine when initiated from the router itself, as well as many other unaffected devices. So the issue must be within the router.</p>
	<p>As usual, every network debugging begins with a packet capture. I start <code class="language-plaintext highlighter-rouge">tcpdump</code> on both the LAN interface and the problematic WAN interface, then try <code class="language-plaintext highlighter-rouge">curl</code>-ing something from an affected device. Packet capture shows a few back-and-forth packets, then the device keeps sending packets but the router doesn’t forward them to the WAN interface anymore. Time for a closer look.</p>
	<h2 id="identifying-the-issue">Identifying the issue</h2>
	<p>On an affected device, <code class="language-plaintext highlighter-rouge">curl</code> gets stuck somewhere in the middle:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>curl <span class="nt">-vso</span> /dev/null https://www.cloudflare.com/
<span class="go">*   Trying 104.16.124.96:443...
</span><span class="gp">* Connected to www.cloudflare.com (104.16.124.96) port 443 (#</span>0<span class="o">)</span>
<span class="go">* ALPN: offers h2,http/1.1
} [5&nbsp;bytes data]
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
} [512&nbsp;bytes data]
*  CAfile: /etc/ssl/certs/ca-certificates.crt
*  CApath: /etc/ssl/certs
{ [5&nbsp;bytes data]
* TLSv1.3 (IN), TLS handshake, Server hello (2):
{ [122&nbsp;bytes data]
^C
</span></code></pre>
		</div>
	</div>
	<p><code class="language-plaintext highlighter-rouge">tcpdump</code> shows nothing special:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>tcpdump <span class="nt">-ni</span> any <span class="s1">'host 104.16.124.96 and tcp port 443'</span>
<span class="gp">02:03:47.403905 lan0 In  IP 172.17.0.2.49194 &gt;</span><span class="w"> </span>104.16.124.96.443: Flags <span class="o">[</span>S], <span class="nb">seq </span>1854398703, win 65535, options <span class="o">[</span>mss 1460,sackOK,TS val 1651776756 ecr 0,nop,wscale 10], length 0
<span class="gp">02:03:47.403956 ppp0 Out IP 10.250.193.4.49194 &gt;</span><span class="w"> </span>104.16.124.96.443: Flags <span class="o">[</span>S], <span class="nb">seq </span>1854398703, win 65535, options <span class="o">[</span>mss 1432,sackOK,TS val 1651776756 ecr 0,nop,wscale 10], length 0
<span class="gp">02:03:47.447663 ppp0 In  IP 104.16.124.96.443 &gt;</span><span class="w"> </span>10.250.193.4.49194: Flags <span class="o">[</span>S.], <span class="nb">seq </span>1391350792, ack 1854398704, win 65535, options <span class="o">[</span>mss 1460,sackOK,TS val 141787839 ecr 1651776756,nop,wscale 13], length 0
<span class="gp">02:03:47.447696 lan0 Out IP 104.16.124.96.443 &gt;</span><span class="w"> </span>172.17.0.2.49194: Flags <span class="o">[</span>S.], <span class="nb">seq </span>1391350792, ack 1854398704, win 65535, options <span class="o">[</span>mss 1460,sackOK,TS val 141787839 ecr 1651776756,nop,wscale 13], length 0
<span class="gp">02:03:47.447720 lan0 In  IP 172.17.0.2.49194 &gt;</span><span class="w"> </span>104.16.124.96.443: Flags <span class="o">[</span>.], ack 1, win 64, options <span class="o">[</span>nop,nop,TS val 1651776800 ecr 141787839], length 0
<span class="gp">02:03:47.452705 lan0 In  IP 172.17.0.2.49194 &gt;</span><span class="w"> </span>104.16.124.96.443: Flags <span class="o">[</span>P.], <span class="nb">seq </span>1:518, ack 1, win 64, options <span class="o">[</span>nop,nop,TS val 1651776804 ecr 141787839], length 517
<span class="gp">02:03:47.452751 ppp0 Out IP 10.250.193.4.49194 &gt;</span><span class="w"> </span>104.16.124.96.443: Flags <span class="o">[</span>P.], <span class="nb">seq </span>1:518, ack 1, win 64, options <span class="o">[</span>nop,nop,TS val 1651776804 ecr 141787839], length 517
<span class="gp">02:03:47.496507 ppp0 In  IP 104.16.124.96.443 &gt;</span><span class="w"> </span>10.250.193.4.49194: Flags <span class="o">[</span>.], ack 518, win 9, options <span class="o">[</span>nop,nop,TS val 141787888 ecr 1651776804], length 0
<span class="gp">02:03:47.496527 lan0 Out IP 104.16.124.96.443 &gt;</span><span class="w"> </span>172.17.0.2.49194: Flags <span class="o">[</span>.], ack 518, win 9, options <span class="o">[</span>nop,nop,TS val 141787888 ecr 1651776804], length 0
<span class="gp">02:03:47.498147 ppp0 In  IP 104.16.124.96.443 &gt;</span><span class="w"> </span>10.250.193.4.49194: Flags <span class="o">[</span>P.], <span class="nb">seq </span>1:2737, ack 518, win 9, options <span class="o">[</span>nop,nop,TS val 141787890 ecr 1651776804], length 2736
<span class="gp">02:03:47.498165 lan0 Out IP 104.16.124.96.443 &gt;</span><span class="w"> </span>172.17.0.2.49194: Flags <span class="o">[</span>P.], <span class="nb">seq </span>1:2737, ack 518, win 9, options <span class="o">[</span>nop,nop,TS val 141787890 ecr 1651776804], length 2736
<span class="gp">02:03:47.498175 lan0 In  IP 172.17.0.2.49194 &gt;</span><span class="w"> </span>104.16.124.96.443: Flags <span class="o">[</span>.], ack 2737, win 70, options <span class="o">[</span>nop,nop,TS val 1651776850 ecr 141787890], length 0
<span class="gp">02:03:47.498195 ppp0 In  IP 104.16.124.96.443 &gt;</span><span class="w"> </span>10.250.193.4.49194: Flags <span class="o">[</span>P.], <span class="nb">seq </span>2737:3758, ack 518, win 9, options <span class="o">[</span>nop,nop,TS val 141787890 ecr 1651776804], length 1021
<span class="gp">02:03:47.498228 ppp0 Out IP 10.250.193.4.49194 &gt;</span><span class="w"> </span>104.16.124.96.443: Flags <span class="o">[</span>R], <span class="nb">seq </span>1854399221, win 0, length 0
<span class="go">^C
711 packets captured
720 packets received by filter
0 packets dropped by kernel
</span></code></pre>
		</div>
	</div>
	<p>Considering the complexity of the policy routing, I tried inspecting conntrack status in parallel. Nothing unusual there either, until I tried matching conntrack events with <code class="language-plaintext highlighter-rouge">tcpdump</code>:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>conntrack <span class="nt">-E</span> <span class="nt">-s</span> 172.17.0.2 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 443 2&gt;/dev/null | ts %.T
<span class="go">02:03:47.404103     [NEW] tcp      6 120 SYN_SENT src=172.17.0.2 dst=104.16.124.96 sport=49194 dport=443 [UNREPLIED] src=104.16.124.96 dst=10.250.193.4 sport=443 dport=49194
02:03:47.447748  [UPDATE] tcp      6 60 SYN_RECV src=172.17.0.2 dst=104.16.124.96 sport=49194 dport=443 src=104.16.124.96 dst=10.250.193.4 sport=443 dport=49194 mark=48
02:03:47.447843 [DESTROY] tcp      6 432000 ESTABLISHED src=172.17.0.2 dst=104.16.124.96 sport=49194 dport=443 src=104.16.124.96 dst=10.250.193.4 sport=443 dport=49194 [ASSURED] mark=48
02:03:47.452798     [NEW] tcp      6 300 ESTABLISHED src=172.17.0.2 dst=104.16.124.96 sport=49194 dport=443 [UNREPLIED] src=104.16.124.96 dst=10.250.193.4 sport=443 dport=49194
02:03:47.496572  [UPDATE] tcp      6 300 src=172.17.0.2 dst=104.16.124.96 sport=49194 dport=443 src=104.16.124.96 dst=10.250.193.4 sport=443 dport=49194 mark=48
02:03:47.498195  [UPDATE] tcp      6 300 src=172.17.0.2 dst=104.16.124.96 sport=49194 dport=443 src=104.16.124.96 dst=10.250.193.4 sport=443 dport=49194 [ASSURED] mark=48
02:03:47.498243 [DESTROY] tcp      6 432000 ESTABLISHED src=172.17.0.2 dst=104.16.124.96 sport=49194 dport=443 src=104.16.124.96 dst=10.250.193.4 sport=443 dport=49194 [ASSURED] mark=48
^C
</span></code></pre>
		</div>
	</div>
	<p>With <code class="language-plaintext highlighter-rouge">ts</code> (from <a href="https://packages.debian.org/stable/moreutils"><code class="language-plaintext highlighter-rouge">moreutils</code></a>) adding timestamps to conntrack events, I can see that the conntrack entry is destroyed right after (+123μs) the second packet comes in from the device. Subsequent packets causes (+93μs) the same conntrack entry to be recreated, so <code class="language-plaintext highlighter-rouge">curl</code> could somehow complete the SSL handshake to a point where it only sends one packet and nothing afterwards for the connection to be recreated for a third time.</p>
	<p>Clearly the second packet should be considered <code class="language-plaintext highlighter-rouge">ESTABLISHED</code> by conntrack and makes no sense to trigger a <code class="language-plaintext highlighter-rouge">DESTROY</code> event. I’m at a loss here and start trying random things hoping to find a clue. I tried downgrading the kernel to 5.10 (from Bullseye) and upgrading to 6.9 (from Bookworm backports), but nothing changed, eliminating the possibility of a kernel bug.</p>
	<p>After scrutinizing my firewall rules, I noticed a small difference between IPv4 and IPv6 rules:</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="c"># rules.v4</span>
<span class="k">*</span>nat
<span class="c"># ...</span>
<span class="nt">-A</span> POSTROUTING <span class="nt">-o</span> ppp+ <span class="nt">-j</span> MASQUERADE
COMMIT

<span class="k">*</span>mangle
:PREROUTING ACCEPT <span class="o">[</span>0:0]
<span class="c"># ...</span>
<span class="nt">-A</span> PREROUTING <span class="nt">-j</span> CONNMARK <span class="nt">--restore-mark</span>
<span class="nt">-A</span> PREROUTING <span class="nt">-m</span> mark <span class="o">!</span> <span class="nt">--mark</span> 0 <span class="nt">-j</span> ACCEPT
<span class="c">#A PREROUTING -m conntrack --ctstate NEW,RELATED -j MARK --set-xmark 0x100/0x100</span>
<span class="nt">-A</span> PREROUTING <span class="nt">-m</span> mark <span class="nt">--mark</span> 0/0xff <span class="nt">-j</span> ExtraConn
<span class="nt">-A</span> PREROUTING <span class="nt">-m</span> mark <span class="nt">--mark</span> 0/0xff <span class="nt">-j</span> IntraConn
<span class="nt">-A</span> PREROUTING <span class="nt">-m</span> mark <span class="nt">--mark</span> 0/0xff <span class="nt">-j</span> MARK <span class="nt">--set-xmark</span> 0x30/0xff
<span class="nt">-A</span> PREROUTING <span class="nt">-j</span> CONNMARK <span class="nt">--save-mark</span>
<span class="c"># ...</span>
<span class="nt">-A</span> ExtraConn <span class="nt">-i</span> ppp0 <span class="nt">-j</span> MARK <span class="nt">--set-xmark</span> 0x30/0xff
<span class="c"># ...</span>
<span class="nt">-A</span> IntraConn <span class="nt">-s</span> 172.17.0.2/32 <span class="nt">-j</span> iBugOptimized
<span class="c"># ...</span>
<span class="nt">-A</span> iBugOptimized <span class="nt">-j</span> MARK <span class="nt">--set-xmark</span> 0x36/0xff
<span class="nt">-A</span> iBugOptimized <span class="nt">-j</span> ACCEPT
COMMIT
</code></pre>
		</div>
	</div>
	<p>However, <code class="language-plaintext highlighter-rouge">rules.v6</code> is missing the last rule in <code class="language-plaintext highlighter-rouge">iBugOptimized</code>, and IPv6 is somehow exempt from the conntrack issue. Removing this extra <code class="language-plaintext highlighter-rouge">ACCEPT</code> rule from <code class="language-plaintext highlighter-rouge">rules.v4</code> fully restores the connectivity. So this is certainly the cause, but how is it related to the actual issue?</p>
	<h2 id="investigation">Investigation</h2>
	<p><em>I know there are some decent tools on GitHub that aids in debugging iptables, which is notorious for its complexity. But since I wrote the entire firewall rule set and am still maintaining it by hand, I’m going for the hard route of watching and understanding every single rule.</em></p>
	<p>The difference for that single <code class="language-plaintext highlighter-rouge">ACCEPT</code> rule is, it skips the <code class="language-plaintext highlighter-rouge">--save-mark</code> step, so the assigned firewall mark is not saved to its corresponding conntrack entry. When a reply packet comes in, conntrack has nothing for the <code class="language-plaintext highlighter-rouge">--restore-mark</code> step, so the packet gets assigned the “default” mark of <code class="language-plaintext highlighter-rouge">0x30</code> and <em>then</em> this value gets saved. I should have noticed the wrong conntrack mark earlier, as <code class="language-plaintext highlighter-rouge">conntrack -L</code> clearly showed a mark of 48 instead of the intended 54 (<code class="language-plaintext highlighter-rouge">0x36</code> from <code class="language-plaintext highlighter-rouge">iBugOptimized</code>). This narrows the cause down to a discrepancy between the packet mark and the conntrack mark.</p>
	<p>Firewall marks are a more flexible way to implement slightly complicated policy-based routing, as it defers the routing decision to the <code class="language-plaintext highlighter-rouge">mangle/PREROUTING</code> chain instead of the single-chain global routing rules. In my case, every ISP gets assigned a fwmark routing rule like this:</p>
	<div class="language-text highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>9:      from all fwmark 0x30/0xff lookup eth0 proto static
9:      from all fwmark 0x31/0xff lookup eth1 proto static
9:      from all fwmark 0x36/0xff lookup ppp0 proto static
</code></pre>
		</div>
	</div>
	<p>Presumably, subsequent packets from the same connection should be routed to <code class="language-plaintext highlighter-rouge">eth0</code> because it has the mark <code class="language-plaintext highlighter-rouge">0x30</code> restored from conntrack entry. This is not the case, however, as <code class="language-plaintext highlighter-rouge">tcpdump</code> shows nothing on <code class="language-plaintext highlighter-rouge">eth0</code> and everything on <code class="language-plaintext highlighter-rouge">ppp0</code>.</p>
	<p>Unless there’s some magic in the kernel for it to decide to destroy a connection simply for a packet mark mismatch, this is not close enough to the root cause. Verifying the magic is relatively easy:</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>iptables <span class="nt">-I</span> PREROUTING <span class="nt">-s</span> 172.17.0.2/32 <span class="nt">-j</span> Test
iptables <span class="nt">-A</span> Test <span class="nt">-m</span> conntrack <span class="nt">--ctstate</span> NEW <span class="nt">-j</span> MARK <span class="nt">--set-xmark</span> 0x36/0xff
iptables <span class="nt">-A</span> Test <span class="nt">-m</span> conntrack <span class="nt">--ctstate</span> ESTABLISHED <span class="nt">-j</span> MARK <span class="nt">--set-xmark</span> 0x30/0xff
</code></pre>
		</div>
	</div>
	<p>This time, even if <code class="language-plaintext highlighter-rouge">conntrack</code> shows no mark (i.e. zero) on the connection, the packets are still routed correctly to <code class="language-plaintext highlighter-rouge">ppp0</code>, and curl gets stuck as the same place as before. So the kernel doesn’t care about the conntrack mark at all.</p>
	<p>Unfortunately, this is about as far as userspace inspection can go. I need to find out why exactly the kernel decides to destroy the conntrack entry.</p>
	<h2 id="bpftrace-comes-in"><code class="language-plaintext highlighter-rouge">bpftrace</code> comes in</h2>
	<p>I’ve seen professional kernel network developers extensively running <code class="language-plaintext highlighter-rouge">bpftrace</code> to debug network issues (THANK YOU to the guy behind the Telegram channel <em>Welcome to the Black Parade</em>), so I’m giving it a try.</p>
	<p>First thing is to figure out what to hook. Searching through Google did not reveal a trace point for conntrack events, but I get to know about the conntrack path. With help from ChatGPT, I begin with <code class="language-plaintext highlighter-rouge">kprobe:nf_ct_delete</code> and putting together all struct definitions starting from <code class="language-plaintext highlighter-rouge">struct nf_conn</code>:</p>
	<div class="language-c highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;linux/socket.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;net/netfilter/nf_conntrack.h&gt;</span><span class="cp">
</span>
<span class="n">kprobe</span><span class="o">:</span><span class="n">nf_ct_delete</span>
<span class="p">{</span>
    <span class="c1">// The first argument is the struct nf_conn</span>
    <span class="err">$</span><span class="n">ct</span> <span class="o">=</span> <span class="p">(</span><span class="k">struct</span> <span class="n">nf_conn</span> <span class="o">*</span><span class="p">)</span><span class="n">arg0</span><span class="p">;</span>

    <span class="c1">// Check if the connection is for IPv4</span>
    <span class="k">if</span> <span class="p">(</span><span class="err">$</span><span class="n">ct</span><span class="o">-&gt;</span><span class="n">tuplehash</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">tuple</span><span class="p">.</span><span class="n">src</span><span class="p">.</span><span class="n">l3num</span> <span class="o">==</span> <span class="n">AF_INET</span><span class="p">)</span> <span class="p">{</span>
        <span class="err">$</span><span class="n">src_ip</span> <span class="o">=</span> <span class="err">$</span><span class="n">ct</span><span class="o">-&gt;</span><span class="n">tuplehash</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">tuple</span><span class="p">.</span><span class="n">src</span><span class="p">.</span><span class="n">u3</span><span class="p">.</span><span class="n">ip</span><span class="p">;</span>
        <span class="err">$</span><span class="n">dst_ip</span> <span class="o">=</span> <span class="err">$</span><span class="n">ct</span><span class="o">-&gt;</span><span class="n">tuplehash</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">tuple</span><span class="p">.</span><span class="n">dst</span><span class="p">.</span><span class="n">u3</span><span class="p">.</span><span class="n">ip</span><span class="p">;</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"Conntrack destroyed (IPv4): src=%s dst=%s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span>
                <span class="n">ntop</span><span class="p">(</span><span class="err">$</span><span class="n">src_ip</span><span class="p">),</span> <span class="n">ntop</span><span class="p">(</span><span class="err">$</span><span class="n">dst_ip</span><span class="p">));</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre>
		</div>
	</div>
	<p>Seems all good, except it won’t compile:</p>
	<div class="language-text highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>ERROR: Can not access field 'u3' on expression of type 'none'
        $dst_ip = $ct-&gt;tuplehash[0].tuple.dst.u3.ip;
                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
</code></pre>
		</div>
	</div>
	<p>After another half-hour of struggling and bothering with ChatGPT, I gave up trying to access the destination tuple, and thought I’d be fine with inspecting the stack trace:</p>
	<div class="language-c highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;linux/socket.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;net/netfilter/nf_conntrack.h&gt;</span><span class="cp">
</span>
<span class="n">kprobe</span><span class="o">:</span><span class="n">nf_ct_delete</span>
<span class="p">{</span>
    <span class="c1">// The first argument is the struct nf_conn</span>
    <span class="err">$</span><span class="n">ct</span> <span class="o">=</span> <span class="p">(</span><span class="k">struct</span> <span class="n">nf_conn</span> <span class="o">*</span><span class="p">)</span><span class="n">arg0</span><span class="p">;</span>

    <span class="c1">// Check if the connection is for IPv4</span>
    <span class="k">if</span> <span class="p">(</span><span class="err">$</span><span class="n">ct</span><span class="o">-&gt;</span><span class="n">tuplehash</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">tuple</span><span class="p">.</span><span class="n">src</span><span class="p">.</span><span class="n">l3num</span> <span class="o">==</span> <span class="n">AF_INET</span><span class="p">)</span> <span class="p">{</span>
        <span class="err">$</span><span class="n">tuple_orig</span> <span class="o">=</span> <span class="err">$</span><span class="n">ct</span><span class="o">-&gt;</span><span class="n">tuplehash</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">tuple</span><span class="p">;</span>
        <span class="err">$</span><span class="n">src_ip</span> <span class="o">=</span> <span class="err">$</span><span class="n">tuple_orig</span><span class="p">.</span><span class="n">src</span><span class="p">.</span><span class="n">u3</span><span class="p">.</span><span class="n">ip</span><span class="p">;</span>
        <span class="err">$</span><span class="n">src_port_n</span> <span class="o">=</span> <span class="err">$</span><span class="n">tuple_orig</span><span class="p">.</span><span class="n">src</span><span class="p">.</span><span class="n">u</span><span class="p">.</span><span class="n">all</span><span class="p">;</span>
        <span class="err">$</span><span class="n">src_port</span> <span class="o">=</span> <span class="p">(</span><span class="err">$</span><span class="n">src_port_n</span> <span class="o">&gt;&gt;</span> <span class="mi">8</span><span class="p">)</span> <span class="o">|</span> <span class="p">((</span><span class="err">$</span><span class="n">src_port_n</span> <span class="o">&lt;&lt;</span> <span class="mi">8</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0x00FF00</span><span class="p">);</span>
        <span class="k">if</span> <span class="p">(</span><span class="err">$</span><span class="n">src_ip</span> <span class="o">!=</span> <span class="mh">0x020011ac</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">return</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="err">$</span><span class="n">mark</span> <span class="o">=</span> <span class="err">$</span><span class="n">ct</span><span class="o">-&gt;</span><span class="n">mark</span><span class="p">;</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"Conntrack destroyed (IPv4): src=%s sport=%d mark=%d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span>
                <span class="n">ntop</span><span class="p">(</span><span class="err">$</span><span class="n">src_ip</span><span class="p">),</span> <span class="err">$</span><span class="n">src_port</span><span class="p">,</span> <span class="err">$</span><span class="n">mark</span><span class="p">);</span>

        <span class="n">printf</span><span class="p">(</span><span class="s">"%s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">kstack</span><span class="p">());</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre>
		</div>
	</div>
	<p>Noteworthy is that I have to filter the connections in the program, otherwise my screen gets flooded with unrelated events.</p>
	<p>The output comes promising:</p>
	<div class="language-text highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>Attaching 1 probe...
Conntrack destroyed (IPv4): src=172.17.0.2 sport=39456 mark=0 proto=6

    nf_ct_delete+1
    nf_nat_inet_fn+188
    nf_nat_ipv4_out+80
    nf_hook_slow+70
    ip_output+220
    ip_forward_finish+132
    ip_forward+1296
    ip_rcv+404
    __netif_receive_skb_one_core+145
    __netif_receive_skb+21
    netif_receive_skb+300
    ...
</code></pre>
		</div>
	</div>
	<p>Reading the source code from the top few functions of the call stack:</p>
	<div class="language-c highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="c1">// net/netfilter/nf_nat_proto.c</span>

<span class="k">static</span> <span class="kt">unsigned</span> <span class="kt">int</span>
<span class="nf">nf_nat_ipv4_out</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">priv</span><span class="p">,</span> <span class="k">struct</span> <span class="n">sk_buff</span> <span class="o">*</span><span class="n">skb</span><span class="p">,</span>
                <span class="k">const</span> <span class="k">struct</span> <span class="n">nf_hook_state</span> <span class="o">*</span><span class="n">state</span><span class="p">)</span>
<span class="p">{</span>
<span class="cp">#ifdef CONFIG_XFRM
</span>    <span class="k">const</span> <span class="k">struct</span> <span class="n">nf_conn</span> <span class="o">*</span><span class="n">ct</span><span class="p">;</span>
    <span class="k">enum</span> <span class="n">ip_conntrack_info</span> <span class="n">ctinfo</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">err</span><span class="p">;</span>
<span class="cp">#endif
</span>    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">ret</span><span class="p">;</span>

    <span class="n">ret</span> <span class="o">=</span> <span class="n">nf_nat_ipv4_fn</span><span class="p">(</span><span class="n">priv</span><span class="p">,</span> <span class="n">skb</span><span class="p">,</span> <span class="n">state</span><span class="p">);</span> <span class="c1">// &lt;-- call to nf_nat_ipv4_fn</span>
<span class="cp">#ifdef CONFIG_XFRM
</span>    <span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">!=</span> <span class="n">NF_ACCEPT</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ret</span><span class="p">;</span>
</code></pre>
		</div>
	</div>
	<div class="language-c highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="c1">// net/netfilter/nf_nat_proto.c</span>

<span class="k">static</span> <span class="kt">unsigned</span> <span class="kt">int</span>
<span class="nf">nf_nat_ipv4_fn</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">priv</span><span class="p">,</span> <span class="k">struct</span> <span class="n">sk_buff</span> <span class="o">*</span><span class="n">skb</span><span class="p">,</span>
               <span class="k">const</span> <span class="k">struct</span> <span class="n">nf_hook_state</span> <span class="o">*</span><span class="n">state</span><span class="p">)</span>
<span class="p">{</span>
    <span class="c1">// ...</span>

    <span class="k">return</span> <span class="n">nf_nat_inet_fn</span><span class="p">(</span><span class="n">priv</span><span class="p">,</span> <span class="n">skb</span><span class="p">,</span> <span class="n">state</span><span class="p">);</span>
<span class="p">}</span>
</code></pre>
		</div>
	</div>
	<div class="language-c highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="c1">// net/netfilter/nf_nat_core.c</span>

<span class="kt">unsigned</span> <span class="kt">int</span>
<span class="nf">nf_nat_inet_fn</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">priv</span><span class="p">,</span> <span class="k">struct</span> <span class="n">sk_buff</span> <span class="o">*</span><span class="n">skb</span><span class="p">,</span>
               <span class="k">const</span> <span class="k">struct</span> <span class="n">nf_hook_state</span> <span class="o">*</span><span class="n">state</span><span class="p">)</span>
<span class="p">{</span>
    <span class="c1">// ...</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">nf_nat_oif_changed</span><span class="p">(</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">hook</span><span class="p">,</span> <span class="n">ctinfo</span><span class="p">,</span> <span class="n">nat</span><span class="p">,</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">out</span><span class="p">))</span>
            <span class="k">goto</span> <span class="n">oif_changed</span><span class="p">;</span>
    <span class="c1">// ...</span>

<span class="nl">oif_changed:</span>
    <span class="n">nf_ct_kill_acct</span><span class="p">(</span><span class="n">ct</span><span class="p">,</span> <span class="n">ctinfo</span><span class="p">,</span> <span class="n">skb</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">NF_DROP</span><span class="p">;</span>
<span class="p">}</span>
</code></pre>
		</div>
	</div>
	<p>As far as function inlining goes, there’s only one way <code class="language-plaintext highlighter-rouge">nf_nat_inet_fn</code> calls into <code class="language-plaintext highlighter-rouge">nf_ct_delete</code>, which is through <code class="language-plaintext highlighter-rouge">nf_ct_kill_acct</code>. And the only reason for that is <code class="language-plaintext highlighter-rouge">nf_nat_oif_changed</code>.</p>
	<h2 id="conclusion">Conclusion</h2>
	<p>Now everything makes sense. With a badly placed <code class="language-plaintext highlighter-rouge">ACCEPT</code> rule, the conntrack connection gets saved a different mark than desired, and then destroyed because subsequent packets are routed differently for having the wrong mark. The timestamp difference of related events also roughly matches up the distance of the code path. It also must be a NAT’ed connection, as this way of <code class="language-plaintext highlighter-rouge">nf_ct_delete</code> is only reachable when the packet is about to be sent to the egress interface.</p>
	]]></content><author><name>iBug</name></author><category term="linux" /><category term="networking" /><summary type="html"><![CDATA[I run a Debian-based software router on my home network. It’s connected to multiple ISPs, so I have some policy routing rules to balance the traffic between them. Some time ago, I noticed that the IPv4 connectivity got stuck intermittently when it didn’t use to, while IPv6 was working fine. It’s also interesting that the issue only happened with one specific ISP, in the egress direction, and only a few specific devices were affected.]]></summary></entry><entry><title type="html">Driving pppd with systemd</title><link href="https://ibug.io/blog/2024/07/pppd-with-systemd/" rel="alternate" type="text/html" title="Driving pppd with systemd" /><published>2024-07-07T00:00:00+00:00</published><updated>2024-07-16T01:25:55+00:00</updated><id>https://ibug.io/blog/2024/07/pppd-with-systemd</id><content type="html" xml:base="https://ibug.io/blog/2024/07/pppd-with-systemd/"><![CDATA[<p>I moved my soft router (Intel N5105, Debian) from school to home, and at home it’s behind an ONU on bridge mode, so it’ll have to do PPPoE itself.</p>
	<p>Getting started with PPPoE on Debian is exactly the same as on Ubuntu: Install <code class="language-plaintext highlighter-rouge">pppoeconf</code> and run <code class="language-plaintext highlighter-rouge">pppoeconf</code>, then fill in the DSL username and password. Then I can see <code class="language-plaintext highlighter-rouge">ppp0</code> interface up and working.</p>
	<p>However, as I use <code class="language-plaintext highlighter-rouge">systemd-networkd</code> on my router while <code class="language-plaintext highlighter-rouge">pppd</code> appears to bundle ifupdown, I’ll have to fix everything needed for <code class="language-plaintext highlighter-rouge">pppd</code> to work with systemd-networkd.</p>
	<h2 id="systemd-service">Systemd service</h2>
	<p>The first thing is to get it to start at boot. Looking through Google, a <a href="https://gist.github.com/rany2/330c8fe202b318cacdcb54830c20f98c">Gist</a> provides the exact systemd service file I need. After copying it to <code class="language-plaintext highlighter-rouge">/etc/systemd/system/ppp@.service</code>, I tried to start it with <code class="language-plaintext highlighter-rouge">systemctl start pppd@dsl-provider</code>. It seems like there’s a misconfiguration:</p>
	<div class="language-text highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>/usr/sbin/pppd: Can't open options file /etc/ppp/peers/dsl/provider: No such file or directory
</code></pre>
		</div>
	</div>
	<p>The instance name is surely <code class="language-plaintext highlighter-rouge">dsl-provider</code> and not <code class="language-plaintext highlighter-rouge">dsl/provider</code>, so I look more closely at the service file.</p>
	<div class="language-ini highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="nn">[...]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">PPP connection for %I</span>
<span class="nn">[...]</span>
<span class="py">ExecStart</span><span class="p">=</span><span class="s">/usr/sbin/pppd up_sdnotify nolog call %I</span>
</code></pre>
		</div>
	</div>
	<p>The systemd man page <a href="https://www.freedesktop.org/software/systemd/man/latest/systemd.unit.html"><code class="language-plaintext highlighter-rouge">systemd.unit(5)</code></a> says:</p>
	<blockquote>
		<table>
			<thead>
				<tr>
					<th>Specifier</th>
					<th>Meaning</th>
					<th>Details</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td>“%i”</td>
					<td>Instance name</td>
					<td>For instantiated units this is the string between the first “@” character and the type suffix. Empty for non-instantiated units.</td>
				</tr>
				<tr>
					<td>“%I”</td>
					<td>Unescaped instance name</td>
					<td>Same as “%i”, but with escaping undone.</td>
				</tr>
			</tbody>
		</table>
	</blockquote>
	<p>Fair enough, let’s change <code class="language-plaintext highlighter-rouge">%I</code> to <code class="language-plaintext highlighter-rouge">%i</code> and try starting <code class="language-plaintext highlighter-rouge">pppd@dsl-provider</code> again.</p>
	<h2 id="systemd-networkd">systemd-networkd</h2>
	<p>Now that <code class="language-plaintext highlighter-rouge">ppp0</code> is up, time to configure routes and routing rules with <code class="language-plaintext highlighter-rouge">systemd-networkd</code>. I created a file <code class="language-plaintext highlighter-rouge">/etc/systemd/network/10-ppp0.network</code>.</p>
	<div class="language-ini highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="nn">[Match]</span>
<span class="py">Name</span><span class="p">=</span><span class="s">ppp0</span>

<span class="nn">[Network]</span>
<span class="py">DHCP</span><span class="p">=</span><span class="s">yes</span>
<span class="c"># ...
</span></code></pre>
		</div>
	</div>
	<p>After restarting systemd-networkd, I was disappointed to see the PPP-negotiated IP address removed, only leaving an SLAAC IPv6 address behind. With some searching through <code class="language-plaintext highlighter-rouge">systemd.network(5)</code>, I found <code class="language-plaintext highlighter-rouge">KeepConfiguration=yes</code> was what I was looking for.</p>
	<h2 id="start-order">Start order</h2>
	<p>One problem still remains: At the time systemd-networkd starts, <code class="language-plaintext highlighter-rouge">ppp0</code> is not yet up, and systemd-networkd simply skips its configuration. A solution seems trivial:</p>
	<div class="language-ini highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="c"># systemctl edit pppd@dsl-provider
</span><span class="nn">[Unit]</span>
<span class="py">Before</span><span class="p">=</span><span class="s">systemd-networkd.service</span>
</code></pre>
		</div>
	</div>
	<p>… except it doesn’t seem to have any effect.</p>
	<p>I wouldn’t bother digging into pppd, so I look around for something analogous to ifupdown’s <code class="language-plaintext highlighter-rouge">up</code> script, which is <code class="language-plaintext highlighter-rouge">/etc/ppp/ip-up.d/</code>. So I could just drop another script to notify systemd-networkd.</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="c"># /etc/ppp/ip-up.d/1systemd-networkd</span>
<span class="c">#!/bin/sh</span>

networkctl reconfigure <span class="s2">"</span><span class="nv">$PPP_IFACE</span><span class="s2">"</span>
</code></pre>
		</div>
	</div>
	<p>I also noticed that when bringing in ifupdown, the <code class="language-plaintext highlighter-rouge">pppoeconf</code>-created config looks like this:</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>auto dsl-provider
iface dsl-provider inet ppp
    pre-up /bin/ip <span class="nb">link set </span>enp3s0 up <span class="c"># line maintained by pppoeconf</span>
    provider dsl-provider
</code></pre>
		</div>
	</div>
	<p>So to maintain behavioral compatibility, I configured the systemd service like this:</p>
	<div class="language-ini highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="c"># systemctl edit pppd@dsl-provider
</span><span class="nn">[Unit]</span>
<span class="py">BindsTo</span><span class="p">=</span><span class="s">sys-subsystem-net-devices-enp3s0.device</span>
<span class="py">After</span><span class="p">=</span><span class="s">sys-subsystem-net-devices-enp3s0.device</span>
</code></pre>
		</div>
	</div>
	<p>After multiple reboots and manual restarts of <code class="language-plaintext highlighter-rouge">pppd@dsl-provider.service</code>, I’m convinced that this is a reliable solution.</p>
	<h2 id="extra">Extra: IPv6 PD</h2>
	<p>As the home ISP provides IPv6 Prefix Delegation (but my school didn’t), it would be nice to take it and distribute it to the LAN. Online tutorials are abundant, e.g. <a href="https://major.io/p/dhcpv6-prefix-delegation-with-systemd-networkd/" rel="nofollow noopener">this one</a>. With everything set supposedly up, I was again disappointed to see only a single SLAAC IPv6 address on <code class="language-plaintext highlighter-rouge">ppp0</code> itself, and <code class="language-plaintext highlighter-rouge">journalctl -eu systemd-networkd</code> shows no sign of receiving a PD allocation.</p>
	<p>After poking around with <code class="language-plaintext highlighter-rouge">IPv6AcceptRA=</code> and <code class="language-plaintext highlighter-rouge">[DHCPv6] PrefixDelegationHint=</code> settings for a while, I decided to capture some packets for investigation. I started <code class="language-plaintext highlighter-rouge">tcpdump -i ppp0 -w /tmp/ppp0.pcap icmp6 or udp port 546</code> and restarted <code class="language-plaintext highlighter-rouge">systemd-networkd</code>. After a few seconds, the pcap file contains exactly 4 packets that I need (some items omitted for brevity):</p>
	<div class="language-markdown highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="p">-</span> ICMPv6: Router Solicitation from 00:00:00:00:00:00
<span class="p">-</span> ICMPv6: Router Advertisement from 00:00:5e:00:01:99
<span class="p">  -</span> Flags: 0x40 (only O)
<span class="p">  -</span> ICMPv6 Option: Prefix information (2001:db8::/64)
<span class="p">    -</span> Flags: L + A
<span class="p">-</span> DHCPv6: Information-request XID: 0x8bf4f0 CID: 00020000ab11503f79e54f10745d
<span class="p">  -</span> Option Request
<span class="p">    -</span> Option: Option Request (6)
<span class="p">    -</span> Length: 10
<span class="p">    -</span> Requested Option code: DNS recursive name server (23)
<span class="p">    -</span> Requested Option code: Simple Network Time Protocol Server (31)
<span class="p">    -</span> Requested Option code: Lifetime (32)
<span class="p">    -</span> Requested Option code: NTP Server (56)
<span class="p">    -</span> Requested Option code: INF_MAX_RT (83)
<span class="p">-</span> DHCPv6: Reply XID: 0x8bf4f0 CID: 00020000ab11503f79e54f10745d
</code></pre>
		</div>
	</div>
	<p>Clearly the client isn’t even requesting a PD allocation with <code class="language-plaintext highlighter-rouge">PrefixDelegationHint=</code> set. With some more Google-ing, I added <code class="language-plaintext highlighter-rouge">[DHCPv6] WithoutRA=solicit</code> to <code class="language-plaintext highlighter-rouge">10-ppp0.network</code> and restarted <code class="language-plaintext highlighter-rouge">systemd-networkd</code>. There are 6 packets, but the order appears a little bit off:</p>
	<div class="language-markdown highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="p">-</span> Solicit XID: 0x2bc2aa CID: 00020000ab11503f79e54f10745d
<span class="p">-</span> Advertise XID: 0x2bc2aa CID: 00020000ab11503f79e54f10745d
<span class="p">-</span> Request XID: 0xf8c1dd CID: 00020000ab11503f79e54f10745d
<span class="p">  -</span> Identity Association for Prefix Delegation
<span class="p">-</span> Reply XID: 0xf8c1dd CID: 00020000ab11503f79e54f10745d
<span class="p">-</span> Router Solicitation from 00:00:00:00:00:00
<span class="p">-</span> Router Advertisement from 00:00:5e:00:01:99
</code></pre>
		</div>
	</div>
	<p>This time DHCP request comes <em>before</em> the RS/RA pair, which is not what I expected. But at least it’s now requesting a PD prefix.</p>
	<p>Then I found <a href="https://unix.stackexchange.com/a/715025/211239">this answer</a> straight to the point, summarized as:</p>
	<ul>
		<li>The “managed” (M) flag indicates the client should acquire an address via DHCPv6, and triggers DHCPv6 Solicit and Request messages.</li>
		<li>The “other” (O) flag indicates the client should do SLAAC while acquiring other configuration information via DHCPv6, and triggers DHCPv6 Information-request messages.</li>
		<li>When both flags are present, the O flag is superseded by the M flag and has no effect.</li>
	</ul>
	<p>So systemd-networkd is implementing everything correctly, and I should configure systemd-networkd to always send Solicit messages regardless of the RA flags received. This is done by setting <code class="language-plaintext highlighter-rouge">[IPv6AcceptRA] DHCPv6Client=always</code></p>
	<p>Now with every detail understood, after a restart of <code class="language-plaintext highlighter-rouge">systemd-networkd</code>, I finally see the PD prefix allocated:</p>
	<div class="language-text highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>systemd-networkd[528]: ppp0: DHCP: received delegated prefix 2001:db8:0:a00::/60
systemd-networkd[528]: enp1s0: DHCP-PD address 2001:db8:0:a00:2a0:c9ff:feee:c4b/64 (valid for 2d 23h 59min 59s, preferred for 1d 23h 59min 59s)
systemd-networkd[528]: enp2s0: DHCP-PD address 2001:db8:0:a01:2a0:c9ff:feee:c4c/64 (valid for 2d 23h 59min 59s, preferred for 1d 23h 59min 59s)
</code></pre>
		</div>
	</div>
	<h2 id="update-1">Update: Stuck booting</h2>
	<p>A few days after this blog post, my local ISP ran into an outage that rendered the PPPoE connection unoperational.
		When I couldn’t identify the issue initially, I tried rebooting the router and it never came back up again.
		I plugged in a monitor and a keyboard, only to find systemd repeatedly trying to bring up <code class="language-plaintext highlighter-rouge">pppd@dsl-provider.service</code> when it would not succeed.
		The failure to start <code class="language-plaintext highlighter-rouge">pppd</code> resulted in complete unavailability of the network stack.</p>
	<p>I recalled that with OpenWRT this wasn’t the case, as the PPPoE interface being down would not impact any other interfaces.
		So I ended up removing all dependencies on <code class="language-plaintext highlighter-rouge">pppd@.service</code>, making it an ordinary system service that’s only <code class="language-plaintext highlighter-rouge">WantedBy=multi-user.target</code>.
		Considering that pppd will call <code class="language-plaintext highlighter-rouge">networkctl reconfigure</code> when it establishes the <code class="language-plaintext highlighter-rouge">ppp0</code> interface, the removal of systemd dependences shouldn’t have any consequences.</p>
	<h2 id="sum-up">Sum up</h2>
	<ul>
		<li>Use systemd to start <code class="language-plaintext highlighter-rouge">pppd</code> as a system service.
			<ul>
				<li>No need to bother with ordering.</li>
			</ul>
		</li>
		<li>Add <code class="language-plaintext highlighter-rouge">KeepConfiguration=yes</code> to <code class="language-plaintext highlighter-rouge">ppp0.network</code>.</li>
		<li>Use a custom script in <code class="language-plaintext highlighter-rouge">ip-up.d</code> to invoake systemd-networkd to reconfigure after it’s up.</li>
		<li>
			<p>For IPv6 PD, use both:</p>
			<div class="language-ini highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="nn">[DHCPv6]</span>
<span class="py">PrefixDelegationHint</span><span class="p">=</span><span class="s">::/60</span>

<span class="nn">[IPv6AcceptRA]</span>
<span class="py">DHCPv6Client</span><span class="p">=</span><span class="s">always</span>
</code></pre>
				</div>
    </div>
		</li>
	</ul>
	]]></content><author><name>iBug</name></author><category term="linux" /><category term="networking" /><summary type="html"><![CDATA[I moved my soft router (Intel N5105, Debian) from school to home, and at home it’s behind an ONU on bridge mode, so it’ll have to do PPPoE itself.]]></summary></entry><entry><title type="html">Migrating Ubuntu onto ZFS</title><link href="https://ibug.io/blog/2024/05/migrate-rootfs-to-zfs/" rel="alternate" type="text/html" title="Migrating Ubuntu onto ZFS" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T23:37:49+00:00</updated><id>https://ibug.io/blog/2024/05/migrate-rootfs-to-zfs</id><content type="html" xml:base="https://ibug.io/blog/2024/05/migrate-rootfs-to-zfs/"><![CDATA[<p>As part of a planned disk migration, I decided to move my Ubuntu installation from a traditional ext4 setup to ZFS.
		I did a lot of preparation and research, but things went much smoother than I had previously anticipated.
		I did not even have to consult IPMI for any recovery.</p>
	<p>Existing partition layout:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>fdisk <span class="nt">-l</span> /dev/nvme1n1
<span class="go">[...]
Device             Start        End    Sectors  Size Type
/dev/nvme1n1p1      2048    1050623    1048576  512M EFI System
/dev/nvme1n1p2   1050624  269486079  268435456  128G Linux filesystem
/dev/nvme1n1p3 269486080 3907029134 3637543055  1.7T Solaris /usr &amp; Apple ZFS
</span></code></pre>
		</div>
	</div>
	<p>Since I already have <code class="language-plaintext highlighter-rouge">/home</code> running on ZFS <code class="language-plaintext highlighter-rouge">pool0</code>, there’s not much to prepare.
		All I need to move is the rootfs itself, which has around 20&nbsp;GB of data.</p>
	<p>Start by installing anything necessary:</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>apt <span class="nb">install </span>zfs-initramfs arch-install-scripts
</code></pre>
		</div>
	</div>
	<p>Then create the dataset layout:</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="c"># pool0 already has xattr=sa</span>
zfs create <span class="se">\</span>
  <span class="nt">-o</span> <span class="nv">canmount</span><span class="o">=</span>off <span class="se">\</span>
  <span class="nt">-o</span> <span class="nv">mountpoint</span><span class="o">=</span>none <span class="se">\</span>
  <span class="nt">-o</span> <span class="nv">acltype</span><span class="o">=</span>posix <span class="se">\</span>
  pool0/ROOT
zfs create <span class="nt">-o</span> <span class="nv">mountpoint</span><span class="o">=</span>/mnt/new pool0/ROOT/ubuntu

rsync <span class="nt">-avSHAXx</span> <span class="nt">--delete</span> / /mnt/new/
</code></pre>
		</div>
	</div>
	<p>Now there’s a little deviation from common setup.
		I don’t trust GRUB’s ZFS support, so I’m going to merge <code class="language-plaintext highlighter-rouge">/boot</code> into the EFI partition (which has a decent 512&nbsp;MB of capacity).
		This is a decision made after surveying my friends’ setup.</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="c"># Merge data</span>
rsync <span class="nt">-ax</span> /boot/ /boot/efi/ <span class="c"># Ignore any errors</span>
umount /boot/efi
vim /etc/fstab
<span class="c"># Change /boot/efi to /boot</span>
<span class="c"># Also remove the current rootfs entry</span>
systemctl daemon-reload
mount /boot
</code></pre>
		</div>
	</div>
	<p>Now prepare GRUB:</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>zpool <span class="nb">set </span><span class="nv">bootfs</span><span class="o">=</span>pool0/ROOT/ubuntu pool0
mount <span class="nt">-o</span> <span class="nb">bind</span> /boot /mnt/new/boot
arch-chroot /mnt/new
</code></pre>
		</div>
	</div>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>grub-install
<span class="go">Installing for x86_64-efi platform.
grub-install: error: cannot find EFI directory.
</span></code></pre>
		</div>
	</div>
	<p>Well, if only <code class="language-plaintext highlighter-rouge">grub-install</code> didn’t hard-code <code class="language-plaintext highlighter-rouge">/boot/efi</code> (which is against the FHS standard anyways).
		Fortunately, I recall a small detail that could make this work in another convenient way:</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>dpkg-reconfigure grub-efi-amd64
</code></pre>
		</div>
	</div>
	<p>Also regenerate GRUB configuration:</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>zfs <span class="nb">set </span><span class="nv">mountpoint</span><span class="o">=</span>/ pool0/ROOT/ubuntu
update-grub
</code></pre>
		</div>
	</div>
	<p>Now double-check the GRUB configuration at <code class="language-plaintext highlighter-rouge">/boot/grub/grub.cfg</code> and make sure there are lines like this:</p>
	<div class="language-text highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>linux /vmlinuz [...] root=ZFS=pool0/ROOT/ubuntu [...]
</code></pre>
		</div>
	</div>
	<p>After verifying paths to the kernel and the initrd image are correct, reboot:</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>reboot
</code></pre>
		</div>
	</div>
	<p>In just a minute, I noticed my server came back up.
		Time to confirm everything is working as expected:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>mount
<span class="go">pool0/ROOT/ubuntu on / type zfs (rw,relatime,xattr,posixacl,casesensitive)

</span><span class="gp">#</span><span class="w"> </span><span class="nb">df</span> <span class="nt">-h</span> /
<span class="go">Filesystem         Size  Used Avail Use% Mounted on
pool0/ROOT/ubuntu  1.2T   11G  1.1T   1% /

</span><span class="gp">#</span><span class="w"> </span>zfs get compressratio pool0/ROOT
<span class="go">NAME        PROPERTY       VALUE  SOURCE
pool0/ROOT  compressratio  2.02x  -
</span></code></pre>
		</div>
	</div>
	<p>The last thing is to rewrite my rootfs backup script to take snapshots directly, instead of rsync-ing to another ZFS pool before taking a snapshot there.
		After taking a snapshot, I can also send it away as a “backup against disk failure”.</p>
	<p>A slightly revised version of my snapshotting script, sans the sending part:</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="c">#!/bin/sh</span>

<span class="nb">set</span> <span class="nt">-e</span>

<span class="nv">DATASET</span><span class="o">=</span>pool0/ROOT/ubuntu
<span class="nv">DATE</span><span class="o">=</span><span class="si">$(</span><span class="nb">date</span> +%Y%m%d<span class="si">)</span>
<span class="nv">SNAPSHOT</span><span class="o">=</span><span class="s2">"</span><span class="nv">$DATASET</span><span class="s2">@</span><span class="nv">$DATE</span><span class="s2">"</span>
<span class="nv">RETENTION_DAYS</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">1</span><span class="k">:-</span><span class="nv">7</span><span class="k">}</span><span class="s2">"</span>
<span class="nv">RETENTION</span><span class="o">=</span><span class="s2">"</span><span class="k">$((</span>RETENTION_DAYS <span class="o">*</span> <span class="m">86400</span><span class="k">))</span><span class="s2">"</span>

<span class="nv">NOW</span><span class="o">=</span><span class="s2">"</span><span class="k">$((</span><span class="si">$(</span><span class="nb">date</span> +%s<span class="si">)</span> <span class="o">-</span> <span class="m">3600</span><span class="k">))</span><span class="s2">"</span>
<span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="si">$(</span>zfs list <span class="nt">-Hpo</span> name <span class="s2">"</span><span class="nv">$SNAPSHOT</span><span class="s2">"</span><span class="si">)</span><span class="s2">"</span> <span class="o">=</span> <span class="s2">"</span><span class="nv">$SNAPSHOT</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
  </span><span class="nb">echo</span> <span class="s2">"Snapshot exists: </span><span class="nv">$SNAPSHOT</span><span class="s2">"</span>
<span class="k">else
  </span>zfs snapshot <span class="nt">-ro</span> ibug:retention<span class="o">=</span><span class="s2">"</span><span class="nv">$RETENTION</span><span class="s2">"</span> <span class="s2">"</span><span class="nv">$SNAPSHOT</span><span class="s2">"</span>
<span class="k">fi

</span>zfs list <span class="nt">-Hpt</span> snapshot <span class="nt">-o</span> name,creation,ibug:retention <span class="s2">"</span><span class="nv">$DATASET</span><span class="s2">"</span> |
  <span class="k">while </span><span class="nb">read</span> <span class="nt">-r</span> zNAME zCREATION zRETENTION<span class="p">;</span> <span class="k">do
  if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$zRETENTION</span><span class="s2">"</span> <span class="o">=</span> <span class="s2">"-"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="c"># assume default value</span>
    <span class="nv">zRETENTION</span><span class="o">=</span><span class="s2">"</span><span class="k">$((</span><span class="m">7</span> <span class="o">*</span> <span class="m">86400</span><span class="k">))</span><span class="s2">"</span>
  <span class="k">fi
  </span><span class="nv">UNTIL</span><span class="o">=</span><span class="s2">"</span><span class="k">$((</span>zCREATION <span class="o">+</span> zRETENTION<span class="k">))</span><span class="s2">"</span>
  <span class="nv">UNTIL_DATE</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span><span class="nb">date</span> <span class="nt">-d</span> <span class="s2">"@</span><span class="nv">$UNTIL</span><span class="s2">"</span> <span class="s2">"+%Y-%m-%d %H:%M:%S"</span><span class="si">)</span><span class="s2">"</span>
  <span class="nb">echo</span> <span class="s2">"</span><span class="nv">$zNAME</span><span class="s2">: </span><span class="nv">$UNTIL_DATE</span><span class="s2">"</span>
  <span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$NOW</span><span class="s2">"</span> <span class="nt">-ge</span> <span class="s2">"</span><span class="nv">$UNTIL</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span>zfs destroy <span class="nt">-rv</span> <span class="s2">"</span><span class="nv">$zNAME</span><span class="s2">"</span>
  <span class="k">fi
done</span>
</code></pre>
		</div>
	</div>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="c"># crontab</span>
15 4 <span class="k">*</span> <span class="k">*</span> 1,5     /root/backup.sh 30
15 4 <span class="k">*</span> <span class="k">*</span> 0,2-4,6 /root/backup.sh  7
</code></pre>
		</div>
	</div>
	]]></content><author><name>iBug</name></author><category term="linux" /><category term="server" /><category term="zfs" /><summary type="html"><![CDATA[As part of a planned disk migration, I decided to move my Ubuntu installation from a traditional ext4 setup to ZFS. I did a lot of preparation and research, but things went much smoother than I had previously anticipated. I did not even have to consult IPMI for any recovery.]]></summary></entry><entry><title type="html">Reload SSL certificates with systemd</title><link href="https://ibug.io/blog/2024/03/reload-ssl-cert-with-systemd/" rel="alternate" type="text/html" title="Reload SSL certificates with systemd" /><published>2024-03-31T00:00:00+00:00</published><updated>2024-04-01T18:23:24+00:00</updated><id>https://ibug.io/blog/2024/03/reload-ssl-cert-with-systemd</id><content type="html" xml:base="https://ibug.io/blog/2024/03/reload-ssl-cert-with-systemd/"><![CDATA[<p>Recently I relinquished an old domain on my server and had to re-issue a certificate to drop that domain off.
		Previously it ran Let’s Encrypt’s official client Certbot, set up back in 2019.
		All my recent setups have been using acme.sh, so I figured that this was a perfect chance to switch this one over as well.</p>
	<p>Getting acme.sh to issue a new certificate for my updated domain list is easy enough and out of scope for this article.
		But when it comes to reloading the certificate for services using it, I have to think twice.
		Back in the days when Nginx was the sole consumer of the certificate, I directly referenced the certificate files in <code class="language-plaintext highlighter-rouge">/etc/letsencrypt/live/</code> from Nginx config, and somehow slappped a <code class="language-plaintext highlighter-rouge">systemctl reload nginx</code> into crontab to handle the reload.
		Now that there are multiple services using the certificate, I no longer consider it a good idea to reload all the services in a crontab.
		There has to be a better way.</p>
	<p>Since all my services are managed by systemd, using an extra “service” or whatever unit to group them together seems like a better idea.
		Systemd’s <code class="language-plaintext highlighter-rouge">ReloadPropagatedFrom=</code> option and its inverse <code class="language-plaintext highlighter-rouge">PropagatesReloadTo=</code> immediately come to mind. With the right direction, it’s easy to Google out this answer: <a href="https://unix.stackexchange.com/q/334471/211239">How do I reload a group of systemd services?</a></p>
	<p>Realizing that “target” is the simplest unit type in systemd’s abstraction, this is the minimum that suits my needs.</p>
	<div class="language-ini highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="c"># /etc/systemd/system/ssl-certificate.target
</span><span class="nn">[Unit]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">SSL certificates reload helper</span>
<span class="py">PropagatesReloadTo</span><span class="p">=</span><span class="s">nginx.service</span>
<span class="py">PropagatesReloadTo</span><span class="p">=</span><span class="s">postfix.service</span>
</code></pre>
		</div>
	</div>
	<p>Then, following the above Unix &amp; Linux answer, here’s a “path” unit that lets systemd monitor the certificate files for changes.</p>
	<div class="language-ini highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="c"># /etc/systemd/system/ssl-certificate.path
</span><span class="nn">[Unit]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">SSL certificate reload helper</span>
<span class="py">Wants</span><span class="p">=</span><span class="s">%N.target</span>

<span class="nn">[Path]</span>
<span class="py">PathChanged</span><span class="p">=</span><span class="s">/etc/ssl/private/%H/cert.pem</span>

<span class="nn">[Install]</span>
<span class="py">WantedBy</span><span class="p">=</span><span class="s">multi-user.target</span>
</code></pre>
		</div>
	</div>
	<p>The <code class="language-plaintext highlighter-rouge">Wants=</code> setting here ensure that the corresponding target unit is activated, otherwise it cannot be <code class="language-plaintext highlighter-rouge">reload</code>ed.</p>
	<p>There’s one deficiency in the answer above: A “path” unit can only <em>activate</em> another unit, not <em>reload</em> it. So I still have to create a oneshot service that calls <code class="language-plaintext highlighter-rouge">systemctl reload</code> on the target, which itself can then be activated by the “path” unit.</p>
	<div class="language-ini highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="c"># /etc/systemd/system/ssl-certificate.service
</span><span class="nn">[Unit]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">SSL certificate reload helper</span>
<span class="py">StartLimitIntervalSec</span><span class="p">=</span><span class="s">5s</span>
<span class="py">StartLimitBurst</span><span class="p">=</span><span class="s">2</span>

<span class="nn">[Service]</span>
<span class="py">Type</span><span class="p">=</span><span class="s">oneshot</span>
<span class="py">ExecStart</span><span class="p">=</span><span class="s">/bin/systemctl reload %N.target</span>
</code></pre>
		</div>
	</div>
	<p>It’s important that this service comes with <code class="language-plaintext highlighter-rouge">Type=oneshot</code> and <em>without</em> <code class="language-plaintext highlighter-rouge">RemainAfterExit=yes</code>, so that it can be repeatedly activated by the “path” unit.</p>
	<p>Now I can test if things work:</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>systemctl daemon-reload
systemctl <span class="nb">enable</span> <span class="nt">--now</span> ssl-certificate.path
acme.sh <span class="nt">--install-cert</span> <span class="nt">-d</span> <span class="s2">"</span><span class="nv">$HOSTNAME</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--cert-file</span> <span class="s2">"/etc/ssl/private/</span><span class="nv">$HOSTNAME</span><span class="s2">/cert.pem"</span> <span class="se">\</span>
  <span class="nt">--key-file</span> <span class="s2">"/etc/ssl/private/</span><span class="nv">$HOSTNAME</span><span class="s2">/privkey.pem"</span> <span class="se">\</span>
  <span class="nt">--fullchain-file</span> <span class="s2">"/etc/ssl/private/</span><span class="nv">$HOSTNAME</span><span class="s2">/fullchain.pem"</span>
</code></pre>
		</div>
	</div>
	<p>And then inspect the services:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>systemctl status nginx.service
<span class="go">[...]
Mar 31 19:20:11 hostname systemd[1]: Reloading A high performance web server and a reverse proxy server...
Mar 31 19:20:12 hostname systemd[1]: Reloaded A high performance web server and a reverse proxy server.

</span><span class="gp">$</span><span class="w"> </span>systemctl status postfix.service
<span class="go">[...]
Mar 31 19:20:11 hostname systemd[1]: Reloading Postfix Mail Transport Agent...
Mar 31 19:20:12 hostname systemd[1]: Reloaded Postfix Mail Transport Agent.
</span></code></pre>
		</div>
	</div>
	<p>So now, job done. As acme.sh stores install information, the next time these certificates are renewed, acme.sh will automatically copy them over to <code class="language-plaintext highlighter-rouge">/etc/ssl/private/$HOSTNAME/</code>, and systemd will pick up the changes and reload the services.</p>
	]]></content><author><name>iBug</name></author><category term="linux" /><summary type="html"><![CDATA[Recently I relinquished an old domain on my server and had to re-issue a certificate to drop that domain off. Previously it ran Let’s Encrypt’s official client Certbot, set up back in 2019. All my recent setups have been using acme.sh, so I figured that this was a perfect chance to switch this one over as well.]]></summary></entry><entry><title type="html">I almost broke our lab’s storage server…</title><link href="https://ibug.io/blog/2024/03/stupid-dell-recovery/" rel="alternate" type="text/html" title="I almost broke our lab’s storage server…" /><published>2024-03-13T00:00:00+00:00</published><updated>2024-03-27T18:34:21+00:00</updated><id>https://ibug.io/blog/2024/03/stupid-dell-recovery</id><content type="html" xml:base="https://ibug.io/blog/2024/03/stupid-dell-recovery/"><![CDATA[<p>Recently we discovered that both SSDs on our storage server were giving worrisome SMART values, so we started replacing them.
		One of them was used only for ZFS L2ARC, so pulling it out was easy.
		The other runs the rootfs and we couldn’t touch it for the time being, so we inserted a backup drive thinking we can migrate the OS later on.</p>
	<p>After returning from the datacenter, I start working on the migration.
		The initial steps are nothing but ordinary:</p>
	<ul>
		<li>Examine the spare drive to ensure there’s no important data on it, then wipe it (<code class="language-plaintext highlighter-rouge">blkdiscard -f /dev/sdb</code>).</li>
		<li>Create the partition table that closely resembles the current system drive’s layout: 100&nbsp;MB for the EFI partition (down from 512&nbsp;MB), 32&nbsp;GB for rootfs, and the rest for an LVM PV.</li>
		<li>Format the partitions: <code class="language-plaintext highlighter-rouge">mkfs.vfat /dev/sdb1</code>, <code class="language-plaintext highlighter-rouge">mkfs.ext4 /dev/sdb2</code>, <code class="language-plaintext highlighter-rouge">pvcreate /dev/sdb3</code>.</li>
		<li>Copy the rootfs over: <code class="language-plaintext highlighter-rouge">mount /dev/sdb2 /t</code>, <code class="language-plaintext highlighter-rouge">rsync -aHAXx / /t</code>.</li>
		<li>Reinstall the bootloader: <code class="language-plaintext highlighter-rouge">mount /dev/sdb1 /t/boot/efi</code>, <code class="language-plaintext highlighter-rouge">arch-chroot /t</code>, <code class="language-plaintext highlighter-rouge">grub-install</code> (target is <code class="language-plaintext highlighter-rouge">x86_64-efi</code>), <code class="language-plaintext highlighter-rouge">update-grub</code>.</li>
		<li>Start migrating LVs: <code class="language-plaintext highlighter-rouge">vgextend pve /dev/sdb3</code>, <code class="language-plaintext highlighter-rouge">pvmove /dev/sda3 /dev/sdb3</code>.</li>
	</ul>
	<p>At this point, a quick thought emerges: This is not the final drive to run the system on and is only here for the transitional period.
		A second migration is planned when the new SSD arrives. So why not take this chance and move the rootfs onto LVM as well?</p>
	<p>With that in mind, I hit Ctrl-C to <code class="language-plaintext highlighter-rouge">pvmove</code>, unbeknownst that it’s interruptible and terminating the <code class="language-plaintext highlighter-rouge">pvmove</code> process only pauses the operation.
		For a moment, I thought I successfully canceled it and tried to re-partition the new drive.
		Since the new PV is still in use by the suspended <code class="language-plaintext highlighter-rouge">pvmove</code> operation, the kernel would not accept any changes to <code class="language-plaintext highlighter-rouge">/dev/sdb3</code>.
		During this process, I deleted and recreated the new rootfs (<code class="language-plaintext highlighter-rouge">/dev/sdb2</code>) and the new PV (<code class="language-plaintext highlighter-rouge">/dev/sdb3</code>) many times, and even tried manually editing LVM metadata (via <code class="language-plaintext highlighter-rouge">vgcfgbackup pve</code>, edit <code class="language-plaintext highlighter-rouge">/etc/lvm/backup/pve</code> and <code class="language-plaintext highlighter-rouge">vgcfgrestore pve</code>), before finally giving up and rebooting the system.</p>
	<p>As a daily dose for a SysAdmin, the server didn’t boot up as expected.
		I fired up a browser to connect to the machine’s IPMI, only to find that the remote console feature for iDRAC 9 was locked behind a paywall for goodness’ sake.
		Thanks to God almighty Dell, things have been unnecessarily more complicated than ever before.
		I carefully recalled every step taken and quickly identified the problem - one important thing forgotten - GRUB was successfully reinstalled on the new EFI partition (which was somehow left intact during the whole fiddling process), pointing to the now-deleted new root partition, and so it’s now stuck with GRUB.</p>
	<p>Fortunately, out of precaution, I had previously configured the IPMI with serial-over-LAN, so I at least still have serial access to the server with <code class="language-plaintext highlighter-rouge">ipmitool</code>. This saved me from a trip back to the datacenter.</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>ipmitool <span class="nt">-I</span> lanplus <span class="nt">-H</span> &lt;ip&gt; <span class="nt">-U</span> &lt;user&gt; <span class="nt">-P</span> &lt;password&gt; sol activate
</code></pre>
		</div>
	</div>
	<p>And better yet, this iDRAC 9 can change BIOS settings, most notably the boot order and one-time boot override. This definitely helped the most in the absence of that goddamn remote console.</p>
	<p><img src="/image/server/idrac-boot-override.png" alt="image" /></p>
	<p>After some trial and error, I got myself into the GRUB command line, and it didn’t look quite well:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">grub rescue&gt;</span><span class="w"> 
</span></code></pre>
		</div>
	</div>
	<p>There’s pretty much just the <code class="language-plaintext highlighter-rouge">ls</code> command, and it doesn’t even recognize the EFI partition (FAT32 filesystem). With some more twiddling, I found this “rescue mode” capable of reading ext4, which shed some light to the situation.</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">grub rescue&gt;</span><span class="w"> </span><span class="nb">set </span><span class="nv">root</span><span class="o">=(</span>hd0,gpt2<span class="o">)</span>
<span class="gp">grub rescue&gt;</span><span class="w"> </span><span class="nb">ls</span> /boot/grub
<span class="go">fonts  grub.cfg  grubenv  locale  unicode.pf2  x86_64-efi
</span></code></pre>
		</div>
	</div>
	<p>Now things began to turn to the upswing.</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">grub rescue&gt;</span><span class="w"> </span><span class="nb">set </span><span class="nv">prefix</span><span class="o">=</span>/boot/grub
<span class="gp">grub rescue&gt;</span><span class="w"> </span>insmod normal
<span class="gp">grub rescue&gt;</span><span class="w"> </span>normal
</code></pre>
		</div>
	</div>
	<p>In a few seconds, I was delighted to discover that the system was up and running, and continued migrating the rootfs.</p>
	<p>After everything’s done, out of every precaution, I installed <code class="language-plaintext highlighter-rouge">grub-efi-amd64-signed</code>, which provides a large, monolithic <code class="language-plaintext highlighter-rouge">grubx64.efi</code> that has all the “optional” modules built-in, so it no longer relies on the filesystem for, e.g., LVM support, in case a similar disaster happens again.</p>
	<h2 id="anecdote">Anecdote</h2>
	<p>When trying to remove the faulty drive from the server, I at first made a wrong recall for its position, and we instead pulled out a running large-capacity HDD. Luckily it was not damaged, so we quickly inserted it back. Thanks to ZFS’s design, it automatically triggered a resilver, which completed in just a blink.</p>
	<div class="language-text highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code># zpool status
  pool: rpool
 state: ONLINE
  scan: resilvered 63.4M in 00:00:03 with 0 errors on Tue Mar 12 17:03:23 2024
</code></pre>
		</div>
	</div>
	<p>If this were a hardware RAID, a tedious and time-consuming rebuild would have been inevitable. It’s only with ZFS that this rapid recovery is possible.</p>
	<h2 id="conclusion">Conclusion</h2>
	<p>This incident was a good lesson for me, and some big takeaways I’d draw:</p>
	<ul>
		<li>Don’t panic under pressure.</li>
		<li>Use ZFS so you can sleep well at night.</li>
		<li>Fuck Dell, next time buy from another vendor that doesn’t lock basic features behind a paywall.</li>
	</ul>
	<p>Plus, the correct way to cancel a <code class="language-plaintext highlighter-rouge">pvmove</code> operation is in <a href="https://linux.die.net/man/8/pvmove"><code class="language-plaintext highlighter-rouge">man 8 pvmove</code></a>, and it’s right at the 2nd paragraph of the Description section.</p>
	]]></content><author><name>iBug</name></author><category term="linux" /><category term="server" /><summary type="html"><![CDATA[Recently we discovered that both SSDs on our storage server were giving worrisome SMART values, so we started replacing them. One of them was used only for ZFS L2ARC, so pulling it out was easy. The other runs the rootfs and we couldn’t touch it for the time being, so we inserted a backup drive thinking we can migrate the OS later on.]]></summary></entry><entry><title type="html">My firewall solution for RDP</title><link href="https://ibug.io/blog/2024/02/linux-firewall-for-rdp/" rel="alternate" type="text/html" title="My firewall solution for RDP" /><published>2024-02-28T00:00:00+00:00</published><updated>2024-02-28T18:33:21+00:00</updated><id>https://ibug.io/blog/2024/02/linux-firewall-for-rdp</id><content type="html" xml:base="https://ibug.io/blog/2024/02/linux-firewall-for-rdp/"><![CDATA[<p>Today I stumbled upon <a href="https://www.v2ex.com/t/1019147">this V2EX post</a> (Simplified Chinese) where the OP shared their PowerShell implementation of a “makeshift fail2ban” for RDP (<a href="https://github.com/Qetesh/rdpFail2Ban">their GitHub repository</a>). Their script looked very clean and robust, but needless to say, it is unnecessarily difficult on Windows. So on this rare (maybe?) occasion I decide to share my firewall for securing RDP access to my Windows hosts.</p>
	<p><strong>None</strong> of my Windows hosts (PCs and VMs) has their RDP port exposed to the public internet directly, and they’re all connected to my mesh VPN (which is out of scope for this blog article). My primary public internet entry gateway for the intranet runs Debian with fully manually configured iptables-based firewall, and I frequently work on it through SSH.</p>
	<p>My goal is to expose the RDP port only to myself. There are a few obvious solutions eliminated for different reasons:</p>
	<ul>
		<li><strong>VPN</strong> is inconvenient as I don’t want to connect to VPN just for RDP when I don’t need it otherwise.</li>
		<li><strong>SSH port forwarding</strong> is not performant for two things: Double-encryption and lack of UDP support.</li>
	</ul>
	<p>The question arises that if SSH access is sufficiently convenient, why not use it as an authentication and authorization mechanism? So I came up with this:</p>
	<ul>
		<li>
			<p>A pre-configured iptables rule set to allow RDP access from a specific IP set. For example:</p>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="k">*</span>filter
:FORWARD DROP
<span class="nt">-A</span> FORWARD <span class="nt">-d</span> 192.0.2.1 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 3389 <span class="nt">-m</span> <span class="nb">set</span> <span class="nt">--set</span> ibug <span class="nt">-j</span> ACCEPT

<span class="k">*</span>nat
<span class="nt">-A</span> RDPForward <span class="nt">-p</span> tcp <span class="nt">--dport</span> 3389 <span class="nt">-j</span> DNAT <span class="nt">--to-destination</span> 192.0.2.1:3389
<span class="nt">-A</span> RDPForward <span class="nt">-p</span> udp <span class="nt">--dport</span> 3389 <span class="nt">-j</span> DNAT <span class="nt">--to-destination</span> 192.0.2.1:3389
</code></pre>
				</div>
    </div>
		</li>
		<li>
			<p>A way to keep the client address in the set for the duration of the SSH session. I use SSH user rc file to proactively refresh it:</p>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c"># rwxr-xr-x ~/.ssh/rc</span>

<span class="k">if</span> <span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="nv">$BASH</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
  </span><span class="nb">exec</span> /bin/bash <span class="nt">--</span> <span class="s2">"</span><span class="nv">$0</span><span class="s2">"</span> <span class="s2">"</span><span class="nv">$@</span><span class="s2">"</span>
  <span class="nb">exit </span>1
<span class="k">fi

</span><span class="nv">_ssh_client</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">SSH_CONNECTION</span><span class="p">%% *</span><span class="k">}</span><span class="s2">"</span>
<span class="nv">_ppid</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>ps <span class="nt">-o</span> <span class="nv">ppid</span><span class="o">=</span> <span class="si">$(</span>ps <span class="nt">-o</span> <span class="nv">ppid</span><span class="o">=</span> <span class="nv">$PPID</span><span class="si">))</span><span class="s2">"</span>

<span class="nb">nohup</span> ~/.local/bin/_ssh_refresh_client <span class="s2">"</span><span class="nv">$_ssh_client</span><span class="s2">"</span> <span class="s2">"</span><span class="nv">$_ppid</span><span class="s2">"</span> &amp;&gt;/dev/null &amp; <span class="nb">exit </span>0
</code></pre>
				</div>
    </div>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="c">#!/bin/sh</span>
<span class="c"># rwxr-xr-x ~/.local/bin/_ssh_refresh_client</span>
<span class="nv">_ssh_client</span><span class="o">=</span><span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span>
<span class="nv">_ppid</span><span class="o">=</span><span class="s2">"</span><span class="nv">$2</span><span class="s2">"</span>
<span class="k">while </span><span class="nb">kill</span> <span class="nt">-0</span> <span class="s2">"</span><span class="nv">$_ppid</span><span class="s2">"</span> 2&gt;/dev/null<span class="p">;</span> <span class="k">do
  </span><span class="nb">sudo </span>ipset <span class="nt">-exist</span> add ibug <span class="s2">"</span><span class="nv">$_ssh_client</span><span class="s2">"</span> <span class="nb">timeout </span>300
  <span class="nb">sleep </span>60
<span class="k">done
</span><span class="nb">exit </span>0
</code></pre>
				</div>
    </div>
		</li>
	</ul>
	<p>The idea is to refresh (<code class="language-plaintext highlighter-rouge">ipset add</code> with timeout) the IPset entry as long as the SSH session remains. When SSH disconnects, the script stops refreshing and IPset will clean it up after the specified time.</p>
	<p>To determine the presence of the associated SSH session, the scripts finds the PID of the “session manager process”. The “parent PID” is read twice because <code class="language-plaintext highlighter-rouge">sshd</code> double-forks. The client address is conveniently provided in the environment variable, so putting all these together yields precisely what I need.</p>
	<p>The only caveat is the use of <code class="language-plaintext highlighter-rouge">sudo</code>, as <code class="language-plaintext highlighter-rouge">ipset</code> requires <code class="language-plaintext highlighter-rouge">CAP_NET_ADMIN</code> for interacting with the kernel network stack. It’s certainly possible to write an SUID binary as a wrapper, but for me configuring passwordless sudo for the <code class="language-plaintext highlighter-rouge">ipset</code> command satisfies my demands.</p>
	<p>So now whenever I need to RDP to my computer through this forwarded port on the public internet, I can just SSH into the gateway and it’ll automatically grant me 5 minutes of RDP access from this specific network. All traffic forwarding is done in the kernel with no extra encapsulation or encryption, ensuring the best possible performance for both the endpoints and the gateway router itself.</p>
	]]></content><author><name>iBug</name></author><category term="linux" /><category term="windows" /><category term="networking" /><summary type="html"><![CDATA[Today I stumbled upon this V2EX post (Simplified Chinese) where the OP shared their PowerShell implementation of a “makeshift fail2ban” for RDP (their GitHub repository). Their script looked very clean and robust, but needless to say, it is unnecessarily difficult on Windows. So on this rare (maybe?) occasion I decide to share my firewall for securing RDP access to my Windows hosts.]]></summary></entry></feed>