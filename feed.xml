<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ibug.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ibug.io/" rel="alternate" type="text/html" /><updated>2024-05-17T19:44:49+00:00</updated><id>https://ibug.io/feed.xml</id><title type="html">iBug</title><subtitle>The little personal site for iBug</subtitle><author><name>iBug</name></author><entry><title type="html">Migrating Ubuntu onto ZFS</title><link href="https://ibug.io/blog/2024/05/migrate-rootfs-to-zfs/" rel="alternate" type="text/html" title="Migrating Ubuntu onto ZFS" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T23:37:49+00:00</updated><id>https://ibug.io/blog/2024/05/migrate-rootfs-to-zfs</id><content type="html" xml:base="https://ibug.io/blog/2024/05/migrate-rootfs-to-zfs/"><![CDATA[<p>As part of a planned disk migration, I decided to move my Ubuntu installation from a traditional ext4 setup to ZFS.
				I did a lot of preparation and research, but things went much smoother than I had previously anticipated.
				I did not even have to consult IPMI for any recovery.</p>
			<p>Existing partition layout:</p>
			<div class="language-console highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>fdisk <span class="nt">-l</span> /dev/nvme1n1
<span class="go">[...]
Device             Start        End    Sectors  Size Type
/dev/nvme1n1p1      2048    1050623    1048576  512M EFI System
/dev/nvme1n1p2   1050624  269486079  268435456  128G Linux filesystem
/dev/nvme1n1p3 269486080 3907029134 3637543055  1.7T Solaris /usr &amp; Apple ZFS
</span></code></pre>
				</div>
			</div>
			<p>Since I already have <code class="language-plaintext highlighter-rouge">/home</code> running on ZFS <code class="language-plaintext highlighter-rouge">pool0</code>, there’s not much to prepare.
				All I need to move is the rootfs itself, which has around 20&nbsp;GB of data.</p>
			<p>Start by installing anything necessary:</p>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code>apt <span class="nb">install </span>zfs-initramfs arch-install-scripts
</code></pre>
				</div>
			</div>
			<p>Then create the dataset layout:</p>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="c"># pool0 already has xattr=sa</span>
zfs create <span class="se">\</span>
  <span class="nt">-o</span> <span class="nv">canmount</span><span class="o">=</span>off <span class="se">\</span>
  <span class="nt">-o</span> <span class="nv">mountpoint</span><span class="o">=</span>none <span class="se">\</span>
  <span class="nt">-o</span> <span class="nv">acltype</span><span class="o">=</span>posix <span class="se">\</span>
  pool0/ROOT
zfs create <span class="nt">-o</span> <span class="nv">mountpoint</span><span class="o">=</span>/mnt/new pool0/ROOT/ubuntu

rsync <span class="nt">-avSHAXx</span> <span class="nt">--delete</span> / /mnt/new/
</code></pre>
				</div>
			</div>
			<p>Now there’s a little deviation from common setup.
				I don’t trust GRUB’s ZFS support, so I’m going to merge <code class="language-plaintext highlighter-rouge">/boot</code> into the EFI partition (which has a decent 512&nbsp;MB of capacity).
				This is a decision made after surveying my friends’ setup.</p>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="c"># Merge data</span>
rsync <span class="nt">-ax</span> /boot/ /boot/efi/ <span class="c"># Ignore any errors</span>
umount /boot/efi
vim /etc/fstab
<span class="c"># Change /boot/efi to /boot</span>
<span class="c"># Also remove the current rootfs entry</span>
systemctl daemon-reload
mount /boot
</code></pre>
				</div>
			</div>
			<p>Now prepare GRUB:</p>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code>zpool <span class="nb">set </span><span class="nv">bootfs</span><span class="o">=</span>pool0/ROOT/ubuntu pool0
mount <span class="nt">-o</span> <span class="nb">bind</span> /boot /mnt/new/boot
arch-chroot /mnt/new
</code></pre>
				</div>
			</div>
			<div class="language-console highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>grub-install
<span class="go">Installing for x86_64-efi platform.
grub-install: error: cannot find EFI directory.
</span></code></pre>
				</div>
			</div>
			<p>Well, if only <code class="language-plaintext highlighter-rouge">grub-install</code> didn’t hard-code <code class="language-plaintext highlighter-rouge">/boot/efi</code> (which is against the FHS standard anyways).
				Fortunately, I recall a small detail that could make this work in another convenient way:</p>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code>dpkg-reconfigure grub-efi-amd64
</code></pre>
				</div>
			</div>
			<p>Also regenerate GRUB configuration:</p>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code>zfs <span class="nb">set </span><span class="nv">mountpoint</span><span class="o">=</span>/ pool0/ROOT/ubuntu
update-grub
</code></pre>
				</div>
			</div>
			<p>Now double-check the GRUB configuration at <code class="language-plaintext highlighter-rouge">/boot/grub/grub.cfg</code> and make sure there are lines like this:</p>
			<div class="language-text highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code>linux /vmlinuz [...] root=ZFS=pool0/ROOT/ubuntu [...]
</code></pre>
				</div>
			</div>
			<p>After verifying paths to the kernel and the initrd image are correct, reboot:</p>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code>reboot
</code></pre>
				</div>
			</div>
			<p>In just a minute, I noticed my server came back up.
				Time to confirm everything is working as expected:</p>
			<div class="language-console highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>mount
<span class="go">pool0/ROOT/ubuntu on / type zfs (rw,relatime,xattr,posixacl,casesensitive)

</span><span class="gp">#</span><span class="w"> </span><span class="nb">df</span> <span class="nt">-h</span> /
<span class="go">Filesystem         Size  Used Avail Use% Mounted on
pool0/ROOT/ubuntu  1.2T   11G  1.1T   1% /

</span><span class="gp">#</span><span class="w"> </span>zfs get compressratio pool0/ROOT
<span class="go">NAME        PROPERTY       VALUE  SOURCE
pool0/ROOT  compressratio  2.02x  -
</span></code></pre>
				</div>
			</div>
			<p>The last thing is to rewrite my rootfs backup script to take snapshots directly, instead of rsync-ing to another ZFS pool before taking a snapshot there.
				After taking a snapshot, I can also send it away as a “backup against disk failure”.</p>
			<p>A slightly revised version of my snapshotting script, sans the sending part:</p>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="c">#!/bin/sh</span>

<span class="nb">set</span> <span class="nt">-e</span>

<span class="nv">DATASET</span><span class="o">=</span>pool0/ROOT/ubuntu
<span class="nv">DATE</span><span class="o">=</span><span class="si">$(</span><span class="nb">date</span> +%Y%m%d<span class="si">)</span>
<span class="nv">SNAPSHOT</span><span class="o">=</span><span class="s2">"</span><span class="nv">$DATASET</span><span class="s2">@</span><span class="nv">$DATE</span><span class="s2">"</span>
<span class="nv">RETENTION_DAYS</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">1</span><span class="k">:-</span><span class="nv">7</span><span class="k">}</span><span class="s2">"</span>
<span class="nv">RETENTION</span><span class="o">=</span><span class="s2">"</span><span class="k">$((</span>RETENTION_DAYS <span class="o">*</span> <span class="m">86400</span><span class="k">))</span><span class="s2">"</span>

<span class="nv">NOW</span><span class="o">=</span><span class="s2">"</span><span class="k">$((</span><span class="si">$(</span><span class="nb">date</span> +%s<span class="si">)</span> <span class="o">-</span> <span class="m">3600</span><span class="k">))</span><span class="s2">"</span>
<span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="si">$(</span>zfs list <span class="nt">-Hpo</span> name <span class="s2">"</span><span class="nv">$SNAPSHOT</span><span class="s2">"</span><span class="si">)</span><span class="s2">"</span> <span class="o">=</span> <span class="s2">"</span><span class="nv">$SNAPSHOT</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
  </span><span class="nb">echo</span> <span class="s2">"Snapshot exists: </span><span class="nv">$SNAPSHOT</span><span class="s2">"</span>
<span class="k">else
  </span>zfs snapshot <span class="nt">-ro</span> ibug:retention<span class="o">=</span><span class="s2">"</span><span class="nv">$RETENTION</span><span class="s2">"</span> <span class="s2">"</span><span class="nv">$SNAPSHOT</span><span class="s2">"</span>
<span class="k">fi

</span>zfs list <span class="nt">-Hpt</span> snapshot <span class="nt">-o</span> name,creation,ibug:retention <span class="s2">"</span><span class="nv">$DATASET</span><span class="s2">"</span> |
  <span class="k">while </span><span class="nb">read</span> <span class="nt">-r</span> zNAME zCREATION zRETENTION<span class="p">;</span> <span class="k">do
  if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$zRETENTION</span><span class="s2">"</span> <span class="o">=</span> <span class="s2">"-"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="c"># assume default value</span>
    <span class="nv">zRETENTION</span><span class="o">=</span><span class="s2">"</span><span class="k">$((</span><span class="m">7</span> <span class="o">*</span> <span class="m">86400</span><span class="k">))</span><span class="s2">"</span>
  <span class="k">fi
  </span><span class="nv">UNTIL</span><span class="o">=</span><span class="s2">"</span><span class="k">$((</span>zCREATION <span class="o">+</span> zRETENTION<span class="k">))</span><span class="s2">"</span>
  <span class="nv">UNTIL_DATE</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span><span class="nb">date</span> <span class="nt">-d</span> <span class="s2">"@</span><span class="nv">$UNTIL</span><span class="s2">"</span> <span class="s2">"+%Y-%m-%d %H:%M:%S"</span><span class="si">)</span><span class="s2">"</span>
  <span class="nb">echo</span> <span class="s2">"</span><span class="nv">$zNAME</span><span class="s2">: </span><span class="nv">$UNTIL_DATE</span><span class="s2">"</span>
  <span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$NOW</span><span class="s2">"</span> <span class="nt">-ge</span> <span class="s2">"</span><span class="nv">$UNTIL</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span>zfs destroy <span class="nt">-rv</span> <span class="s2">"</span><span class="nv">$zNAME</span><span class="s2">"</span>
  <span class="k">fi
done</span>
</code></pre>
				</div>
			</div>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="c"># crontab</span>
15 4 <span class="k">*</span> <span class="k">*</span> 1,5     /root/backup.sh 30
15 4 <span class="k">*</span> <span class="k">*</span> 0,2-4,6 /root/backup.sh  7
</code></pre>
				</div>
			</div>
			]]></content><author><name>iBug</name></author><category term="linux" /><category term="server" /><category term="zfs" /><summary type="html"><![CDATA[As part of a planned disk migration, I decided to move my Ubuntu installation from a traditional ext4 setup to ZFS. I did a lot of preparation and research, but things went much smoother than I had previously anticipated. I did not even have to consult IPMI for any recovery.]]></summary></entry><entry><title type="html">Reload SSL certificates with systemd</title><link href="https://ibug.io/blog/2024/03/reload-ssl-cert-with-systemd/" rel="alternate" type="text/html" title="Reload SSL certificates with systemd" /><published>2024-03-31T00:00:00+00:00</published><updated>2024-04-01T18:23:24+00:00</updated><id>https://ibug.io/blog/2024/03/reload-ssl-cert-with-systemd</id><content type="html" xml:base="https://ibug.io/blog/2024/03/reload-ssl-cert-with-systemd/"><![CDATA[<p>Recently I relinquished an old domain on my server and had to re-issue a certificate to drop that domain off.
			Previously it ran Let’s Encrypt’s official client Certbot, set up back in 2019.
			All my recent setups have been using acme.sh, so I figured that this was a perfect chance to switch this one over as well.</p>
		<p>Getting acme.sh to issue a new certificate for my updated domain list is easy enough and out of scope for this article.
			But when it comes to reloading the certificate for services using it, I have to think twice.
			Back in the days when Nginx was the sole consumer of the certificate, I directly referenced the certificate files in <code class="language-plaintext highlighter-rouge">/etc/letsencrypt/live/</code> from Nginx config, and somehow slappped a <code class="language-plaintext highlighter-rouge">systemctl reload nginx</code> into crontab to handle the reload.
			Now that there are multiple services using the certificate, I no longer consider it a good idea to reload all the services in a crontab.
			There has to be a better way.</p>
		<p>Since all my services are managed by systemd, using an extra “service” or whatever unit to group them together seems like a better idea.
			Systemd’s <code class="language-plaintext highlighter-rouge">ReloadPropagatedFrom=</code> option and its inverse <code class="language-plaintext highlighter-rouge">PropagatesReloadTo=</code> immediately come to mind. With the right direction, it’s easy to Google out this answer: <a href="https://unix.stackexchange.com/q/334471/211239">How do I reload a group of systemd services?</a></p>
		<p>Realizing that “target” is the simplest unit type in systemd’s abstraction, this is the minimum that suits my needs.</p>
		<div class="language-ini highlighter-rouge">
			<div class="highlight">
				<pre class="highlight"><code><span class="c"># /etc/systemd/system/ssl-certificate.target
</span><span class="nn">[Unit]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">SSL certificates reload helper</span>
<span class="py">PropagatesReloadTo</span><span class="p">=</span><span class="s">nginx.service</span>
<span class="py">PropagatesReloadTo</span><span class="p">=</span><span class="s">postfix.service</span>
</code></pre>
			</div>
		</div>
		<p>Then, following the above Unix &amp; Linux answer, here’s a “path” unit that lets systemd monitor the certificate files for changes.</p>
		<div class="language-ini highlighter-rouge">
			<div class="highlight">
				<pre class="highlight"><code><span class="c"># /etc/systemd/system/ssl-certificate.path
</span><span class="nn">[Unit]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">SSL certificate reload helper</span>
<span class="py">Wants</span><span class="p">=</span><span class="s">%N.target</span>

<span class="nn">[Path]</span>
<span class="py">PathChanged</span><span class="p">=</span><span class="s">/etc/ssl/private/%H/cert.pem</span>

<span class="nn">[Install]</span>
<span class="py">WantedBy</span><span class="p">=</span><span class="s">multi-user.target</span>
</code></pre>
			</div>
		</div>
		<p>The <code class="language-plaintext highlighter-rouge">Wants=</code> setting here ensure that the corresponding target unit is activated, otherwise it cannot be <code class="language-plaintext highlighter-rouge">reload</code>ed.</p>
		<p>There’s one deficiency in the answer above: A “path” unit can only <em>activate</em> another unit, not <em>reload</em> it. So I still have to create a oneshot service that calls <code class="language-plaintext highlighter-rouge">systemctl reload</code> on the target, which itself can then be activated by the “path” unit.</p>
		<div class="language-ini highlighter-rouge">
			<div class="highlight">
				<pre class="highlight"><code><span class="c"># /etc/systemd/system/ssl-certificate.service
</span><span class="nn">[Unit]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">SSL certificate reload helper</span>
<span class="py">StartLimitIntervalSec</span><span class="p">=</span><span class="s">5s</span>
<span class="py">StartLimitBurst</span><span class="p">=</span><span class="s">2</span>

<span class="nn">[Service]</span>
<span class="py">Type</span><span class="p">=</span><span class="s">oneshot</span>
<span class="py">ExecStart</span><span class="p">=</span><span class="s">/bin/systemctl reload %N.target</span>
</code></pre>
			</div>
		</div>
		<p>It’s important that this service comes with <code class="language-plaintext highlighter-rouge">Type=oneshot</code> and <em>without</em> <code class="language-plaintext highlighter-rouge">RemainAfterExit=yes</code>, so that it can be repeatedly activated by the “path” unit.</p>
		<p>Now I can test if things work:</p>
		<div class="language-shell highlighter-rouge">
			<div class="highlight">
				<pre class="highlight"><code>systemctl daemon-reload
systemctl <span class="nb">enable</span> <span class="nt">--now</span> ssl-certificate.path
acme.sh <span class="nt">--install-cert</span> <span class="nt">-d</span> <span class="s2">"</span><span class="nv">$HOSTNAME</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--cert-file</span> <span class="s2">"/etc/ssl/private/</span><span class="nv">$HOSTNAME</span><span class="s2">/cert.pem"</span> <span class="se">\</span>
  <span class="nt">--key-file</span> <span class="s2">"/etc/ssl/private/</span><span class="nv">$HOSTNAME</span><span class="s2">/privkey.pem"</span> <span class="se">\</span>
  <span class="nt">--fullchain-file</span> <span class="s2">"/etc/ssl/private/</span><span class="nv">$HOSTNAME</span><span class="s2">/fullchain.pem"</span>
</code></pre>
			</div>
		</div>
		<p>And then inspect the services:</p>
		<div class="language-console highlighter-rouge">
			<div class="highlight">
				<pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>systemctl status nginx.service
<span class="go">[...]
Mar 31 19:20:11 hostname systemd[1]: Reloading A high performance web server and a reverse proxy server...
Mar 31 19:20:12 hostname systemd[1]: Reloaded A high performance web server and a reverse proxy server.

</span><span class="gp">$</span><span class="w"> </span>systemctl status postfix.service
<span class="go">[...]
Mar 31 19:20:11 hostname systemd[1]: Reloading Postfix Mail Transport Agent...
Mar 31 19:20:12 hostname systemd[1]: Reloaded Postfix Mail Transport Agent.
</span></code></pre>
			</div>
		</div>
		<p>So now, job done. As acme.sh stores install information, the next time these certificates are renewed, acme.sh will automatically copy them over to <code class="language-plaintext highlighter-rouge">/etc/ssl/private/$HOSTNAME/</code>, and systemd will pick up the changes and reload the services.</p>
		]]></content><author><name>iBug</name></author><category term="linux" /><summary type="html"><![CDATA[Recently I relinquished an old domain on my server and had to re-issue a certificate to drop that domain off. Previously it ran Let’s Encrypt’s official client Certbot, set up back in 2019. All my recent setups have been using acme.sh, so I figured that this was a perfect chance to switch this one over as well.]]></summary></entry><entry><title type="html">I almost broke our lab’s storage server…</title><link href="https://ibug.io/blog/2024/03/stupid-dell-recovery/" rel="alternate" type="text/html" title="I almost broke our lab’s storage server…" /><published>2024-03-13T00:00:00+00:00</published><updated>2024-03-27T18:34:21+00:00</updated><id>https://ibug.io/blog/2024/03/stupid-dell-recovery</id><content type="html" xml:base="https://ibug.io/blog/2024/03/stupid-dell-recovery/"><![CDATA[<p>Recently we discovered that both SSDs on our storage server were giving worrisome SMART values, so we started replacing them.
		One of them was used only for ZFS L2ARC, so pulling it out was easy.
		The other runs the rootfs and we couldn’t touch it for the time being, so we inserted a backup drive thinking we can migrate the OS later on.</p>
	<p>After returning from the datacenter, I start working on the migration.
		The initial steps are nothing but ordinary:</p>
	<ul>
		<li>Examine the spare drive to ensure there’s no important data on it, then wipe it (<code class="language-plaintext highlighter-rouge">blkdiscard -f /dev/sdb</code>).</li>
		<li>Create the partition table that closely resembles the current system drive’s layout: 100&nbsp;MB for the EFI partition (down from 512&nbsp;MB), 32&nbsp;GB for rootfs, and the rest for an LVM PV.</li>
		<li>Format the partitions: <code class="language-plaintext highlighter-rouge">mkfs.vfat /dev/sdb1</code>, <code class="language-plaintext highlighter-rouge">mkfs.ext4 /dev/sdb2</code>, <code class="language-plaintext highlighter-rouge">pvcreate /dev/sdb3</code>.</li>
		<li>Copy the rootfs over: <code class="language-plaintext highlighter-rouge">mount /dev/sdb2 /t</code>, <code class="language-plaintext highlighter-rouge">rsync -aHAXx / /t</code>.</li>
		<li>Reinstall the bootloader: <code class="language-plaintext highlighter-rouge">mount /dev/sdb1 /t/boot/efi</code>, <code class="language-plaintext highlighter-rouge">arch-chroot /t</code>, <code class="language-plaintext highlighter-rouge">grub-install</code> (target is <code class="language-plaintext highlighter-rouge">x86_64-efi</code>), <code class="language-plaintext highlighter-rouge">update-grub</code>.</li>
		<li>Start migrating LVs: <code class="language-plaintext highlighter-rouge">vgextend pve /dev/sdb3</code>, <code class="language-plaintext highlighter-rouge">pvmove /dev/sda3 /dev/sdb3</code>.</li>
	</ul>
	<p>At this point, a quick thought emerges: This is not the final drive to run the system on and is only here for the transitional period.
		A second migration is planned when the new SSD arrives. So why not take this chance and move the rootfs onto LVM as well?</p>
	<p>With that in mind, I hit Ctrl-C to <code class="language-plaintext highlighter-rouge">pvmove</code>, unbeknownst that it’s interruptible and terminating the <code class="language-plaintext highlighter-rouge">pvmove</code> process only pauses the operation.
		For a moment, I thought I successfully canceled it and tried to re-partition the new drive.
		Since the new PV is still in use by the suspended <code class="language-plaintext highlighter-rouge">pvmove</code> operation, the kernel would not accept any changes to <code class="language-plaintext highlighter-rouge">/dev/sdb3</code>.
		During this process, I deleted and recreated the new rootfs (<code class="language-plaintext highlighter-rouge">/dev/sdb2</code>) and the new PV (<code class="language-plaintext highlighter-rouge">/dev/sdb3</code>) many times, and even tried manually editing LVM metadata (via <code class="language-plaintext highlighter-rouge">vgcfgbackup pve</code>, edit <code class="language-plaintext highlighter-rouge">/etc/lvm/backup/pve</code> and <code class="language-plaintext highlighter-rouge">vgcfgrestore pve</code>), before finally giving up and rebooting the system.</p>
	<p>As a daily dose for a SysAdmin, the server didn’t boot up as expected.
		I fired up a browser to connect to the machine’s IPMI, only to find that the remote console feature for iDRAC 9 was locked behind a paywall for goodness’ sake.
		Thanks to God almighty Dell, things have been unnecessarily more complicated than ever before.
		I carefully recalled every step taken and quickly identified the problem - one important thing forgotten - GRUB was successfully reinstalled on the new EFI partition (which was somehow left intact during the whole fiddling process), pointing to the now-deleted new root partition, and so it’s now stuck with GRUB.</p>
	<p>Fortunately, out of precaution, I had previously configured the IPMI with serial-over-LAN, so I at least still have serial access to the server with <code class="language-plaintext highlighter-rouge">ipmitool</code>. This saved me from a trip back to the datacenter.</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>ipmitool <span class="nt">-I</span> lanplus <span class="nt">-H</span> &lt;ip&gt; <span class="nt">-U</span> &lt;user&gt; <span class="nt">-P</span> &lt;password&gt; sol activate
</code></pre>
		</div>
	</div>
	<p>And better yet, this iDRAC 9 can change BIOS settings, most notably the boot order and one-time boot override. This definitely helped the most in the absence of that goddamn remote console.</p>
	<p><img src="/image/server/idrac-boot-override.png" alt="image" /></p>
	<p>After some trial and error, I got myself into the GRUB command line, and it didn’t look quite well:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">grub rescue&gt;</span><span class="w"> 
</span></code></pre>
		</div>
	</div>
	<p>There’s pretty much just the <code class="language-plaintext highlighter-rouge">ls</code> command, and it doesn’t even recognize the EFI partition (FAT32 filesystem). With some more twiddling, I found this “rescue mode” capable of reading ext4, which shed some light to the situation.</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">grub rescue&gt;</span><span class="w"> </span><span class="nb">set </span><span class="nv">root</span><span class="o">=(</span>hd0,gpt2<span class="o">)</span>
<span class="gp">grub rescue&gt;</span><span class="w"> </span><span class="nb">ls</span> /boot/grub
<span class="go">fonts  grub.cfg  grubenv  locale  unicode.pf2  x86_64-efi
</span></code></pre>
		</div>
	</div>
	<p>Now things began to turn to the upswing.</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">grub rescue&gt;</span><span class="w"> </span><span class="nb">set </span><span class="nv">prefix</span><span class="o">=</span>/boot/grub
<span class="gp">grub rescue&gt;</span><span class="w"> </span>insmod normal
<span class="gp">grub rescue&gt;</span><span class="w"> </span>normal
</code></pre>
		</div>
	</div>
	<p>In a few seconds, I was delighted to discover that the system was up and running, and continued migrating the rootfs.</p>
	<p>After everything’s done, out of every precaution, I installed <code class="language-plaintext highlighter-rouge">grub-efi-amd64-signed</code>, which provides a large, monolithic <code class="language-plaintext highlighter-rouge">grubx64.efi</code> that has all the “optional” modules built-in, so it no longer relies on the filesystem for, e.g., LVM support, in case a similar disaster happens again.</p>
	<h2 id="anecdote">Anecdote</h2>
	<p>When trying to remove the faulty drive from the server, I at first made a wrong recall for its position, and we instead pulled out a running large-capacity HDD. Luckily it was not damaged, so we quickly inserted it back. Thanks to ZFS’s design, it automatically triggered a resilver, which completed in just a blink.</p>
	<div class="language-text highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code># zpool status
  pool: rpool
 state: ONLINE
  scan: resilvered 63.4M in 00:00:03 with 0 errors on Tue Mar 12 17:03:23 2024
</code></pre>
		</div>
	</div>
	<p>If this were a hardware RAID, a tedious and time-consuming rebuild would have been inevitable. It’s only with ZFS that this rapid recovery is possible.</p>
	<h2 id="conclusion">Conclusion</h2>
	<p>This incident was a good lesson for me, and some big takeaways I’d draw:</p>
	<ul>
		<li>Don’t panic under pressure.</li>
		<li>Use ZFS so you can sleep well at night.</li>
		<li>Fuck Dell, next time buy from another vendor that doesn’t lock basic features behind a paywall.</li>
	</ul>
	<p>Plus, the correct way to cancel a <code class="language-plaintext highlighter-rouge">pvmove</code> operation is in <a href="https://linux.die.net/man/8/pvmove"><code class="language-plaintext highlighter-rouge">man 8 pvmove</code></a>, and it’s right at the 2nd paragraph of the Description section.</p>
	]]></content><author><name>iBug</name></author><category term="linux" /><category term="server" /><summary type="html"><![CDATA[Recently we discovered that both SSDs on our storage server were giving worrisome SMART values, so we started replacing them. One of them was used only for ZFS L2ARC, so pulling it out was easy. The other runs the rootfs and we couldn’t touch it for the time being, so we inserted a backup drive thinking we can migrate the OS later on.]]></summary></entry><entry><title type="html">My firewall solution for RDP</title><link href="https://ibug.io/blog/2024/02/linux-firewall-for-rdp/" rel="alternate" type="text/html" title="My firewall solution for RDP" /><published>2024-02-28T00:00:00+00:00</published><updated>2024-02-28T18:33:21+00:00</updated><id>https://ibug.io/blog/2024/02/linux-firewall-for-rdp</id><content type="html" xml:base="https://ibug.io/blog/2024/02/linux-firewall-for-rdp/"><![CDATA[<p>Today I stumbled upon <a href="https://www.v2ex.com/t/1019147">this V2EX post</a> (Simplified Chinese) where the OP shared their PowerShell implementation of a “makeshift fail2ban” for RDP (<a href="https://github.com/Qetesh/rdpFail2Ban">their GitHub repository</a>). Their script looked very clean and robust, but needless to say, it is unnecessarily difficult on Windows. So on this rare (maybe?) occasion I decide to share my firewall for securing RDP access to my Windows hosts.</p>
	<p><strong>None</strong> of my Windows hosts (PCs and VMs) has their RDP port exposed to the public internet directly, and they’re all connected to my mesh VPN (which is out of scope for this blog article). My primary public internet entry gateway for the intranet runs Debian with fully manually configured iptables-based firewall, and I frequently work on it through SSH.</p>
	<p>My goal is to expose the RDP port only to myself. There are a few obvious solutions eliminated for different reasons:</p>
	<ul>
		<li><strong>VPN</strong> is inconvenient as I don’t want to connect to VPN just for RDP when I don’t need it otherwise.</li>
		<li><strong>SSH port forwarding</strong> is not performant for two things: Double-encryption and lack of UDP support.</li>
	</ul>
	<p>The question arises that if SSH access is sufficiently convenient, why not use it as an authentication and authorization mechanism? So I came up with this:</p>
	<ul>
		<li>
			<p>A pre-configured iptables rule set to allow RDP access from a specific IP set. For example:</p>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="k">*</span>filter
:FORWARD DROP
<span class="nt">-A</span> FORWARD <span class="nt">-d</span> 192.0.2.1 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 3389 <span class="nt">-m</span> <span class="nb">set</span> <span class="nt">--set</span> ibug <span class="nt">-j</span> ACCEPT

<span class="k">*</span>nat
<span class="nt">-A</span> RDPForward <span class="nt">-p</span> tcp <span class="nt">--dport</span> 3389 <span class="nt">-j</span> DNAT <span class="nt">--to-destination</span> 192.0.2.1:3389
<span class="nt">-A</span> RDPForward <span class="nt">-p</span> udp <span class="nt">--dport</span> 3389 <span class="nt">-j</span> DNAT <span class="nt">--to-destination</span> 192.0.2.1:3389
</code></pre>
				</div>
    </div>
		</li>
		<li>
			<p>A way to keep the client address in the set for the duration of the SSH session. I use SSH user rc file to proactively refresh it:</p>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c"># rwxr-xr-x ~/.ssh/rc</span>

<span class="k">if</span> <span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="nv">$BASH</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
  </span><span class="nb">exec</span> /bin/bash <span class="nt">--</span> <span class="s2">"</span><span class="nv">$0</span><span class="s2">"</span> <span class="s2">"</span><span class="nv">$@</span><span class="s2">"</span>
  <span class="nb">exit </span>1
<span class="k">fi

</span><span class="nv">_ssh_client</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">SSH_CONNECTION</span><span class="p">%% *</span><span class="k">}</span><span class="s2">"</span>
<span class="nv">_ppid</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>ps <span class="nt">-o</span> <span class="nv">ppid</span><span class="o">=</span> <span class="si">$(</span>ps <span class="nt">-o</span> <span class="nv">ppid</span><span class="o">=</span> <span class="nv">$PPID</span><span class="si">))</span><span class="s2">"</span>

<span class="nb">nohup</span> ~/.local/bin/_ssh_refresh_client <span class="s2">"</span><span class="nv">$_ssh_client</span><span class="s2">"</span> <span class="s2">"</span><span class="nv">$_ppid</span><span class="s2">"</span> &amp;&gt;/dev/null &amp; <span class="nb">exit </span>0
</code></pre>
				</div>
    </div>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="c">#!/bin/sh</span>
<span class="c"># rwxr-xr-x ~/.local/bin/_ssh_refresh_client</span>
<span class="nv">_ssh_client</span><span class="o">=</span><span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span>
<span class="nv">_ppid</span><span class="o">=</span><span class="s2">"</span><span class="nv">$2</span><span class="s2">"</span>
<span class="k">while </span><span class="nb">kill</span> <span class="nt">-0</span> <span class="s2">"</span><span class="nv">$_ppid</span><span class="s2">"</span> 2&gt;/dev/null<span class="p">;</span> <span class="k">do
  </span><span class="nb">sudo </span>ipset <span class="nt">-exist</span> add ibug <span class="s2">"</span><span class="nv">$_ssh_client</span><span class="s2">"</span> <span class="nb">timeout </span>300
  <span class="nb">sleep </span>60
<span class="k">done
</span><span class="nb">exit </span>0
</code></pre>
				</div>
    </div>
		</li>
	</ul>
	<p>The idea is to refresh (<code class="language-plaintext highlighter-rouge">ipset add</code> with timeout) the IPset entry as long as the SSH session remains. When SSH disconnects, the script stops refreshing and IPset will clean it up after the specified time.</p>
	<p>To determine the presence of the associated SSH session, the scripts finds the PID of the “session manager process”. The “parent PID” is read twice because <code class="language-plaintext highlighter-rouge">sshd</code> double-forks. The client address is conveniently provided in the environment variable, so putting all these together yields precisely what I need.</p>
	<p>The only caveat is the use of <code class="language-plaintext highlighter-rouge">sudo</code>, as <code class="language-plaintext highlighter-rouge">ipset</code> requires <code class="language-plaintext highlighter-rouge">CAP_NET_ADMIN</code> for interacting with the kernel network stack. It’s certainly possible to write an SUID binary as a wrapper, but for me configuring passwordless sudo for the <code class="language-plaintext highlighter-rouge">ipset</code> command satisfies my demands.</p>
	<p>So now whenever I need to RDP to my computer through this forwarded port on the public internet, I can just SSH into the gateway and it’ll automatically grant me 5 minutes of RDP access from this specific network. All traffic forwarding is done in the kernel with no extra encapsulation or encryption, ensuring the best possible performance for both the endpoints and the gateway router itself.</p>
	]]></content><author><name>iBug</name></author><category term="linux" /><category term="windows" /><category term="networking" /><summary type="html"><![CDATA[Today I stumbled upon this V2EX post (Simplified Chinese) where the OP shared their PowerShell implementation of a “makeshift fail2ban” for RDP (their GitHub repository). Their script looked very clean and robust, but needless to say, it is unnecessarily difficult on Windows. So on this rare (maybe?) occasion I decide to share my firewall for securing RDP access to my Windows hosts.]]></summary></entry><entry><title type="html">Request limiting in Nginx</title><link href="https://ibug.io/blog/2024/01/nginx-limit-req/" rel="alternate" type="text/html" title="Request limiting in Nginx" /><published>2024-01-23T00:00:00+00:00</published><updated>2024-01-23T06:15:20+00:00</updated><id>https://ibug.io/blog/2024/01/nginx-limit-req</id><content type="html" xml:base="https://ibug.io/blog/2024/01/nginx-limit-req/"><![CDATA[<p>Nginx has a built-in module <code class="language-plaintext highlighter-rouge">limit_req</code> for rate-limiting requests, which does a decent job, except its documentation is not known for its conciseness, plus a few questionable design choices. I happen to have a specific need for this feature so I examined it a bit.</p>
	<p>As always, everything begins with <a href="https://nginx.org/en/docs/http/ngx_http_limit_req_module.html">the documentation</a>. A quick-start example is given:</p>
	<div class="language-nginx highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="k">http</span> <span class="p">{</span>
    <span class="kn">limit_req_zone</span> <span class="nv">$binary_remote_addr</span> <span class="s">zone=one:10m</span> <span class="s">rate=1r/s</span><span class="p">;</span>
    <span class="kn">...</span>
    <span class="s">server</span> <span class="p">{</span>
        <span class="kn">...</span>
        <span class="s">location</span> <span class="n">/search/</span> <span class="p">{</span>
            <span class="kn">limit_req</span> <span class="s">zone=one</span> <span class="s">burst=5</span><span class="p">;</span>
        <span class="p">}</span>
</code></pre>
		</div>
	</div>
	<p>The basis is the <code class="language-plaintext highlighter-rouge">limit_req_zone</code> directive, which defines a shared memory zone for storing the states of the rate-limiting. Its arguments include the key, the size and the name of the zone, followed by the average or sustained rate limit. The rate limit has two possible units: <code class="language-plaintext highlighter-rouge">r/s</code> or <code class="language-plaintext highlighter-rouge">r/m</code>. It also says</p>
	<blockquote>
		<p>The limitation is done using the “<a href="https://en.wikipedia.org/wiki/Leaky_bucket">leaky bucket</a>” method.</p>
	</blockquote>
	<p>So far so good, except the burst limit is … specified on where it’s used? Moving on for now.</p>
	<p>The <code class="language-plaintext highlighter-rouge">limit_req</code> directive specifies when the requests should be limited.</p>
	<blockquote>
		<p>If the requests rate exceeds the rate configured for a zone, their processing is delayed such that requests are processed at a defined rate.</p>
	</blockquote>
	<p>Seems pretty clear but slightly counter-intuitive. By default, burst requests are queued up and delayed until the rate is below the limit, whereas most common rate-limiting implementations would simply serve them.</p>
	<p>I find it easier to understand this model with a queue. Each key defines a queue where items are popped at the specified rate (e.g. <code class="language-plaintext highlighter-rouge">1r/s</code>). Incoming requests are added to the queue, and are only served <em>upon exiting</em> the queue. The queue size is defined by the burst limit, and excess requests are dropped when the queue is full.</p>
	<p><img src="/image/server/nginx-limit-req.png" alt="Default queue behavior" /></p>
	<p>The more common behavior, however, requires an extra option:</p>
	<blockquote>
		<p>If delaying of excessive requests while requests are being limited is not desired, the parameter <code class="language-plaintext highlighter-rouge">nodelay</code> should be used:</p>
		<div class="language-nginx highlighter-rouge">
			<div class="highlight">
				<pre class="highlight"><code><span class="k">limit_req</span> <span class="s">zone=one</span> <span class="s">burst=5</span> <span class="s">nodelay</span><span class="p">;</span>
</code></pre>
			</div>
  </div>
	</blockquote>
	<p>With <code class="language-plaintext highlighter-rouge">nodelay</code>, requests are served as soon as they <em>enter the queue</em>:</p>
	<p><img src="/image/server/nginx-limit-req-nodelay.png" alt="nodelay queue behavior" /></p>
	<p>The next confusing option, conflicting with <code class="language-plaintext highlighter-rouge">nodelay</code>, is <code class="language-plaintext highlighter-rouge">delay</code>:</p>
	<blockquote>
		<p>The <code class="language-plaintext highlighter-rouge">delay</code> parameter specifies a limit at which excessive requests become delayed. Default value is zero, i.e. all excessive requests are delayed.</p>
	</blockquote>
	<p>After a bit of fiddling, I realized the model is now like this:</p>
	<p><img src="/image/server/nginx-limit-req-delay.png" alt="delay queue behavior" /></p>
	<p>So what <code class="language-plaintext highlighter-rouge">delay</code> actually means is to delay requests after this “delay limit” is reached. In other words, requests are served as soon as they arrive at the n-th position in the front of the queue.</p>
	<p>During all these testing, I wasn’t happy with existing tools for testing, so I wrote my own one, despite its simplicity: <a href="https://gist.github.com/iBug/351b458633ff89fea0fc9f0edd07fc28">GitHub Gist</a>.</p>
	<p>With this new tool, I can now (textually) visualize the behavior of different options. Under the <code class="language-plaintext highlighter-rouge">burst=5</code> and <code class="language-plaintext highlighter-rouge">delay=1</code> setup, the output is like this:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>go run main.go <span class="nt">-i</span> 10ms <span class="nt">-c</span> 10 http://localhost/test
<span class="go">[1] Done [0s] [200 in 2ms]
[2] Done [10ms] [200 in 1ms]
[3] Done [21ms] [200 in 981ms]
[4] Done [31ms] [200 in 1.972s]
[5] Done [42ms] [200 in 2.962s]
[6] Done [53ms] [200 in 3.948s]
[7] Done [64ms] [503 in 0s]
[8] Done [75ms] [503 in 1ms]
[9] Done [85ms] [503 in 0s]
[10] Done [95ms] [503 in 0s]
</span></code></pre>
		</div>
	</div>
	<p>If you try the tool yourself, the HTTP status codes are colored for even better prominence.</p>
	<p>In the above example, the first request is served immediately as it also exits the queue immediately. The second request is queued at the front, and because <code class="language-plaintext highlighter-rouge">delay=1</code>, it’s also served immediately. Subsequent requests are queued up until the sixth when the queue becomes full. The seventh and thereafter are dropped.</p>
	<p>If we change <code class="language-plaintext highlighter-rouge">delay=0</code>, the output becomes:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>go run main.go <span class="nt">-i</span> 10ms <span class="nt">-c</span> 10 http://localhost/test
<span class="go">[1] Done [0s] [200 in 2ms]
[2] Done [10ms] [200 in 993ms]
[3] Done [21ms] [200 in 1.982s]
[4] Done [32ms] [200 in 2.973s]
[5] Done [43ms] [200 in 3.959s]
[6] Done [54ms] [200 in 4.949s]
[7] Done [65ms] [503 in 1ms]
[8] Done [75ms] [503 in 1ms]
[9] Done [85ms] [503 in 2ms]
[10] Done [96ms] [503 in 1ms]
</span></code></pre>
		</div>
	</div>
	<p>Still only the first 6 requests are served, but the 2nd to the 6th are delayed by an additional second due to the removal of <code class="language-plaintext highlighter-rouge">delay=1</code>.</p>
	<p>Under this model, the <code class="language-plaintext highlighter-rouge">nodelay</code> option can be understood as <code class="language-plaintext highlighter-rouge">delay=infinity</code>, while still respecting the <code class="language-plaintext highlighter-rouge">burst</code> limit.</p>
	<h2 id="one-more-question">One more question</h2>
	<p>Why is the burst limit specified at use time, instead of at zone definition? Only experiments can find out:</p>
	<div class="language-nginx highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="k">location</span> <span class="n">/a</span> <span class="p">{</span>
    <span class="kn">limit_req</span> <span class="s">zone=test</span> <span class="s">burst=1</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">location</span> <span class="n">/b</span> <span class="p">{</span>
    <span class="kn">limit_req</span> <span class="s">zone=test</span> <span class="s">burst=5</span><span class="p">;</span>
<span class="p">}</span>
</code></pre>
		</div>
	</div>
	<p>Then I fire up two simultaneous batches of 10 requests each to <code class="language-plaintext highlighter-rouge">/a</code> and <code class="language-plaintext highlighter-rouge">/b</code> respectively:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>go run main.go <span class="nt">-i</span> 10ms <span class="nt">-c</span> 10 http://localhost/a
<span class="go">[1] Done [0s] [200 in 2ms]
[2] Done [10ms] [200 in 992ms]
[3] Done [21ms] [503 in 0s]
[4] Done [32ms] [503 in 0s]
[5] Done [42ms] [503 in 0s]
[6] Done [53ms] [503 in 0s]
[7] Done [63ms] [503 in 0s]
[8] Done [73ms] [503 in 0s]
[9] Done [83ms] [503 in 0s]
[10] Done [94ms] [503 in 0s]
</span></code></pre>
		</div>
	</div>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>go run main.go <span class="nt">-i</span> 10ms <span class="nt">-c</span> 10 http://localhost/b
<span class="go">[1] Done [0s] [200 in 1.862s]
[2] Done [11ms] [200 in 2.852s]
[3] Done [21ms] [200 in 3.842s]
[4] Done [32ms] [200 in 4.832s]
[5] Done [43ms] [503 in 1ms]
[6] Done [54ms] [503 in 0s]
[7] Done [64ms] [503 in 0s]
[8] Done [75ms] [503 in 1ms]
[9] Done [85ms] [503 in 0s]
[10] Done [95ms] [503 in 1ms]
</span></code></pre>
		</div>
	</div>
	<p>As can be seen from the output, the batch to <code class="language-plaintext highlighter-rouge">/a</code> is served as usual, but the batch to <code class="language-plaintext highlighter-rouge">/b</code> is significantly delayed, and two fewer requests are served.</p>
	<p>If I reverse the order of sending the batches, the result is different again:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>go run main.go <span class="nt">-i</span> 10ms <span class="nt">-c</span> 10 http://localhost/b
<span class="go">[1] Done [0s] [200 in 2ms]
[2] Done [10ms] [200 in 993ms]
[3] Done [20ms] [200 in 1.982s]
[4] Done [31ms] [200 in 2.974s]
[5] Done [42ms] [200 in 3.963s]
[6] Done [52ms] [200 in 4.955s]
[7] Done [63ms] [503 in 0s]
[8] Done [74ms] [503 in 0s]
[9] Done [84ms] [503 in 0s]
[10] Done [95ms] [503 in 0s]
</span></code></pre>
		</div>
	</div>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>go run main.go <span class="nt">-i</span> 10ms <span class="nt">-c</span> 10 http://localhost/a
<span class="go">[1] Done [0s] [503 in 1ms]
[2] Done [10ms] [503 in 1ms]
[3] Done [20ms] [503 in 0s]
[4] Done [31ms] [503 in 0s]
[5] Done [42ms] [503 in 0s]
[6] Done [52ms] [503 in 0s]
[7] Done [63ms] [503 in 1ms]
[8] Done [73ms] [503 in 0s]
[9] Done [83ms] [503 in 0s]
[10] Done [93ms] [503 in 0s]
</span></code></pre>
		</div>
	</div>
	<p>This time the batch to <code class="language-plaintext highlighter-rouge">/b</code> is served as usual, but the entire batch to <code class="language-plaintext highlighter-rouge">/a</code> is rejected.</p>
	<p>I am now convinced that the queue itself is shared between <code class="language-plaintext highlighter-rouge">/a</code> and <code class="language-plaintext highlighter-rouge">/b</code>, and each <code class="language-plaintext highlighter-rouge">limit_req</code> directive decides for itself whether and when to serve the requests. So when <code class="language-plaintext highlighter-rouge">/a</code> is served first, the queue holds one burst request, and <code class="language-plaintext highlighter-rouge">/b</code> fills the queue up to 5 requests. When <code class="language-plaintext highlighter-rouge">/b</code> is served first, the queue is already holding 5 requests and leaves no room for <code class="language-plaintext highlighter-rouge">/a</code>. Similarly, with the <code class="language-plaintext highlighter-rouge">delay</code> option, each <code class="language-plaintext highlighter-rouge">limit_req</code> directive can still decide when the request is ready to serve.</p>
	<p>This is probably not the most straightforward design, and I can’t come up with a use case for this behavior. But at least now I understand how it works.</p>
	<h2 id="one-last-thing">One last thing</h2>
	<p>I originally wanted to set up a 403 page for banned clients, and wanted to limit the rate of log writing in case of an influx of requests. The limit_req module does provide a <code class="language-plaintext highlighter-rouge">$limit_req_status</code> variable which appears to be useful. This is what I ended up with:</p>
	<div class="language-nginx highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="k">limit_req_zone</span> <span class="nv">$binary_remote_addr</span> <span class="s">zone=403:64k</span> <span class="s">rate=1r/s</span><span class="p">;</span>

<span class="k">map</span> <span class="nv">$limit_req_status</span> <span class="nv">$loggable_403</span> <span class="p">{</span>
    <span class="kn">default</span> <span class="mi">0</span><span class="p">;</span>
    <span class="kn">PASSED</span> <span class="mi">1</span><span class="p">;</span>
    <span class="kn">DELAYED</span> <span class="mi">1</span><span class="p">;</span>
    <span class="kn">DELAYED_DRY_RUN</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">server</span> <span class="p">{</span>
    <span class="kn">access_log</span> <span class="n">/var/log/nginx/403/access.log</span> <span class="s">main</span> <span class="s">if=</span><span class="nv">$loggable_403</span><span class="p">;</span>
    <span class="kn">error_log</span> <span class="n">/var/log/nginx/403/error.log</span> <span class="s">warn</span><span class="p">;</span>
    <span class="kn">error_page</span> <span class="mi">403</span> <span class="n">/403.html</span><span class="p">;</span>
    <span class="kn">error_page</span> <span class="mi">404</span> <span class="p">=</span><span class="mi">403</span> <span class="n">/403.html</span><span class="p">;</span>
    <span class="kn">limit_req</span> <span class="s">zone=403</span><span class="p">;</span>
    <span class="kn">limit_req_status</span> <span class="mi">403</span><span class="p">;</span>
    <span class="kn">limit_req_log_level</span> <span class="s">info</span><span class="p">;</span>

    <span class="kn">location</span> <span class="n">/</span> <span class="p">{</span>
        <span class="kn">return</span> <span class="mi">403</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="kn">location</span> <span class="p">=</span> <span class="n">/403.html</span> <span class="p">{</span>
        <span class="kn">internal</span><span class="p">;</span>
        <span class="kn">root</span> <span class="n">/srv/nginx</span><span class="p">;</span>
        <span class="kn">sub_filter</span> <span class="s">"%remote_addr%"</span> <span class="s">"</span><span class="nv">$remote_addr</span><span class="s">"</span><span class="p">;</span>
        <span class="kn">sub_filter_once</span> <span class="no">off</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre>
		</div>
	</div>
	<p>With this setup, excessive requests are rejected by <code class="language-plaintext highlighter-rouge">limit_req</code> with a 403 status. Only <code class="language-plaintext highlighter-rouge">1r/s</code> passes through the rate limiting, which will carry the <code class="language-plaintext highlighter-rouge">PASSED</code> status and be logged, albeit still seeing the 403 page from the <code class="language-plaintext highlighter-rouge">return 403</code> rule. This does exactly what I want, so time to call it a day.</p>
	]]></content><author><name>iBug</name></author><category term="server" /><category term="nginx" /><summary type="html"><![CDATA[Nginx has a built-in module limit_req for rate-limiting requests, which does a decent job, except its documentation is not known for its conciseness, plus a few questionable design choices. I happen to have a specific need for this feature so I examined it a bit.]]></summary></entry><entry><title type="html">Visualizing Weather Forecast with Grafana</title><link href="https://ibug.io/blog/2024/01/weather-forecast-with-grafana/" rel="alternate" type="text/html" title="Visualizing Weather Forecast with Grafana" /><published>2024-01-08T00:00:00+00:00</published><updated>2024-01-08T04:40:54+00:00</updated><id>https://ibug.io/blog/2024/01/weather-forecast-with-grafana</id><content type="html" xml:base="https://ibug.io/blog/2024/01/weather-forecast-with-grafana/"><![CDATA[<p>Grafana is a great piece of software for visualizing data and monitoring. It’s outstanding at what it does when paired with a time-series database like InfluxDB, except this time I’m trying to get it to work as a weather forecast dashboard, instead of any historical time-series data.</p>
	<p>I choose <a href="https://open.caiyunapp.com/%E5%BD%A9%E4%BA%91%E5%A4%A9%E6%B0%94_API_%E4%B8%80%E8%A7%88%E8%A1%A8">CaiYun Weather (彩云天气) API</a> for having previous experience with it, as well as its unlimited free tier. I must admit that I initially came up with this idea for having seen the presence of <a href="https://grafana.com/grafana/plugins/marcusolsson-json-datasource/">JSON API datasource plugin</a> for Grafana, which reminds me of CaiYun’s JSON API being a perfect fit.</p>
	<h2 id="json-api-datasource">JSON API Datasource</h2>
	<p>Configuring the datasource seems easy at first, like just inserting the URL and configure HTTP headers as needed. Since CY’s API puts the API key in the URL path, there’s no headers to configure. So I can just put a single URL and save it.</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>https://api.caiyunapp.com/v2.5/TAkhjf8d1nlSlspN/121.6544,25.1552/hourly.json
</code></pre>
		</div>
	</div>
	<p>I choose the hourly API so I can have forecast for the upcoming 48 hours.</p>
	<p>So far this is a readily available datasource that I can query. But after reviewing the <a href="https://grafana.github.io/grafana-json-datasource/query-editor">JSON query editor</a>, I decided to chop off the last segments of the URL and leave just the part up to the API key:</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>https://api.caiyunapp.com/v2.5/TAkhjf8d1nlSlspN/
</code></pre>
		</div>
	</div>
	<p>The point here is, the query editor allows specifying an extra Path, which appears to be concatenated with this URL in the datasource configuration. Notably, I can then put the coordinates in a variable, use it in the query, and build a single dashboard for many cities.</p>
	<h2 id="dashboard-variables">Dashboard variables</h2>
	<p>Now that I have the query format planned, I can add a dashboard variable for selecting cities.</p>
	<p>First things first, since I’m going to use the same datasource for all panels, I first add a variable for the datasource and restrict it to “CaiYun Weather”:</p>
	<p><img src="/image/grafana/dashboard-variable-datasource.png" alt="Datasource variable" /></p>
	<p>Then I add a variable <code class="language-plaintext highlighter-rouge">$location</code> for the city name, and provide it with a list of cities I want to show. The variable type would be “Custom” since this is just a human-maintained list. There certainly are better ways like using a relational database or an external API, making it easier to update, but for now I’d like to keep it simple.</p>
	<div class="language-text highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>Beijing : 116.4074\,39.9042,Shanghai : 121.4691\,31.2243,Guangzhou : 113.2644\,23.1291,Shenzhen : 114.0596\,22.5429
</code></pre>
		</div>
	</div>
	<h2 id="panels">Panels</h2>
	<p>First and foremost, the most intuitive metric to show is temperature. I add a time series panel and configure it to graph the temperature. Start by building the query:</p>
	<ul>
		<li>Datasource: Select <code class="language-plaintext highlighter-rouge">${datasource}</code></li>
		<li>Query A:
			<ul>
				<li>Path: <code class="language-plaintext highlighter-rouge">/${location}/hourly.json</code></li>
				<li>Fields:
					<ul>
						<li>JSONPath: <code class="language-plaintext highlighter-rouge">$.result.hourly.temperature[*].value</code>, Type: <code class="language-plaintext highlighter-rouge">Number</code>, Alias: <code class="language-plaintext highlighter-rouge">${location:text}</code></li>
						<li>JSONPath: <code class="language-plaintext highlighter-rouge">$.result.hourly.temperature[*].datetime</code>, Type: <code class="language-plaintext highlighter-rouge">Time</code></li>
					</ul>
				</li>
			</ul>
		</li>
	</ul>
	<p>I stumbled on getting the <em>time</em> series to display correctly. It wasn’t anywhere obvious in the documentation for the JSON API plugin, but a series with Type = Time is required. Fortunately, CY’s API returns the time in ISO 8601 format in the <code class="language-plaintext highlighter-rouge">datetime</code> field, so I can feed it directly to Grafana.</p>
	<p>So far so good, except Grafana shows “No data”. I realized Grafana is trying to show past data, but apparently a weather forecast provides <em>future</em> data. I need to change the time range to “now” and “now + 48h”. Ideally, this time range is fixed and not affected by the time range selector, since after all it’s limited by the API.</p>
	<p>This is another place where I spent half an hour on Google. The answer is “Relative time” in “Query options”. Its format, however, is again unintuitive. While <a href="https://community.grafana.com/t/how-to-give-different-time-ranges-for-grafana-panels-i-am-using-azure-monitor-as-data-source/80300">community posts</a> shows <code class="language-plaintext highlighter-rouge">1d</code> for “last 1 day” and the <a href="https://grafana.com/docs/grafana/latest/panels-visualizations/query-transform-data/">official docs</a> gives several examples on using <code class="language-plaintext highlighter-rouge">now</code>, none of them told me how to indicate “next 48 hours”. The answer is just <code class="language-plaintext highlighter-rouge">+48h</code> or <code class="language-plaintext highlighter-rouge">+2d</code>. Notably, entering <code class="language-plaintext highlighter-rouge">now+48h</code> would result in an error.</p>
	<p>To make the graph look nicer, I set the unit to “°C”, limit decimals to 1, and set the Y-axis range to 0-40, and add a series of thresholds with colors to indicate the temperature range. Also worth mentioning is to make the graph change its color according to the temperature, so I set “Graph style → Gradient mode” to “Scheme” and “Standard options → Color scheme” to “From thresholds (by value)”.</p>
	<p>Now this panel looks stunning.</p>
	<p><img src="/image/grafana/caiyun-temperature-panel.png" alt="Temperature panel" /></p>
	<h3 id="more-panels">More panels</h3>
	<p>CY’s API offers a variety of weather data, so with little effort I can add more panels for humidity, precipitation and more, by duplicating the temperature panel and changing the query. I also need to change the unit and thresholds accordingly but that goes without saying.</p>
	<p>There’s also a small piece worth displaying: A <code class="language-plaintext highlighter-rouge">description</code> text. It’s easy to put it in a “Stat” panel and display as “String” (instead of “Number”). And better yet, CY provides two descriptions: One for the next two hours, and one for the next two days. Two panels for two pieces of text, yeah.</p>
	<p>One last thing I decided to leave out for now: The <code class="language-plaintext highlighter-rouge">skycon</code> field that describes the weather condition, like “CLEAR_DAY” or “RAIN”. It’d be comparably easy to add a panel for it, using “Value mapping” to change the text to something more human-readable, but I’m not at the high mood for it right now, so maybe I’ll pick it up later.</p>
	<h2 id="results">Results</h2>
	<p>Now I have a nice dashboard for viewing weather forecast for multiple cities:</p>
	<p><img src="/image/grafana/caiyun-forecast-example.png" alt="Dashboard" /></p>
	<p>If you’d like to try it yourself, I’ve published the dashboard on Grafana.com: <a href="https://grafana.com/grafana/dashboards/20259-weather-forecast/">Weather Forecast</a>. Just add the same datasource with your API key, and you can import my dashboard and start getting weather forecast for yourself.</p>
	]]></content><author><name>iBug</name></author><category term="software" /><summary type="html"><![CDATA[Grafana is a great piece of software for visualizing data and monitoring. It’s outstanding at what it does when paired with a time-series database like InfluxDB, except this time I’m trying to get it to work as a weather forecast dashboard, instead of any historical time-series data.]]></summary></entry><entry><title type="html">Understanding ZFS block sizes</title><link href="https://ibug.io/blog/2023/10/zfs-block-size/" rel="alternate" type="text/html" title="Understanding ZFS block sizes" /><published>2023-10-30T00:00:00+00:00</published><updated>2023-10-30T02:58:51+00:00</updated><id>https://ibug.io/blog/2023/10/zfs-block-size</id><content type="html" xml:base="https://ibug.io/blog/2023/10/zfs-block-size/"><![CDATA[<p>ZFS is about the most complex filesystem for single-node storage servers. Coming with its sophistication is its equally confusing “block size”, which is normally self-evident on common filesystems like ext4 (or more primitively, FAT). The enigma continues as ZFS bundles more optimizations, either for performance or in the name of “intuition” (which I would hardly agree). So recently I read a lot of materials on this and try to make sense of it.</p>
	<p>We’ll begin with a slide from a ZFS talk from Lustre<sup id="fnref:dilger" role="doc-noteref"><a href="#fn:dilger" class="footnote" rel="footnote">1</a></sup> (page 5):</p>
	<figure class=""><img src="/image/zfs/zfs-io-stack.png" alt="ZFS I/O Stack" />
		<figcaption>
			Figure 1. ZFS I/O Stack
		</figcaption>
	</figure>
	<!-- This article will focus on the topmost layer (ZPL and DMU) and the lowermost layer (vdev and disk sectors). -->
	<h2 id="logical-blocks">Logical blocks</h2>
	<p>The first thing to understand is that there are at least two levels of “block” concepts in ZFS. There’s “logical blocks” on an upper layer (DMU), and “physical blocks” on a lower layer (vdev). The latter is easier to understand and it’s almost synonymous to “disk sectors”. It’s precisely the <code class="language-plaintext highlighter-rouge">ashift</code> parameter in <code class="language-plaintext highlighter-rouge">zpool create</code> command and usually matches the physical sector size of your disks (4&nbsp;KiB for modern disks). Once set, <code class="language-plaintext highlighter-rouge">ashift</code> is immutable and can only be changed when recreating the entire vdev array (fortunately not the entire pool<sup id="fnref:zfs101" role="doc-noteref"><a href="#fn:zfs101" class="footnote" rel="footnote">2</a></sup>). The “logical block”, however, is slightly more complicated, and beyond the expressibility of a few words. In short, it’s the smallest <em>meaningful</em> unit of data that ZFS can operate on, including reading, writing, checksumming, compression and deduplication.</p>
	<h3 id="recordsize-and-volblocksize">“recordsize” and “volblocksize”</h3>
	<p>You’ve probably seen <code class="language-plaintext highlighter-rouge">recordsize</code> being talked about extensively in ZFS tuning guides<sup id="fnref:tuning" role="doc-noteref"><a href="#fn:tuning" class="footnote" rel="footnote">3</a></sup>, which is already a great source of confusion. The default <code class="language-plaintext highlighter-rouge">recordsize</code> is 128&nbsp;KiB, which controls the <em>maximum</em> size of a logical block. The <em>actual</em> block size depends on the file you’re writing:</p>
	<ul>
		<li>If the file is smaller than or equal to <code class="language-plaintext highlighter-rouge">recordsize</code>, it’s stored as a single logical block of its size, rounded up to the nearest multiple of 512&nbsp;bytes.</li>
		<li>If the file is larger than <code class="language-plaintext highlighter-rouge">recordsize</code>, it’s split into multiple logical blocks of <code class="language-plaintext highlighter-rouge">recordsize</code> each, with the last block being zero-padded to <code class="language-plaintext highlighter-rouge">recordsize</code>.</li>
	</ul>
	<p>As with other filesystems, if you change a small portion of a large file, only 128&nbsp;KiB (or whatever your <code class="language-plaintext highlighter-rouge">recordsize</code> is) is rewritten, along with new metadata and checksums. Large <code class="language-plaintext highlighter-rouge">recordsize</code> bloats the read/write amplification for random I/O workloads, while small <code class="language-plaintext highlighter-rouge">recordsize</code> increases the fragmentation and metadata overhead for large files. Note that ZFS always validates checksums, so every read operation is done on an entire block, even if only a few bytes are requested. So it is important to align your <code class="language-plaintext highlighter-rouge">recordsize</code> with your workload, like using 16&nbsp;KiB for (most) databases and 1&nbsp;MiB for media files. The default 128&nbsp;KiB is a good compromise for general-purpose workloads, and there certainly isn’t a one-size-fits-all solution. Also note that while <code class="language-plaintext highlighter-rouge">recordsize</code> can be changed on the fly, it only affects newly written data, and existing ones stay intact.</p>
	<p>For ZVOLs, as you’d imagine, the rule is much simpler: Every block of <code class="language-plaintext highlighter-rouge">volblocksize</code> is a logical block, and it’s aligned to its own size. Since ZFS 2.2, the default <code class="language-plaintext highlighter-rouge">volblocksize</code> is 16&nbsp;KiB, providing a good balance between performance and compatibility.</p>
	<h3 id="compression">Compression</h3>
	<p>Compression is applied on a per-block basis, and compressed data is not shared between blocks. This is best shown with an example:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>zfs get compression tank/test
<span class="go">NAME       PROPERTY     VALUE  SOURCE
tank/test  compression  zstd   inherited from tank
</span><span class="gp">$</span><span class="w"> </span><span class="nb">head</span> <span class="nt">-c</span> 131072 /dev/urandom <span class="o">&gt;</span> 128k
<span class="gp">$</span><span class="w"> </span><span class="nb">cat </span>128k 128k 128k 128k 128k 128k 128k 128k <span class="o">&gt;</span> 1m
<span class="gp">$</span><span class="w"> </span><span class="nb">du</span> <span class="nt">-sh</span> 128k 1m
<span class="go">129K    128k
1.1M    1m
</span></code></pre>
		</div>
	</div>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">$</span><span class="w"> </span><span class="nb">head</span> <span class="nt">-c</span> 16384 /dev/urandom <span class="o">&gt;</span> 16k
<span class="gp">$</span><span class="w"> </span><span class="nb">cat </span>16k 16k 16k 16k 16k 16k 16k 16k <span class="o">&gt;</span> 128k1
<span class="gp">$</span><span class="w"> </span><span class="nb">cat </span>128k1 128k1 128k1 128k1 128k1 128k1 128k1 128k1 <span class="o">&gt;</span> 1m1
<span class="gp">$</span><span class="w"> </span><span class="nb">du</span> <span class="nt">-sh</span> 16k 128k1 1m1
<span class="go">17K     16k
21K     128k1
169K    1m1
</span></code></pre>
		</div>
	</div>
	<p>As you can see from <code class="language-plaintext highlighter-rouge">du</code>’s output above, despite containing 8 identical copies of the same 128&nbsp;KiB random data, the 1&nbsp;MiB file gains precisely nothing from compression, as each 128&nbsp;KiB block is compressed individually. The other test of combining 8 copies of 16&nbsp;KiB random data into one 128&nbsp;KiB file shows positive results, as the 128&nbsp;KiB file is only 21&nbsp;KiB in size. Similarly, the 1&nbsp;MiB file that contains 64 exact copies of the same 16&nbsp;KiB chunk is exactly 8 times the size of that 128&nbsp;KiB file, because the chunk data is not shared across 128&nbsp;KiB boundaries.</p>
	<p>This brings up an interesting point: <strong>It’s beneficial to turn on compression even for filesystems with uncompressible data</strong><sup id="fnref:cks-1" role="doc-noteref"><a href="#fn:cks-1" class="footnote" rel="footnote">4</a></sup>. One direct impact is on the last block of a large file, where its zero-filled bytes up to <code class="language-plaintext highlighter-rouge">recordsize</code> compress very well. Using LZ4 or ZSTD, compression should have negligible impact on any reasonably modern CPU and reasonably sized disks.</p>
	<p>There are two more noteworthy points about compression, both from <a href="https://openzfs.github.io/openzfs-docs/man/master/7/zfsprops.7.html"><code class="language-plaintext highlighter-rouge">man zfsprops.7</code></a>:</p>
	<ol>
		<li>
			<blockquote>
				<p>When any setting except <strong>off</strong> is selected, compression will explicitly check for blocks consisting of only zeroes (the NUL byte). When a zero-filled block is detected, it is stored as a hole and not compressed using the indicated compression algorithm.</p>
			</blockquote>
			<p>Instead of compressing entire blocks of zeroes like the last block of a large file, ZFS will not store anything about these zero blocks. Technically, this is done by omitting the corresponding ranges from the file’s indirect blocks<sup id="fnref:cks-1:1" role="doc-noteref"><a href="#fn:cks-1" class="footnote" rel="footnote">4</a></sup>.</p>
			<p>Take this test for example: I created a file with 64&nbsp;KiB of urandom, then 256&nbsp;KiB of zeroes, then another 64&nbsp;KiB of urandom. The file is 384&nbsp;KiB in size, but only 128&nbsp;KiB is actually stored on disk:</p>
			<div class="language-console highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>zfs create pool0/srv/test
<span class="gp">#</span><span class="w"> </span><span class="nb">cat</span> &lt;<span class="o">(</span><span class="nb">head</span> <span class="nt">-c</span> 64K /dev/urandom<span class="o">)</span> &lt;<span class="o">(</span><span class="nb">head</span> <span class="nt">-c</span> 256K /dev/zero<span class="o">)</span> &lt;<span class="o">(</span><span class="nb">head</span> <span class="nt">-c</span> 64K /dev/urandom<span class="o">)</span> <span class="o">&gt;</span> /srv/test/test
<span class="gp">#</span><span class="w"> </span><span class="nb">du</span> <span class="nt">-sh</span> /srv/test/test
<span class="go">145K    /srv/test/test
</span></code></pre>
				</div>
    </div>
			<p>We can also examine the file’s indirect blocks with <code class="language-plaintext highlighter-rouge">zdb</code>:</p>
			<div class="language-console highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="gp">#</span><span class="w"> </span><span class="nb">ls</span> <span class="nt">-li</span> /srv/test/test
<span class="go">2 -rw-r--r-- 1 root root 393216 Oct 30 02:05 /srv/test/test
</span><span class="gp">#</span><span class="w"> </span>zdb <span class="nt">-ddddd</span> pool0/srv/test 2
<span class="go">[...]
Indirect blocks:
               0 L1  0:1791b7d3000:1000 20000L/1000P F=2 B=9769680/9769680 cksum=[...]
               0  L0 0:1791b7b1000:11000 20000L/11000P F=1 B=9769680/9769680 cksum=[...]
           40000  L0 0:1791b7c2000:11000 20000L/11000P F=1 B=9769680/9769680 cksum=[...]

                segment [0000000000000000, 0000000000020000) size  128K
                segment [0000000000040000, 0000000000060000) size  128K
</span></code></pre>
				</div>
    </div>
			<p>Here we can see only two L0 blocks allocated, each being 20000 (hex, dec = 131072) bytes logical and 11000 (hex, dec = 69632) bytes physical in size. The two L0 blocks match the two segments shown at the bottom, with the middle segment nowhere to be found.</p>
		</li>
		<li>
			<blockquote>
				<p>Any block being compressed must be no larger than 7/8 of its original size after compression, otherwise the compression will not be considered worthwhile and the block saved uncompressed. […] for example, 8&nbsp;KiB blocks on disks with 4&nbsp;KiB disk sectors must compress to 1/2 or less of their original size.</p>
			</blockquote>
			<p>This one should be self-explanatory.</p>
		</li>
	</ol>
	<h2 id="raidz">RAIDZ</h2>
	<p>Up until now we’ve only talked about logical blocks, which are all on the higher layers of the ZFS hierarchy. RAIDZ is where physical blocks (disk sectors) really come into play and adds another field of confusion.</p>
	<p>Unlike traditional RAID 5/6/7<sup class="no-select">(?)</sup> that combine disks into an array and presents a single volume for the filesystem, RAIDZ handles each <em>logical block</em> separately. I’ll cite this illustration from Delphix<sup id="fnref:delphix" role="doc-noteref"><a href="#fn:delphix" class="footnote" rel="footnote">5</a></sup> to explain:</p>
	<figure class=""><img src="/image/zfs/raidz-block-layout.png" alt="RAID-Z block layout" />
		<figcaption>
			Figure 2. RAID-Z block layout
		</figcaption>
	</figure>
	<p>This example shows a 5-wide RAID-Z1 setup.</p>
	<ul>
		<li>A single-sector block takes another sector for parity, like the dark red block on row 3.</li>
		<li>
			<p>Multi-sector blocks are striped across disks, with parity sectors inserted every 4 sectors, matching the data-to-parity ratio of the vdev array.</p>
			<ul>
				<li>You may have noticed that parity sectors for the same block are always stored on the same disk that resembles RAID-4 instead of RAID-5. Keep in mind that ZFS reads, writes and verifies entire blocks, so interleaving parity sectors across disks will not provide any benefit, while keeping “stripes” on the same disk simplifies the logic for validation and reconstruction.</li>
			</ul>
		</li>
		<li>In order to avoid unusable fragments, ZFS requires each allocated block to be padded to a multiple of (<em>p+1</em>) sectors, where <em>p</em> is the number of parity disks. For example, RAID-Z1 requires each block to be padded to a multiple of 2 sectors, and RAID-Z2 requires each block to be padded to a multiple of 3 sectors. This can be seen on rows 7 to 9, where the X sectors are reserved for parity padding.</li>
	</ul>
	<p>This design allows RAID to play well with ZFS’s log-structured design and avoids the need for read-modify-write cycles. Consequently, the RAID overhead is now dependent on your data and is no longer an intrinsic property of the RAID level and array width. The same Delphix article shares a nice spreadsheet<sup id="fnref:delphix-spreadsheet" role="doc-noteref"><a href="#fn:delphix-spreadsheet" class="footnote" rel="footnote">6</a></sup> that calculates RAID overhead for you:</p>
	<p><a href="https://docs.google.com/a/delphix.com/spreadsheets/d/1tf4qx1aMJp8Lo_R6gpT689wTjHv6CGVElrPqTA0w_ZY/"><img src="/image/zfs/raidz1-parity-overhead.png" alt="Size of parity overhead for RAID-Z1" /></a></p>
	<h3 id="raidz-accounting">Accounting</h3>
	<p>Accounting the storage space for a RAIDZ array is as problematic as it seems: There’s no way to calculate the available space in advance without knowledge on the block size pattern.</p>
	<p>ZFS works around this by showing an estimate, assuming all data were stored as 128&nbsp;KiB blocks<sup id="fnref:zfs-4599" role="doc-noteref"><a href="#fn:zfs-4599" class="footnote" rel="footnote">7</a></sup>. On my test setup with five 16&nbsp;GiB disks in RAID-Z1 and <code class="language-plaintext highlighter-rouge">ashift=12</code>, the available space shows as 61.5G, while <code class="language-plaintext highlighter-rouge">zpool</code> shows the raw size as 79.5G:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>zpool create <span class="nt">-o</span> <span class="nv">ashift</span><span class="o">=</span>12 <span class="nb">test </span>raidz1 nvme3n1p<span class="o">{</span>1,2,3,4,5<span class="o">}</span>
<span class="gp">#</span><span class="w"> </span>zfs list <span class="nb">test</span>
<span class="go">NAME   USED  AVAIL     REFER  MOUNTPOINT
test   614K  61.5G      153K  /test
</span><span class="gp">#</span><span class="w"> </span>zpool list <span class="nb">test</span>
<span class="go">NAME   SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
test  79.5G   768K  79.5G        -         -     0%     0%  1.00x    ONLINE  -
</span></code></pre>
		</div>
	</div>
	<p>When I increase <code class="language-plaintext highlighter-rouge">ashift</code> to 15 (32&nbsp;KiB sectors), the available space drops quite a bit, even if <code class="language-plaintext highlighter-rouge">zpool</code> shows the same raw size:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>zpool create <span class="nt">-o</span> <span class="nv">ashift</span><span class="o">=</span>15 <span class="nb">test </span>raidz1 nvme3n1p<span class="o">{</span>1,2,3,4,5<span class="o">}</span>
<span class="gp">#</span><span class="w"> </span>zfs list <span class="nb">test</span>
<span class="go">NAME   USED  AVAIL     REFER  MOUNTPOINT
test  4.00M  51.3G     1023K  /test
</span><span class="gp">#</span><span class="w"> </span>zpool list <span class="nb">test</span>
<span class="go">NAME   SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
test  79.5G  7.31M  79.5G        -         -     0%     0%  1.00x    ONLINE  -
</span></code></pre>
		</div>
	</div>
	<p>In both cases, calculating the “raw” disk space from the available space gives roughly congruent results:</p>
	<ul>
		<li>61.5&nbsp;GiB × (1 + 25%) = 76.9&nbsp;GiB</li>
		<li>51.3&nbsp;GiB × (1 + 50%) = 76.9&nbsp;GiB</li>
	</ul>
	<p>The default <code class="language-plaintext highlighter-rouge">refreservation</code> for non-sparse ZVOLs exhibits a similar behavior:</p>
	<div class="language-console highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>zfs create <span class="nt">-V</span> 4G <span class="nt">-o</span> <span class="nv">volblocksize</span><span class="o">=</span>8K <span class="nb">test</span>/v8k
<span class="gp">#</span><span class="w"> </span>zfs create <span class="nt">-V</span> 4G <span class="nt">-o</span> <span class="nv">volblocksize</span><span class="o">=</span>16K <span class="nb">test</span>/v16k
<span class="gp">#</span><span class="w"> </span>zfs get refreservation <span class="nb">test</span>/v8k <span class="nb">test</span>/v16k
<span class="go">NAME       PROPERTY        VALUE      SOURCE
test/v16k  refreservation  4.86G      local
test/v8k   refreservation  6.53G      local
</span></code></pre>
		</div>
	</div>
	<p>Interestingly, neither of the <code class="language-plaintext highlighter-rouge">refreservation</code> sizes matches the RAID overhead as calculated in the Delphix spreadsheet<sup id="fnref:delphix-spreadsheet:1" role="doc-noteref"><a href="#fn:delphix-spreadsheet" class="footnote" rel="footnote">6</a></sup>, as you would expect some 6.0&nbsp;GiB for the 16k-volblocksized ZVOL and some 8.0&nbsp;GiB for the 8k-volblocksized one. <strong>Let’s just don’t forget that the whole accounting system assumed 128&nbsp;KiB blocks and scaled by that<sup id="fnref:acct-128k" role="doc-noteref"><a href="#fn:acct-128k" class="footnote" rel="footnote">8</a></sup>.</strong> So the actual meaning of 4.86G and 6.53G would be “the <em>equivalent</em> space if volblocksize had been 128&nbsp;KiB”. If we multiply both values by 1.25 (overhead for 128&nbsp;KiB blocks and 5-wide RAIDZ), we get 6.08&nbsp;GiB and 8.16&nbsp;GiB of raw disk spaces respectively, both of which match more closely the expected values. The final minor difference is due to the different amount of metadata required for different number of blocks.</p>
	<h2 id="thoughts">Thoughts</h2>
	<p>I never imagined I would delve this deep into ZFS when I first stumbled upon the question. There are lots of good write-ups on individual components of ZFS all around the web, and <a href="https://utcc.utoronto.ca/~cks/space/blog/">Chris Siebenmann’s blog</a> in particular. But few combine all the pieces together and paint the whole picture, so I had to spend some time synthesizing them by myself. As you’ve seen in the Luster slide, ZFS is so complex a beast that it’s hard to digest in its entirety. So for now I have no idea how much effort I would put into learning it, nor any future blogs I would write. But anyways, that’s one large mystery solved, for myself and my readers (you), and time to call it a day.</p>
	<h2 id="references">References</h2>
	<div class="footnotes" role="doc-endnotes">
		<ol>
			<li id="fn:dilger" role="doc-endnote">
				<p>Andreas Dilger (2010) <a href="https://wiki.lustre.org/images/4/49/Beijing-2010.2-ZFS_overview_3.1_Dilger.pdf">ZFS Features &amp; Concepts TOI</a> <a href="#fnref:dilger" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
			</li>
			<li id="fn:zfs101" role="doc-endnote">
				<p>Jim Salter (2020) <a href="https://arstechnica.com/information-technology/2020/05/zfs-101-understanding-zfs-storage-and-performance/">ZFS 101 – Understanding ZFS storage and performance</a> <a href="#fnref:zfs101" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
			</li>
			<li id="fn:tuning" role="doc-endnote">
				<p>OpenZFS <a href="https://openzfs.github.io/openzfs-docs/Performance%20and%20Tuning/Workload%20Tuning.html">Workload Tuning</a> <a href="#fnref:tuning" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
			</li>
			<li id="fn:cks-1" role="doc-endnote">
				<p>Chris Siebenmann (2017) <a href="https://utcc.utoronto.ca/~cks/space/blog/solaris/ZFSFilePartialAndHoleStorage">ZFS’s recordsize, holes in files, and partial blocks</a> <a href="#fnref:cks-1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:cks-1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
			</li>
			<li id="fn:delphix" role="doc-endnote">
				<p>Matthew Ahrens (2014) <a href="https://www.delphix.com/blog/zfs-raidz-stripe-width-or-how-i-learned-stop-worrying-and-love-raidz">How I Learned to Stop Worrying and Love RAIDZ</a> <a href="#fnref:delphix" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
			</li>
			<li id="fn:delphix-spreadsheet" role="doc-endnote">
				<p><a href="https://docs.google.com/a/delphix.com/spreadsheets/d/1tf4qx1aMJp8Lo_R6gpT689wTjHv6CGVElrPqTA0w_ZY/">RAID-Z parity cost</a> (Google Sheets) <a href="#fnref:delphix-spreadsheet" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:delphix-spreadsheet:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
			</li>
			<li id="fn:zfs-4599" role="doc-endnote">
				<p>openzfs/zfs#4599 (2016) <a href="https://github.com/openzfs/zfs/issues/4599">disk usage wrong when using larger recordsize, raidz and ashift=12</a> <a href="#fnref:zfs-4599" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
			</li>
			<li id="fn:acct-128k" role="doc-endnote">
				<p>Mike Gerdts (2019) <a href="https://github.com/illumos/illumos-gate/blob/b73ccab03ec36581b1ae5945ef1fee1d06c79ccf/usr/src/lib/libzfs/common/libzfs_dataset.c#L5118">(Code comment in <code class="language-plaintext highlighter-rouge">libzfs_dataset.c</code>)</a> <a href="#fnref:acct-128k" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
			</li>
		</ol>
	</div>
	]]></content><author><name>iBug</name></author><category term="linux" /><category term="zfs" /><summary type="html"><![CDATA[ZFS is about the most complex filesystem for single-node storage servers. Coming with its sophistication is its equally confusing “block size”, which is normally self-evident on common filesystems like ext4 (or more primitively, FAT). The enigma continues as ZFS bundles more optimizations, either for performance or in the name of “intuition” (which I would hardly agree). So recently I read a lot of materials on this and try to make sense of it.]]></summary></entry><entry><title type="html">Debugging Proxmox VE Firewall Dropping TCP Reset Packets</title><link href="https://ibug.io/blog/2023/10/pve-firewall-drops-tcp-reset/" rel="alternate" type="text/html" title="Debugging Proxmox VE Firewall Dropping TCP Reset Packets" /><published>2023-10-06T00:00:00+00:00</published><updated>2023-10-06T13:37:51+00:00</updated><id>https://ibug.io/blog/2023/10/pve-firewall-drops-tcp-reset</id><content type="html" xml:base="https://ibug.io/blog/2023/10/pve-firewall-drops-tcp-reset/"><![CDATA[<p>A few days back when I was setting up a new VM to host some extra websites, I noticed an unexpected Nginx error page. As I don’t administer the new websites, I just added reverse proxy rules on the gateway Nginx server, and deferred the actual configuration to whoever is in charge of them.</p>
	<p>When I reviewed my edited Nginx configuration and tried visiting the new website, I received a 504 Gateway Timeout error after <code class="language-plaintext highlighter-rouge">curl</code> hung for a minute. Knowing that the web server had yet to be set up, I was expecting a 502 Bad Gateway error. I quickly recalled the conditions for Nginx to return these specific errors: 502 if the upstream server is immediately known down, and 504 if the upstream server is up but not responding.</p>
	<p>Since the actual web application hadn’t been set up yet, the new VM should have nothing listening on the configured ports. Consequently, the kernel should immediately respond with a TCP Reset for any incoming connections. To verify this, I ran <code class="language-plaintext highlighter-rouge">tcpdump</code> on both sides to check if the TCP reset packets actually came out. To my surprise, the packets were indeed sent out from the new VM, but the gateway server received nothing. So there was certainly something wrong with the firewall. I took a glance at the output of <code class="language-plaintext highlighter-rouge">pve-firewall compile</code>. They were very structured and adequately easy to understand, but I couldn’t immediately identify anything wrong. Things were apparently more complicated than I had previously anticipated.</p>
	<h2 id="searching">Searching for information</h2>
	<p>As usual, the first thing to try is Googling. Searching for <code class="language-plaintext highlighter-rouge">pve firewall tcp reset</code> brought <a href="https://forum.proxmox.com/threads/tcp-rst-packets-dropped-by-pve-firewall.56300/">this post on Proxmox Forum</a> as the first result. Their symptoms were precisely the same as mine:</p>
	<blockquote>
		<ul>
			<li>Assume we have a service running on TCP port 12354</li>
			<li>Clients can communicate with it while running</li>
			<li>While service is down, clients recieved “Connection timed out” (no answer) even if OS send TCP RST packets:</li>
		</ul>
		<p>[…]</p>
		<p>However, these RST packets are dropped somewhere in PVE firewall.<br />
			On the VM options :</p>
		<ul>
			<li>Firewall &gt; Options &gt; Firewall = No, Has no effect</li>
			<li>Firewall &gt; Options &gt; * Policy = ACCEPT, Has no effect (even with NO rule in active for this VM)</li>
			<li>Hardware &gt; Network Device &gt; <code class="language-plaintext highlighter-rouge">firewall=0</code>, allows packets RST to pass!</li>
		</ul>
	</blockquote>
	<p>I gave the last suggestion a try, and it worked! I could now see connections immediately reset on the gateway server, and Nginx started producing 502 errors. But I was still confused why this happened in the first place. The first thread contained nothing else useful, so I continued scanning through other search results and noticed <a href="https://forum.proxmox.com/threads/turning-on-the-pve-firewall-stops-vm-lxc-connectivity.55634/#post-261316">another post</a> about another seemingly unrelated problem, with a plausible solution:</p>
	<blockquote>
		<p>[…], and the fix was just to add the <code class="language-plaintext highlighter-rouge">nf_conntrack_allow_invalid: 1</code> in the <code class="language-plaintext highlighter-rouge">host.fw</code> for each node - I didn’t have to do anything other than that.</p>
	</blockquote>
	<p>That seemed understandable to me, so I gave it a try as well, and to my pleasure, it also worked.</p>
	<p>Regrettably, useful information ceased to exist online beyond this, and it was far from painting the whole picture. So anything further would have to be uncovered on my own.</p>
	<h2 id="reviewing">Reviewing information</h2>
	<p>I reviewed the two helpful workarounds and made myself abundantly clear about their effects:</p>
	<ul>
		<li>
			<p>Disabling the firewall on the virtual network device stops PVE from bridging the interface an extra time, as shown in the following diagram:</p>
			<p><img src="/image/pve-firewall/pve-fwbr.png" alt="PVE Firewall Diagram" /></p>
		</li>
		<li>
			<p>Adding <code class="language-plaintext highlighter-rouge">nf_conntrack_allow_invalid: 1</code> removes one single iptables rule:</p>
			<div class="language-shell highlighter-rouge">
				<div class="highlight">
					<pre class="highlight"><code><span class="nt">-A</span> PVEFW-FORWARD <span class="nt">-m</span> conntrack <span class="nt">--ctstate</span> INVALID <span class="nt">-j</span> DROP
</code></pre>
				</div>
    </div>
		</li>
	</ul>
	<p>I couldn’t figure out how the first difference was relevant, but the second one provided an important clue: The firewall was dropping TCP Reset packets because conntrack considered them invalid.</p>
	<p>Conntrack (<strong>conn</strong>ection <strong>track</strong>ing) is a Linux kernel subsystem that tracks network connections and aids in stateful packet inspection and network address translation. The first packet of a connection is considered “NEW”, and subsequent packets from the same connection are considered “ESTABLISHED”, including the TCP Reset packet when it’s first seen, which causes conntrack to delete the connection entry.</p>
	<p>There was still yet anything obvious, so time to start debugging.</p>
	<h2 id="tcpdump">Inspecting packet captures</h2>
	<p>I ran <code class="language-plaintext highlighter-rouge">tcpdump -ni any host 172.31.0.2 and host 172.31.1.11 and tcp</code> on the PVE host to capture packets between the two VMs. This is what I got (output trimmed):</p>
	<div class="language-text highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>tcpdump: data link type LINUX_SLL2
tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144&nbsp;bytes
16:33:11.911184 veth101i1 P   IP 172.31.0.2.50198 &gt; 172.31.1.11.80: Flags [S], seq 3404503761, win 64240
16:33:11.911202 fwln101i1 Out IP 172.31.0.2.50198 &gt; 172.31.1.11.80: Flags [S], seq 3404503761, win 64240
16:33:11.911203 fwpr101p1 P   IP 172.31.0.2.50198 &gt; 172.31.1.11.80: Flags [S], seq 3404503761, win 64240
16:33:11.911206 fwpr811p0 Out IP 172.31.0.2.50198 &gt; 172.31.1.11.80: Flags [S], seq 3404503761, win 64240
16:33:11.911207 fwln811i0 P   IP 172.31.0.2.50198 &gt; 172.31.1.11.80: Flags [S], seq 3404503761, win 64240
16:33:11.911213 tap811i0  Out IP 172.31.0.2.50198 &gt; 172.31.1.11.80: Flags [S], seq 3404503761, win 64240
16:33:11.911262 tap811i0  P   IP 172.31.1.11.80 &gt; 172.31.0.2.50198: Flags [R.], seq 0, ack 3404503762, win 0, length 0
16:33:11.911267 fwln811i0 Out IP 172.31.1.11.80 &gt; 172.31.0.2.50198: Flags [R.], seq 0, ack 1, win 0, length 0
16:33:11.911269 fwpr811p0 P   IP 172.31.1.11.80 &gt; 172.31.0.2.50198: Flags [R.], seq 0, ack 1, win 0, length 0
^C
9 packets captured
178 packets received by filter
0 packets dropped by kernel
</code></pre>
		</div>
	</div>
	<p><img src="/image/pve-firewall/fw-diagram-1.png" alt="Diagram" /></p>
	<p>The first thing to notice is the ACK number. After coming from <code class="language-plaintext highlighter-rouge">tap811i0</code>, it suddenly became 1 with no apparent reason. I struggled on this for a good while and temporarily put it aside.</p>
	<p>Adding <code class="language-plaintext highlighter-rouge">nf_conntrack_allow_invalid: 1</code> to the firewall options and capturing packets again, I got the following:</p>
	<div class="language-text highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144&nbsp;bytes
16:46:15.243002 veth101i1 P   IP 172.31.0.2.58784 &gt; 172.31.1.11.80: Flags [S], seq 301948896, win 64240
16:46:15.243015 fwln101i1 Out IP 172.31.0.2.58784 &gt; 172.31.1.11.80: Flags [S], seq 301948896, win 64240
16:46:15.243016 fwpr101p1 P   IP 172.31.0.2.58784 &gt; 172.31.1.11.80: Flags [S], seq 301948896, win 64240
16:46:15.243020 fwpr811p0 Out IP 172.31.0.2.58784 &gt; 172.31.1.11.80: Flags [S], seq 301948896, win 64240
16:46:15.243021 fwln811i0 P   IP 172.31.0.2.58784 &gt; 172.31.1.11.80: Flags [S], seq 301948896, win 64240
16:46:15.243027 tap811i0  Out IP 172.31.0.2.58784 &gt; 172.31.1.11.80: Flags [S], seq 301948896, win 64240
16:46:15.243076 tap811i0  P   IP 172.31.1.11.80 &gt; 172.31.0.2.58784: Flags [R.], seq 0, ack 301948897, win 0, length 0
16:46:15.243081 fwln811i0 Out IP 172.31.1.11.80 &gt; 172.31.0.2.58784: Flags [R.], seq 0, ack 1, win 0, length 0
16:46:15.243083 fwpr811p0 P   IP 172.31.1.11.80 &gt; 172.31.0.2.58784: Flags [R.], seq 0, ack 1, win 0, length 0
16:46:15.243086 fwpr101p1 Out IP 172.31.1.11.80 &gt; 172.31.0.2.58784: Flags [R.], seq 0, ack 1, win 0, length 0
16:46:15.243087 fwln101i1 P   IP 172.31.1.11.80 &gt; 172.31.0.2.58784: Flags [R.], seq 0, ack 1, win 0, length 0
16:46:15.243090 veth101i1 Out IP 172.31.1.11.80 &gt; 172.31.0.2.58784: Flags [R.], seq 0, ack 1, win 0, length 0
^C
</code></pre>
		</div>
	</div>
	<p><img src="/image/pve-firewall/fw-diagram-2.png" alt="Diagram" /></p>
	<p>This time while the ACK number was still wrong, the RST packet somehow got through. Ignoring the ACK numbers for now, the output suggested that the RST packet was dropped between <code class="language-plaintext highlighter-rouge">fwpr811p0 P</code> and <code class="language-plaintext highlighter-rouge">fwln811i0 Out</code>. That was the main bridge <code class="language-plaintext highlighter-rouge">vmbr0</code>. All right then, that was where the <code class="language-plaintext highlighter-rouge">PVEFW-FORWARD</code> chain kicked in, so at this point the RST packet was <code class="language-plaintext highlighter-rouge">--ctstate INVALID</code>. Everything was logical so far.</p>
	<p>So how about disabling firewall for the interface on VM 811?</p>
	<div class="language-text highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144&nbsp;bytes
17:19:01.812030 veth101i1 P   IP 172.31.0.2.39734 &gt; 172.31.1.11.80: Flags [S], seq 1128018611, win 64240
17:19:01.812045 fwln101i1 Out IP 172.31.0.2.39734 &gt; 172.31.1.11.80: Flags [S], seq 1128018611, win 64240
17:19:01.812046 fwpr101p1 P   IP 172.31.0.2.39734 &gt; 172.31.1.11.80: Flags [S], seq 1128018611, win 64240
17:19:01.812051 tap811i0  Out IP 172.31.0.2.39734 &gt; 172.31.1.11.80: Flags [S], seq 1128018611, win 64240
17:19:01.812178 tap811i0  P   IP 172.31.1.11.80 &gt; 172.31.0.2.39734: Flags [R.], seq 0, ack 1128018612, win 0, length 0
17:19:01.812183 fwpr101p1 Out IP 172.31.1.11.80 &gt; 172.31.0.2.39734: Flags [R.], seq 0, ack 1, win 0, length 0
17:19:01.812185 fwln101i1 P   IP 172.31.1.11.80 &gt; 172.31.0.2.39734: Flags [R.], seq 0, ack 1, win 0, length 0
17:19:01.812190 veth101i1 Out IP 172.31.1.11.80 &gt; 172.31.0.2.39734: Flags [R.], seq 0, ack 1, win 0, length 0
^C
</code></pre>
		</div>
	</div>
	<p><img src="/image/pve-firewall/fw-diagram-3.png" alt="Diagram" /></p>
	<p>This time <code class="language-plaintext highlighter-rouge">fwbr811i0</code> was missing, and the RST packet didn’t get dropped at <code class="language-plaintext highlighter-rouge">vmbr0</code>. I was left totally confused.</p>
	<p>I decided to sort out the ACK number issue, but ended up asking my friends for help. It turned out this was well documented in <code class="language-plaintext highlighter-rouge">tcpdump(8)</code>:</p>
	<blockquote>
		<p><code class="language-plaintext highlighter-rouge">-S</code><br />
			<code class="language-plaintext highlighter-rouge">--absolute-tcp-sequence-numbers</code><br />
			Print absolute, rather than relative, TCP sequence numbers.</p>
	</blockquote>
	<p>This certainly came out unexpected, but at least I was assured there was nothing wrong with the ACK numbers.</p>
	<p>Up to now, that’s one more step forward, and a small conclusion:</p>
	<ul>
		<li>At the point the RST packet reached <code class="language-plaintext highlighter-rouge">vmbr0</code>, it was already <code class="language-plaintext highlighter-rouge">--ctstate INVALID</code>.</li>
	</ul>
	<p>But how? As far as I knew, when the RST packet came out, it should still be considered part of the connection, and thus should be <code class="language-plaintext highlighter-rouge">--ctstate ESTABLISHED</code>. I was still missing something.</p>
	<p>Time to investigate conntrack.</p>
	<h2 id="conntrack">Inspecting conntrack</h2>
	<p><code class="language-plaintext highlighter-rouge">conntrack</code> is the tool to inspect and modify conntrack entries. I ran <code class="language-plaintext highlighter-rouge">conntrack -L</code> to list all entries, only to realize it’s inefficient. So instead, I ran <code class="language-plaintext highlighter-rouge">conntrack -E</code> to watch for “events” in real time, so that I could compare the output with <code class="language-plaintext highlighter-rouge">tcpdump</code>. Except that the entire connection concluded so quickly that I couldn’t identify anything.</p>
	<p>I had to add artificial delays to the packets to clearly separate each hop that the RST packet goes through:</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>tc qdisc add dev tap811i0 root netem delay 200ms
tc qdisc add dev fwln811i0 root netem delay 200ms
</code></pre>
		</div>
	</div>
	<p>I also tuned the output on both sides to show the timestamp in a consistent format. For conntrack, <code class="language-plaintext highlighter-rouge">-o timestamp</code> produced Unix timestamps (which is the only supported format), so for <code class="language-plaintext highlighter-rouge">tcpdump</code> I also resorted to <code class="language-plaintext highlighter-rouge">-tt</code> to show Unix timestamps as well.</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>conntrack <span class="nt">-E</span> <span class="nt">-o</span> timestamp <span class="nt">-s</span> 172.31.0.2 <span class="nt">-d</span> 172.31.1.11
tcpdump <span class="nt">-ttSni</span> any host 172.31.0.2 and host 172.31.1.11 and tcp
</code></pre>
		</div>
	</div>
	<p>Now I could watch the outputs on two separate tmux panes. The problem immediately emerged (blank lines added for readability):</p>
	<div class="language-text highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144&nbsp;bytes
1696412047.886575 veth101i1 P   IP 172.31.0.2.47066 &gt; 172.31.1.11.80: Flags [S]
1696412047.886592 fwln101i1 Out IP 172.31.0.2.47066 &gt; 172.31.1.11.80: Flags [S]
1696412047.886594 fwpr101p1 P   IP 172.31.0.2.47066 &gt; 172.31.1.11.80: Flags [S]
1696412047.886599 fwpr811p0 Out IP 172.31.0.2.47066 &gt; 172.31.1.11.80: Flags [S]
1696412047.886600 fwln811i0 P   IP 172.31.0.2.47066 &gt; 172.31.1.11.80: Flags [S]

1696412048.086620 tap811i0  Out IP 172.31.0.2.47066 &gt; 172.31.1.11.80: Flags [S]
1696412048.086841 tap811i0  P   IP 172.31.1.11.80 &gt; 172.31.0.2.47066: Flags [R.]

1696412048.286919 fwln811i0 Out IP 172.31.1.11.80 &gt; 172.31.0.2.47066: Flags [R.]
1696412048.286930 fwpr811p0 P   IP 172.31.1.11.80 &gt; 172.31.0.2.47066: Flags [R.]
^C
</code></pre>
		</div>
	</div>
	<div class="language-text highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>[1696412047.886657]         [NEW] tcp      6 120 SYN_SENT src=172.31.0.2 dst=172.31.1.11 sport=47066 dport=80 [UNREPLIED] src=172.31.1.11 dst=172.31.0.2 sport=80 dport=47066
[1696412048.086899]     [DESTROY] tcp      6 119 CLOSE src=172.31.0.2 dst=172.31.1.11 sport=47066 dport=80 [UNREPLIED] src=172.31.1.11 dst=172.31.0.2 sport=80 dport=47066
</code></pre>
		</div>
	</div>
	<p>The artificial delays and the timestamps were absolutely useful: It was clear that the corresponding conntrack connection was destroyed as soon as the RST packet passed through <code class="language-plaintext highlighter-rouge">fwbr811i0</code>, before it came out via <code class="language-plaintext highlighter-rouge">fwln811i0</code>. When it reached <code class="language-plaintext highlighter-rouge">vmbr0</code>, the connection was already gone, and the RST packet was considered invalid.</p>
	<p><img src="/image/pve-firewall/fw-diagram-4.png" alt="Diagram" /></p>
	<p>It also became explainable how <code class="language-plaintext highlighter-rouge">firewall=0</code> on the virtual network device remedied the issue: It removed an extra bridge <code class="language-plaintext highlighter-rouge">fwbr811i0</code>, so the connection stayed alive when the RST packet reached <code class="language-plaintext highlighter-rouge">vmbr0</code>, at which point a previous rule for <code class="language-plaintext highlighter-rouge">--ctstate ESTABLISHED</code> gave an <code class="language-plaintext highlighter-rouge">ACCEPT</code> verdict. While it was still <code class="language-plaintext highlighter-rouge">INVALID</code> when passing through <code class="language-plaintext highlighter-rouge">fwbr101i1</code>, there was no rule concerning <code class="language-plaintext highlighter-rouge">--ctstate</code> at play, so it slipped through this stage with no problem.</p>
	<p>After double-checking the intention of the extra <code class="language-plaintext highlighter-rouge">fwbr*</code> bridge, I drew the conclusion that <strong>this must be a bug with PVE Firewall</strong>. I reported it on the Proxmox VE bug tracker as <a href="https://bugzilla.proxmox.com/show_bug.cgi?id=4983">#4983</a>, and soon received a reply:</p>
	<blockquote>
		<p>Thank you for the detailed write-up!</p>
		<p>This is a known limitation for our kind of firewall setup, since the conntrack is shared between all interfaces on the host.</p>
		<p>[…]</p>
		<p>If you know of any other way to avoid this happening, other than using conntrack zones, I’d be happy to take a look.</p>
	</blockquote>
	<p>So they admitted that this was a limitation but without a satisfactory solution. Guess I’m still on my own, though.</p>
	<h2 id="solution">Finding the solution</h2>
	<p>The actual problem is, when passing through <code class="language-plaintext highlighter-rouge">fwbr811i0</code>, the RST packet isn’t supposed to be processed by conntrack by then. There is no <code class="language-plaintext highlighter-rouge">sysctl</code> option to disable conntrack on a specific interface (or even just all bridges altogether), but at the right time the rarely-used <code class="language-plaintext highlighter-rouge">raw</code> table came to my mind. It didn’t take long to work this out:</p>
	<div class="language-shell highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code>iptables <span class="nt">-t</span> raw <span class="nt">-A</span> PREROUTING <span class="nt">-i</span> fwbr+ <span class="nt">-j</span> CT <span class="nt">--notrack</span>
</code></pre>
		</div>
	</div>
	<p>After verifying this is the intended solution, I added it as a reply to the bug report. At the time of writing this blog post, the bug report is still open, but I’m sure it’s to be resolved soon.</p>
	<h2 id="conclusion">Conclusion</h2>
	<p>Debugging Linux networking has always been a pain for its lack of proper tools and its complexity. Most of the times even reading and understanding packet captures requires immense knowledge of the protocols and all the involved components, as well as scrutinizing every single detail available. Sometimes it’s even necessary to think outside the box but fortunately not today.</p>
	<p>Also worth mentioning is that it’s easy to suspect the fault of another piece of software, but detailed investigation is always necessary to actually lay the blame.</p>
	<p>Just as a late reminder, useful bug reports always require detailed information and solid evidence. Glad I was able to have them at hand this time.</p>
	]]></content><author><name>iBug</name></author><category term="linux" /><category term="networking" /><category term="proxmox-ve" /><summary type="html"><![CDATA[A few days back when I was setting up a new VM to host some extra websites, I noticed an unexpected Nginx error page. As I don’t administer the new websites, I just added reverse proxy rules on the gateway Nginx server, and deferred the actual configuration to whoever is in charge of them.]]></summary></entry><entry><title type="html">Running a dual-protocol OpenVPN/WireGuard VPN server on one port</title><link href="https://ibug.io/blog/2023/09/dual-protocol-vpn-port/" rel="alternate" type="text/html" title="Running a dual-protocol OpenVPN/WireGuard VPN server on one port" /><published>2023-09-26T00:00:00+00:00</published><updated>2023-09-26T14:26:49+00:00</updated><id>https://ibug.io/blog/2023/09/dual-protocol-vpn-port</id><content type="html" xml:base="https://ibug.io/blog/2023/09/dual-protocol-vpn-port/"><![CDATA[<p>Public Wi-Fi and some campus network typically block traffic from unauthenticated clients, but more often allow traffic targeting UDP port 53 to pass through, which is normally used for DNS queries. This feature can be exploited to bypass authentication by connecting to a VPN server that’s also running on UDP 53.</p>
	<p>In previous times, OpenVPN was the general preference for personal VPN services. Since the emergence of WireGuard, however, popularity has shifted significantly for its simplicity and performance. A challenge presents itself as there’s only one UDP port numbered 53, making it seemingly impossible to run both OpenVPN and WireGuard on the same port.</p>
	<p>There solution hinges itself on a little bit of insights.</p>
	<h2 id="inspiration">Inspiration</h2>
	<p>In a similar situation, many local proxy software like Shadowsocks and V2ray support a feature called “mixed mode”, which accepts both HTTP and SOCKS5 connections on the same TCP port. This also seems impossible at first glance, but with a bit of knowledge in both protocols, it’s actually easy to pull it off.</p>
	<ul>
		<li>An HTTP proxy request, just like other HTTP requests, begins with an HTTP verb. In proxy requests, it’s either <code class="language-plaintext highlighter-rouge">GET</code> or <code class="language-plaintext highlighter-rouge">CONNECT</code>,</li>
		<li>A SOCKS proxy request begins with a 1-byte header containing its version, which is <code class="language-plaintext highlighter-rouge">0x04</code> for SOCKS4 or <code class="language-plaintext highlighter-rouge">0x05</code> for SOCKS5.</li>
	</ul>
	<p>Now there’s a clear line between the two protocols, and we can identify them by looking at the first byte of the request. This is how most proxy implementations work, like <a href="https://github.com/3proxy/3proxy/commit/fb56b7d307a7bce1f2109c73864bad7c71716f3b#diff-e268b23274bc9df1b2c0957dfa85d684519282ed611f6135e795205e53fb6e3b">3proxy</a> and <a href="https://github.com/nadoo/glider/blob/4f12a4f3082940d8a4c56ba4f06f02a72d90d5d6/proxy/mixed/mixed.go#L84">glider</a>.</p>
	<p>So the question is, is there a similar trait between OpenVPN and WireGuard? The answer is, as you would expect, yes.</p>
	<h2 id="protocols">Protocols</h2>
	<p>WireGuard runs over UDP and defines 4 packet types: 3 for handshake and 1 for data. All 4 packet types share the same 4-byte <a href="https://github.com/WireGuard/wireguard-linux/blob/fa41884c1c6deb6774135390e5813a97184903e0/drivers/net/wireguard/messages.h#L65">header</a>:</p>
	<div class="language-rust highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="k">struct</span> <span class="n">message_header</span> <span class="p">{</span>
    <span class="nb">u8</span> <span class="k">type</span><span class="p">;</span>
    <span class="nb">u8</span> <span class="n">reserved_zero</span><span class="p">[</span><span class="mi">3</span><span class="p">];</span>
<span class="p">}</span>
</code></pre>
		</div>
	</div>
	<p>Similarly, all OpenVPN packet types share the same 1-byte <a href="https://build.openvpn.net/doxygen/network_protocol.html#network_protocol_external_types">header</a>:</p>
	<div class="language-c highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="k">struct</span> <span class="n">header_byte</span> <span class="p">{</span>
    <span class="kt">uint8_t</span> <span class="n">opcpde</span> <span class="o">:</span> <span class="mi">5</span><span class="p">;</span>
    <span class="kt">uint8_t</span> <span class="n">key_id</span> <span class="o">:</span> <span class="mi">3</span><span class="p">;</span>
<span class="p">}</span>
</code></pre>
		</div>
	</div>
	<p>It’s worth noting that 0 is not a defined opcode, so the smallest valid value for this byte is 8, as <code class="language-plaintext highlighter-rouge">key_id</code> can be anything from 0 to 7.</p>
	<h2 id="implementation">Implementation</h2>
	<p>Now that we have the packet format for both protocols understood, we can implement a classifier that filters traffic in one protocol from the other.</p>
	<p>Considering that the WireGuard packet format is much simpler than that of OpenVPN, I choose to identify WireGuard. With kernel firewall <code class="language-plaintext highlighter-rouge">iptables</code>, options are abundant, though I find <code class="language-plaintext highlighter-rouge">u32</code> the easiest:</p>
	<div class="language-sh highlighter-rouge">
		<div class="highlight">
			<pre class="highlight"><code><span class="k">*</span>nat
:iBugVPN - <span class="o">[</span>0:0]
<span class="nt">-A</span> PREROUTING <span class="nt">-m</span> addrtype <span class="nt">--dst-type</span> LOCAL <span class="nt">-p</span> udp <span class="nt">--dport</span> 53 <span class="nt">-j</span> iBugVPN
<span class="nt">-A</span> iBugVPN <span class="nt">-m</span> u32 <span class="nt">--u32</span> <span class="s2">"25 &amp; 0xFF = 1:4 &amp;&amp; 28 &amp; 0xFFFFFF = 0"</span> <span class="nt">-j</span> REDIRECT <span class="nt">--to-port</span> 51820
<span class="nt">-A</span> iBugVPN <span class="nt">-j</span> REDIRECT <span class="nt">--to-port</span> 1194
COMMIT
</code></pre>
		</div>
	</div>
	<p>With both OpenVPN and WireGuard running on their standard ports, this will redirect each protocol to its respective service port. While these rules only operate on the initial packet, Linux conntrack will handle the rest of the connection.</p>
	<p>The <code class="language-plaintext highlighter-rouge">u32</code> match is explained:</p>
	<ul>
		<li>Basic syntax: <code class="language-plaintext highlighter-rouge">&lt;offset&gt; [operators...] = &lt;range&gt;</code>, where <code class="language-plaintext highlighter-rouge">&lt;offset&gt;</code> is relative to the IP header. For UDP over IPv4, the application payload starts from 28 (20&nbsp;bytes of IPv4 and 8&nbsp;bytes of UDP)</li>
		<li><code class="language-plaintext highlighter-rouge">25 &amp; 0xFF = 1:4</code>: The 28th byte is in range <code class="language-plaintext highlighter-rouge">1:4</code>.</li>
		<li><code class="language-plaintext highlighter-rouge">28 &amp; 0xFFFFFF = 0</code>: The 29th to 31th bytes are all zero.</li>
	</ul>
	<p>For IPv6, you just need to increase the offset by 20 (IPv6 header is 40&nbsp;bytes), so the rule becomes <code class="language-plaintext highlighter-rouge">45 &amp; 0xFF = 1:4 &amp;&amp; 48 &amp; 0xFFFFFF = 0</code>.</p>
	<p>This VPN server is running like a hearse so proofs are left out for brevity.</p>
	]]></content><author><name>iBug</name></author><category term="linux" /><category term="networking" /><summary type="html"><![CDATA[Public Wi-Fi and some campus network typically block traffic from unauthenticated clients, but more often allow traffic targeting UDP port 53 to pass through, which is normally used for DNS queries. This feature can be exploited to bypass authentication by connecting to a VPN server that’s also running on UDP 53.]]></summary></entry><entry><title type="html">Vlab 远程教学云桌面</title><link href="https://ibug.io/blog/2023/08/nju-talk/" rel="alternate" type="text/html" title="Vlab 远程教学云桌面" /><published>2023-08-19T00:00:00+00:00</published><updated>2024-01-08T04:40:54+00:00</updated><id>https://ibug.io/blog/2023/08/nju-talk</id><content type="html" xml:base="https://ibug.io/blog/2023/08/nju-talk/"><![CDATA[<section id="title">
		<h1 class="title">Vlab<br>
			远程教学云桌面</h1>
		<p class="date">iBug @ USTC</p>
		<p class="date">2023 年 8 月 19 日<br />
			南京大学</p>
	</section>
	<section id="cover-image">
		<style>
			:root {
			  --r-heading-font-weight: bold;
			}

			.slides section {
			  max-height: 100%;
			}

			li img {
			  vertical-align: middle;
			}

			li+li {
			  margin-top: 0.25em;
			}

			.img-container {
			  width: 100%;
			  height: 100%;

			  display: flex;
			  flex-direction: column;
			  justify-content: center;
			}

			.img-container img {
			  display: block;
			  max-height: 100%;
			  margin: auto !important;
			  object-fit: contain;
			}

			.reveal section>img {
			  display: block;
			  margin: auto !important;
			  max-height: 95vh;
			}

			.border {
			  border: 1px solid black;
			}
		</style>
		<div class="img-container">
			<img src="https://image.ibugone.com/vlab/vlab-in-browser.jpg" />
		</div>
	</section>
	<section id="目录">
		<h2>目录</h2>
		<ol type="1">
			<li>背景</li>
			<li>第一代 Vlab</li>
			<li>第二代 Vlab</li>
			<li>技术分享</li>
			<li>共享灵车</li>
			<li>成果</li>
		</ol>
	</section>
	<section>
		<section id="background">
			<h2>背景</h2>
			<p>计算机实验的环境配置问题：</p>
			<ul>
				<li>学校机房开放时间有限，利用率低</li>
				<li>部分实验软件体积大、对配置要求高（如 Vivado）</li>
				<li>学生使用的系统环境不同，导致安装与使用时出现奇怪的问题</li>
				<li>部分实验环境安装时容易损坏（如双系统安装）</li>
			</ul>
		</section>
		<section id="background-idea">
			<h2>思考</h2>
			<p>能不能通过提供预先配置好实验环境的虚拟机来解决这个问题呢？</p>
			<ul>
				<li>Linux 虚拟机还是 Windows 虚拟机？</li>
				<li>实验软件怎么配？</li>
				<li>给学生分配多少系统资源？主机需要多少硬件配置？</li>
				<li>单位支持：计算机实验教学中心</li>
			</ul>
		</section>
	</section>
	<section>
		<section id="1st-gen">
			<h2>第一代 Vlab</h2>
			<ul>
				<li>2019 年暑假搭建完成<br />
					秋季学期小范围运营</li>
				<li>接入校园网，提供 VNC 连接</li>
				<li>打包虚拟机镜像预装 Vivado 方便实验</li>
			</ul>
		</section>
		<section id="1st-gen-features">
			<h3>平台特点</h3>
			<ul>
				<li>单台 E5 2630 v4 (2S)，<s>64</s> 128&nbsp;GB 内存，一些固态和机械</li>
				<li>Ubuntu 18.04 + 3.10.0-957.el7🤔 + LXD snap</li>
				<li>lxdbr0 ↔ USTCnet</li>
			</ul>
			<hr />
			<ul>
				<li>校园网接入：可以使用网络通选择出口或从校外连接
					<ul>
						<li><s>也可以挂 Minecraft 服务器、Terraria 服务器、……</s></li>
					</ul>
				</li>
				<li>虚拟机镜像：(Ubuntu 1&nbsp;GB) + Xfce4 (2&nbsp;GB) + Vivado (<b>18&nbsp;GB</b>)
					<ul>
						<li>好在单机有 ZFS 可以用</li>
					</ul>
				</li>
				<li>用 Django 糊了个面板（@taoky），使用统一身份认证登录</li>
			</ul>
		</section>
		<section id="1st-gen-sumup">
			<h3>总结经验</h3>
			<ul>
				<li><s>一台母鸡超卖也卖不动多少啊</s></li>
				<li>避免将用户虚拟机直接连接在校园网上
					<ul>
						<li>这样既不方便使用，也不安全</li>
						<li>机房 IP 早晚会不够用的（3× /24）</li>
						<li>开个 NAT</li>
					</ul>
				</li>
				<li>提供桌面和命令行的统一登录接口，方便用户连接使用</li>
				<li>配备更多实验软件</li>
				<li>完善用户文档</li>
			</ul>
		</section>
	</section>
	<section>
		<section id="2nd-gen">
			<h2>第二代 Vlab</h2>
			<ul>
				<li>2020 年寒假基本配置完成<br />
					春季学期投入使用</li>
				<li>改进了第一代 Vlab 的许多不足点</li>
			</ul>
		</section>
		<section id="2nd-gen-infrastructure">
			<h3>基础设施</h3>
			<ul>
				<li>采购：HPE MSA 1050，Gen10 节点 ×8, 251 交换机</li>
				<li>Ubuntu ❌ Proxmox VE ✔</li>
				<li>iSCSI 存储共享：LVM（no thin provisioning）</li>
				<li>网络：VXLAN、NAT 网关</li>
				<li><s>小修小补的</s> Django 面板</li>
				<li>超卖能力++++</li>
			</ul>
		</section>
		<section id="2nd-gen-network">
			<div class="img-container">
				<img src="https://image.ibugone.com/vlab/network-external-1.png" />
			</div>
		</section>
		<section id="2nd-gen-network-internal">
			<div class="img-container">
				<img src="https://image.ibugone.com/vlab/network-internal.png" />
			</div>
		</section>
		<section id="gateway">
			<h3>年轻人的第一次卵路由实践</h3>
			<ul>
				<li>基础功能：为虚拟机提供 NAT 上网</li>
				<li>基本操作：Debian LXC + 手搓 iproute2 + iptables（其实也没那么复杂）</li>
				<li>DNS + 监控：AdGuard Home</li>
				<li>流量记录：<code>-m conntrack --ctstate NEW -j NFLOG</code>
					<ul>
						<li>没有磁带，不宜全量镜像</li>
					</ul>
				</li>
			</ul>
		</section>
		<section id="lxc-build">
			<h3>稳定可靠的 LXC 镜像构建技术</h3>
			<ul>
				<li>Docker 提供 build environment，PVE 提供 base image</li>
				<li>基于 shell 脚本和 GitHub Actions 的自动化流程
					<ul>
						<li><code>add_file</code>, <code>add_package</code>, <code>run</code> 等“指令”</li>
						<li><s>就差发明一个 <code>Lxcfile</code> DSL 了</s></li>
					</ul>
				</li>
				<li>Repository：<a href="https://github.com/USTC-vlab/labstrap"><i class="fab fa-github"></i>
						USTC-vlab/labstrap</a>
					<ul>
						<li>精神前辈：图书馆查询🐔的 PXE 镜像构建：<a href="https://github.com/ustclug/liimstrap"><i class="fab fa-github"></i>
								ustclug/liimstrap</a></li>
					</ul>
				</li>
			</ul>
		</section>
	</section>
	<section>
		<section id="login">
			<h2>登录方式</h2>
			<p>不开放端口，各种协议都需要转发</p>
			<p>VNC, SSH, and what?</p>
		</section>
		<section id="login-ssh-1">
			<h3>SSH 统一登录</h3>
			<ul>
				<li>SSH 没有 Host header 怎么办：来点 PubkeyAuthentication</li>
				<li><code>ssh <b>-i vm-114514.pem</b> ubuntu@vlab.ustc.edu.cn</code></li>
				<li>鉴权：就像 GitHub / GitLab 一样直接按公钥区分用户（VM）
					<ul>
						<li>Django 提供一个 pubkey → VM IP address 的接口</li>
					</ul>
				</li>
				<li>后端：<code>golang.org/x/crypto/ssh</code>
					<ul>
						<li>初版：Forked from <a href="https://github.com/tg123/sshpiper"><i class="fab fa-github"></i>
								tg123/sshpiper</a></li>
						<li>现在：<a href="https://github.com/USTC-vlab/sshmux"><i class="fab fa-github"></i> USTC-vlab/sshmux</a>
						</li>
						<li>sshpiper 重构了，不好用了 QwQ</li>
					</ul>
				</li>
			</ul>
		</section>
		<section id="login-ssh-2">
			<h3>SSH 统一登录</h3>
			<ul>
				<li>恢复模式（LXC）：<code>ssh <b>recovery</b>@vlab.ustc.edu.cn</code>
					<ul>
						<li>后台转接到 <code>pct enter &lt;vmid&gt;</code></li>
					</ul>
				</li>
				<li>控制台模式（LXC）：<code>ssh <b>console</b>@vlab.ustc.edu.cn</code>
					<ul>
						<li>后台转接到 <code>pct console &lt;vmid&gt;</code></li>
					</ul>
				</li>
				<li>控制台模式（KVM）：<code>ssh <b>serial</b>@vlab.ustc.edu.cn</code>
					<ul>
						<li>后台转接到 <code>qm serial &lt;vmid&gt;</code></li>
					</ul>
				</li>
			</ul>
		</section>
		<section id="login-vnc-1">
			<h3>VNC 统一登录</h3>
			<ul>
				<li>请出神仙：<a href="https://github.com/pdlan"><i class="fab fa-github"></i> pdlan</a>
					<ul>
						<li>逆向了 RealVNC，写了 10,000 行 C艹，到处 <code>co_await</code>，……</li>
						<li>顺带还实现了 TLS 加密</li>
						<li>顺带还实现了……</li>
						<li>外加一个 unix-domain socket 发送管理指令</li>
					</ul>
				</li>
				<li><s>一起来大受震撼吧</s></li>
				<li>使用 VNC 软件连接：
					<ul>
						<li>服务器：<code>vlab.ustc.edu.cn</code>（标准端口 5900/tcp）</li>
						<li>用户名：<code>PB17000001:114514</code>（用户名 + VM ID，如果用户有多个 VM 的话）</li>
					</ul>
				</li>
			</ul>
		</section>
		<section id="login-vnc-2">
			<h3>VNC 统一登录</h3>
			<ul>
				<li>开源贡献：</li>
				<li>
					<img src="https://image.ibugone.com/vlab/tigervnc-pr-pdlan.png" />
					<img src="https://image.ibugone.com/vlab/novnc-pr-pdlan.png" />
				</li>
			</ul>
		</section>
		<section id="login-rdp">
			<h3>RDP 统一登录</h3>
			<ul>
				<li><s>咕咕咕了，Windows VM 支持还没搞定</s></li>
				<li>RDP 一大坨非常起夜级的协议，<s>不是很想逆向</s></li>
				<li><i class="fas fa-fw fa-lightbulb-on"></i> 计划规格：<code>loadbalanceinfo</code></li>
			</ul>
		</section>
		<section id="login-browser">
			<h3>浏览器登录</h3>
			<ul>
				<li>VNC：魔改版 noVNC
					<ul>
						<li>没错，vncmux 顺带还实现了 WebSocket</li>
					</ul>
				</li>
				<li>SSH：Go → WASM</li>
				<li>RDP：从入门到放弃</li>
			</ul>
		</section>
	</section>
	<section>
		<section id="hearses">
			<h2>灵车时间</h2>
		</section>
		<section id="vlab-software-1">
			<h3>磁盘容量</h3>
			<ul>
				<li>一个虚拟机镜像就已经 21&nbsp;GB 了</li>
				<li>+MATLAB, +Mathematica, +<a href="https://image.ibugone.com/vlab/node_modules-meme.png">node_modules</a></li>
				<li>我们的存储阵列里就有 114514 份 Vivado</li>
			</ul>
		</section>
		<section id="vlab-software-2">
			<h3>Bind mount!</h3>
			<ul>
				<li>local-lvm 开个新卷给 <code>/opt/vlab</code></li>
				<li>mp0: /opt/vlab,mp=/opt/vlab,ro=1</li>
				<li>易于维护：Rsync cron job</li>
			</ul>
		</section>
		<section id="novnc-fun">
			<h3>No... VNC?</h3>
			<p><img src="https://image.ibugone.com/vlab/no-vnc.png" /></p>
		</section>
		<section>
			<img src="https://image.ibugone.com/vlab/502.png" />
		</section>
		<section id="lvm-metadata-full">
			<h3>Everything breaks if pushed too hard...</h3>
			<pre style="font-size: 1em;"><code>VG test 1723 metadata on /dev/sdc1 (521759&nbsp;bytes) exceeds maximum metadata size (521472&nbsp;bytes)
Failed to write VG test.</code></pre>
			<p><a href="https://ibug.io/p/52"><i class="fas fa-fw fa-arrow-alt-circle-right"></i> ibug.io/p/52</a></p>
		</section>
		<section id="iowait-spike">
			<h3>IOWait（<code>%wa</code>）午夜准时爆炸</h3>
			<p style="width: 100%; height: 8em; display: flex; justify-content: space-evenly;">
				<img src="https://image.ibugone.com/vlab/iowait-load-average.png" />
				<img src="https://image.ibugone.com/vlab/iowait-iowait.png" />
			</p>
			<p>
				替用户停掉了 <code>man-db.timer</code> 和 <code>apt-daily-upgrade.timer</code>，
				<br />
				给 <code>logrotate.timer</code> 补上了 <code>RandomizedDelaySec=3h</code>。
			</p>
		</section>
		<section id="other-software-gore">
			<h3>其他灵异事件</h3>
			<ul>
				<li>
					一运行备份，网卡就掉了 😦<br />
					解决方法：两边开启 jumbo frame，MTU 拉到 9000 字节
				</li>
				<li>存储服务器的密码掉了</li>
				<li>PVE HA 过于热情（+<code>nofailback</code>）</li>
				<li>Vivado 又双叒叕炸了
					<ol type="1">
						<li><code>LD_PRELOAD</code> += <code>libudev.so.1</code></li>
						<li><code>LD_PRELOAD</code> += <code>libdbus-glib-1.so.2</code></li>
					</ol>
				</li>
			</ul>
		</section>
	</section>
	<section>
		<section id="results">
			<h2>成果</h2>
			<ul>
				<li>自 2020 年春季学期运行至今</li>
				<li>与 <a href="https://fpgaol.ustc.edu.cn/">FPGA Online</a> 和 <a href="https://verilogoj.ustc.edu.cn/">Verilog
						OJ</a> 等项目结合，实现纯在线 FPGA 编程教学</li>
				<li>Grafana：<a href="https://monitor.ibugone.com/grafana/d/2">monitor.ibugone.com/grafana/d/2</a></li>
			</ul>
		</section>
		<section>
			<div class="img-container">
				<img src="https://image.ibugone.com/vlab/containers-2023-08.png" />
			</div>
		</section>
		<section>
			<h3>学习资料</h3>
			<ul>
				<li>用户文档：<a href="https://vlab.ustc.edu.cn/docs/">vlab.ustc.edu.cn/docs</a></li>
				<li>维护文档：<a href="https://vlab.ibugone.com/">vlab.ibugone.com</a></li>
				<li>GitHub Org：<a href="https://github.com/USTC-vlab"><i class="fab fa-github"></i> USTC-vlab</a></li>
			</ul>
		</section>
	</section>
	<section id="outro">
		<h1>谢谢！</h1>
		<p><small>本页面的链接：<a href="https://ibug.io/p/59"><i class="fas fa-fw fa-link"></i> ibug.io/p/59</a></small></p>
	</section>
	]]></content><author><name>iBug</name></author></entry></feed>